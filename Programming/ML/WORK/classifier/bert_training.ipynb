{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>full_content</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89541</td>\n",
       "      <td>International Business Times</td>\n",
       "      <td>UN Chief Urges World To 'Stop The Madness' Of ...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89542</td>\n",
       "      <td>Prtimes.jp</td>\n",
       "      <td>RANDEBOOよりワンランク上の大人っぽさが漂うニットとベストが新登場。</td>\n",
       "      <td>[株式会社Ainer]\\nRANDEBOO（ランデブー）では2023年7月18日(火)より公...</td>\n",
       "      <td>RANDEBOO2023718()WEB2023 Autumn Winter \\n\"Nepa...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89543</td>\n",
       "      <td>VOA News</td>\n",
       "      <td>UN Chief Urges World to 'Stop the Madness' of ...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>Kathmandu, Nepal  UN Secretary-General Antonio...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89545</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Sikkim warning: Hydroelectricity push must be ...</td>\n",
       "      <td>Ecologists caution against the adverse effects...</td>\n",
       "      <td>At least 14 persons lost their lives and more ...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>At least 14 persons lost their lives and more ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89547</td>\n",
       "      <td>The Times of Israel</td>\n",
       "      <td>200 foreigners, dual nationals cut down in Ham...</td>\n",
       "      <td>France lost 35 citizens, Thailand 33, US 31, U...</td>\n",
       "      <td>Scores of foreign citizens were killed, taken ...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105370</th>\n",
       "      <td>781108</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Have done no wrong, only did party work, says ...</td>\n",
       "      <td>The High Court today allowed Shivakumar to wit...</td>\n",
       "      <td>Karnataka Deputy Chief Minister D K Shivakumar...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Karnataka Deputy Chief Minister D K Shivakumar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105371</th>\n",
       "      <td>781129</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>FC Barcelona Guarantees $77.6 Million Champion...</td>\n",
       "      <td>FC Barcelona have guaranteed at least $77.6 mi...</td>\n",
       "      <td>FC Barcelona have guaranteed at least $767.6 m...</td>\n",
       "      <td>Home</td>\n",
       "      <td>FC Barcelona have guaranteed at least $767.6 m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105372</th>\n",
       "      <td>781235</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Three hospitals ignored her gravely ill fiancé...</td>\n",
       "      <td>Forty years ago, Sarah Lubarsky came home from...</td>\n",
       "      <td>The photo from David and Sarah Lubarsky's wedd...</td>\n",
       "      <td>Home</td>\n",
       "      <td>The photo from David and Sarah Lubarsky's wedd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105373</th>\n",
       "      <td>781240</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Kerber’s Farm: Bringing Farm To Table To Manha...</td>\n",
       "      <td>A farmstand in Long Island, Kerber’s Farms has...</td>\n",
       "      <td>Kerbers Farm: Bringing Farm To Table To Manhat...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Kerber’s Farm: Bringing Farm To Table To Manha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105374</th>\n",
       "      <td>781308</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Tips For Investing In Short-Term Rentals In Dubai</td>\n",
       "      <td>By exploring your options and keeping a few be...</td>\n",
       "      <td>Cofounder at UpperKey. Passionate about proper...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Cofounder at UpperKey. Passionate about proper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105375 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id                   source_name  \\\n",
       "0            89541  International Business Times   \n",
       "1            89542                    Prtimes.jp   \n",
       "2            89543                      VOA News   \n",
       "3            89545            The Indian Express   \n",
       "4            89547           The Times of Israel   \n",
       "...            ...                           ...   \n",
       "105370      781108            The Indian Express   \n",
       "105371      781129                        Forbes   \n",
       "105372      781235                           NPR   \n",
       "105373      781240                        Forbes   \n",
       "105374      781308                        Forbes   \n",
       "\n",
       "                                                    title  \\\n",
       "0       UN Chief Urges World To 'Stop The Madness' Of ...   \n",
       "1                   RANDEBOOよりワンランク上の大人っぽさが漂うニットとベストが新登場。   \n",
       "2       UN Chief Urges World to 'Stop the Madness' of ...   \n",
       "3       Sikkim warning: Hydroelectricity push must be ...   \n",
       "4       200 foreigners, dual nationals cut down in Ham...   \n",
       "...                                                   ...   \n",
       "105370  Have done no wrong, only did party work, says ...   \n",
       "105371  FC Barcelona Guarantees $77.6 Million Champion...   \n",
       "105372  Three hospitals ignored her gravely ill fiancé...   \n",
       "105373  Kerber’s Farm: Bringing Farm To Table To Manha...   \n",
       "105374  Tips For Investing In Short-Term Rentals In Dubai   \n",
       "\n",
       "                                              description  \\\n",
       "0       UN Secretary-General Antonio Guterres urged th...   \n",
       "1       [株式会社Ainer]\\nRANDEBOO（ランデブー）では2023年7月18日(火)より公...   \n",
       "2       UN Secretary-General Antonio Guterres urged th...   \n",
       "3       Ecologists caution against the adverse effects...   \n",
       "4       France lost 35 citizens, Thailand 33, US 31, U...   \n",
       "...                                                   ...   \n",
       "105370  The High Court today allowed Shivakumar to wit...   \n",
       "105371  FC Barcelona have guaranteed at least $77.6 mi...   \n",
       "105372  Forty years ago, Sarah Lubarsky came home from...   \n",
       "105373  A farmstand in Long Island, Kerber’s Farms has...   \n",
       "105374  By exploring your options and keeping a few be...   \n",
       "\n",
       "                                                  content category  \\\n",
       "0       UN Secretary-General Antonio Guterres urged th...    Nepal   \n",
       "1       RANDEBOO2023718()WEB2023 Autumn Winter \\n\"Nepa...    Nepal   \n",
       "2       Kathmandu, Nepal  UN Secretary-General Antonio...    Nepal   \n",
       "3       At least 14 persons lost their lives and more ...    Nepal   \n",
       "4       Scores of foreign citizens were killed, taken ...    Nepal   \n",
       "...                                                   ...      ...   \n",
       "105370  Karnataka Deputy Chief Minister D K Shivakumar...     Home   \n",
       "105371  FC Barcelona have guaranteed at least $767.6 m...     Home   \n",
       "105372  The photo from David and Sarah Lubarsky's wedd...     Home   \n",
       "105373  Kerbers Farm: Bringing Farm To Table To Manhat...     Home   \n",
       "105374  Cofounder at UpperKey. Passionate about proper...     Home   \n",
       "\n",
       "                                             full_content  relevant  \n",
       "0       UN Secretary-General Antonio Guterres urged th...         0  \n",
       "1                                                     NaN         0  \n",
       "2                                                     NaN         0  \n",
       "3       At least 14 persons lost their lives and more ...         0  \n",
       "4                                                     NaN         0  \n",
       "...                                                   ...       ...  \n",
       "105370  Karnataka Deputy Chief Minister D K Shivakumar...         0  \n",
       "105371  FC Barcelona have guaranteed at least $767.6 m...         0  \n",
       "105372  The photo from David and Sarah Lubarsky's wedd...         0  \n",
       "105373  Kerber’s Farm: Bringing Farm To Table To Manha...         0  \n",
       "105374  Cofounder at UpperKey. Passionate about proper...         0  \n",
       "\n",
       "[105375 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'C:\\\\Users\\\\tanxe\\\\Programming\\\\ML\\\\WORK\\\\classifier\\\\data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(columns=['source_id', 'author', 'published_at', 'url_to_image', 'url' ])\n",
    "filtered_df = df\n",
    "filtered_df['relevant'] = filtered_df['category'].apply(lambda x: 1 if x == 'Stock' or x == 'Finance' else 0)\n",
    "df_cleaned = filtered_df.dropna(subset=['content'])\n",
    "balanced_df = df_cleaned\n",
    "balanced_df\n",
    "\n",
    "# filtered_df = df[df['source_name'].isin(['GlobeNewswire', 'The Times of India'])]\n",
    "\n",
    "# filtered_df['relevant'] = filtered_df['category'].apply(lambda x: 1 if x == 'COVID' else 0)\n",
    "# df_cleaned = filtered_df.dropna(subset=['full_content'])\n",
    "# df_relevant_zero = df_cleaned[df_cleaned['relevant'] == 0]\n",
    "# df_relevant_one = df_cleaned[df_cleaned['relevant'] == 1]\n",
    "# df_sampled = df_relevant_zero.sample(n=1400, random_state=42)\n",
    "# balanced_df = pd.concat([df_sampled, df_relevant_one], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanxe\\AppData\\Local\\Temp\\ipykernel_12716\\2424080986.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  balanced_df_24k['relevant'] = balanced_df_24k['category'].apply(lambda x: 1 if x == 'Stock' else 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "relevant\n",
       "0    20757\n",
       "1     3503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df_24k = balanced_df[balanced_df['source_name'].isin([\"ETF Daily News\", \"The Times of India\"])]\n",
    "balanced_df_24k['relevant'] = balanced_df_24k['category'].apply(lambda x: 1 if x == 'Stock' else 0)\n",
    "balanced_df_24k = balanced_df_24k.dropna(subset=['content'])\n",
    "balanced_df_24k['relevant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to download the right version for office gpu\n",
    "# %pip uninstall spacy\n",
    "# %pip install cupy-cuda117\n",
    "# %pip install spacy[cuda117]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>full_content</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3109</th>\n",
       "      <td>94343</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>These 9 commodity stocks hit 52-week high on T...</td>\n",
       "      <td>During Thursday's trading session, the Sensex ...</td>\n",
       "      <td>Nov 02, 2023, 07:22:41 PM IST\\nDuring Thursday...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57910</th>\n",
       "      <td>133924</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Fundamental Radar: Varun Beverages poised to b...</td>\n",
       "      <td>Varun Beverages Ltd is the second-largest fran...</td>\n",
       "      <td>SynopsisVarun Beverages Ltd is the second-larg...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57935</th>\n",
       "      <td>134021</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Stock market update: Mining stocks up as marke...</td>\n",
       "      <td>The 30-share BSE Sensex was  up  425.32 points...</td>\n",
       "      <td>NEW DELHI: Mining stocks were trading higher o...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Getty Images Nifty moved in a tight range of 8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57936</th>\n",
       "      <td>134022</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Stock market update: Fertilisers stocks up as ...</td>\n",
       "      <td>The 30-share BSE Sensex was  up  441.31 points...</td>\n",
       "      <td>NEW DELHI: Fertilisers stocks were trading hig...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Getty Images NEW DELHI: Fertilisers stocks wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57937</th>\n",
       "      <td>134023</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>InterGlobe stock price up 0.08 per cent as Sen...</td>\n",
       "      <td>As of 30-Sep-2023, promoters held 38.02 per ce...</td>\n",
       "      <td>Shares of InterGlobe Aviation Ltd. rose 0.08 p...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Reuters On an immediate basis, 15,770/52,500 a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102076</th>\n",
       "      <td>693939</td>\n",
       "      <td>ETF Daily News</td>\n",
       "      <td>Universal (NYSE:UVV) vs. British American Toba...</td>\n",
       "      <td>Universal (NYSE:UVV – Get Free Report) and Bri...</td>\n",
       "      <td>Universal (NYSE:UVV – Get Free Report) and Bri...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Universal (NYSE:UVV–Get Free Report) and Briti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102077</th>\n",
       "      <td>693944</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Do we have enough retail money in debt markets?</td>\n",
       "      <td>​​For example, as per the monthly data release...</td>\n",
       "      <td>Generally, the retail investors are late to th...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>IANS INSIGHTS  \\n    \\t                    Rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102078</th>\n",
       "      <td>693947</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>FII action, OPEC+ meet among top 10 factors to...</td>\n",
       "      <td>Meena expects the market to experience some di...</td>\n",
       "      <td>Indian frontline indices S&amp;amp;P BSE Sensex an...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>ETMarkets.com Indian frontline indices S&amp;P BSE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102079</th>\n",
       "      <td>693954</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>For workers at this iPhone plant, Tata means a...</td>\n",
       "      <td>At the Narasapura facility, the recent takeove...</td>\n",
       "      <td>It is the Tata tag we aim for, who doesnt want...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>“It is the  Tata  tag we aim for, who doesn’t ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102820</th>\n",
       "      <td>711912</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Riding on the digitization of Indian capital m...</td>\n",
       "      <td>\"The BSE derivatives market share jumped to 14...</td>\n",
       "      <td>Stating Indian exchanges are benefiting from f...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Reuters Bombay Stock Exchange Related FII acti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3503 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id         source_name  \\\n",
       "3109         94343  The Times of India   \n",
       "57910       133924  The Times of India   \n",
       "57935       134021  The Times of India   \n",
       "57936       134022  The Times of India   \n",
       "57937       134023  The Times of India   \n",
       "...            ...                 ...   \n",
       "102076      693939      ETF Daily News   \n",
       "102077      693944  The Times of India   \n",
       "102078      693947  The Times of India   \n",
       "102079      693954  The Times of India   \n",
       "102820      711912  The Times of India   \n",
       "\n",
       "                                                    title  \\\n",
       "3109    These 9 commodity stocks hit 52-week high on T...   \n",
       "57910   Fundamental Radar: Varun Beverages poised to b...   \n",
       "57935   Stock market update: Mining stocks up as marke...   \n",
       "57936   Stock market update: Fertilisers stocks up as ...   \n",
       "57937   InterGlobe stock price up 0.08 per cent as Sen...   \n",
       "...                                                   ...   \n",
       "102076  Universal (NYSE:UVV) vs. British American Toba...   \n",
       "102077    Do we have enough retail money in debt markets?   \n",
       "102078  FII action, OPEC+ meet among top 10 factors to...   \n",
       "102079  For workers at this iPhone plant, Tata means a...   \n",
       "102820  Riding on the digitization of Indian capital m...   \n",
       "\n",
       "                                              description  \\\n",
       "3109    During Thursday's trading session, the Sensex ...   \n",
       "57910   Varun Beverages Ltd is the second-largest fran...   \n",
       "57935   The 30-share BSE Sensex was  up  425.32 points...   \n",
       "57936   The 30-share BSE Sensex was  up  441.31 points...   \n",
       "57937   As of 30-Sep-2023, promoters held 38.02 per ce...   \n",
       "...                                                   ...   \n",
       "102076  Universal (NYSE:UVV – Get Free Report) and Bri...   \n",
       "102077  ​​For example, as per the monthly data release...   \n",
       "102078  Meena expects the market to experience some di...   \n",
       "102079  At the Narasapura facility, the recent takeove...   \n",
       "102820  \"The BSE derivatives market share jumped to 14...   \n",
       "\n",
       "                                                  content category  \\\n",
       "3109    Nov 02, 2023, 07:22:41 PM IST\\nDuring Thursday...    Stock   \n",
       "57910   SynopsisVarun Beverages Ltd is the second-larg...    Stock   \n",
       "57935   NEW DELHI: Mining stocks were trading higher o...    Stock   \n",
       "57936   NEW DELHI: Fertilisers stocks were trading hig...    Stock   \n",
       "57937   Shares of InterGlobe Aviation Ltd. rose 0.08 p...    Stock   \n",
       "...                                                   ...      ...   \n",
       "102076  Universal (NYSE:UVV – Get Free Report) and Bri...    Stock   \n",
       "102077  Generally, the retail investors are late to th...    Stock   \n",
       "102078  Indian frontline indices S&amp;P BSE Sensex an...    Stock   \n",
       "102079  It is the Tata tag we aim for, who doesnt want...    Stock   \n",
       "102820  Stating Indian exchanges are benefiting from f...    Stock   \n",
       "\n",
       "                                             full_content  relevant  \n",
       "3109                                                  NaN         1  \n",
       "57910                                                 NaN         1  \n",
       "57935   Getty Images Nifty moved in a tight range of 8...         1  \n",
       "57936   Getty Images NEW DELHI: Fertilisers stocks wer...         1  \n",
       "57937   Reuters On an immediate basis, 15,770/52,500 a...         1  \n",
       "...                                                   ...       ...  \n",
       "102076  Universal (NYSE:UVV–Get Free Report) and Briti...         1  \n",
       "102077  IANS INSIGHTS  \\n    \\t                    Rea...         1  \n",
       "102078  ETMarkets.com Indian frontline indices S&P BSE...         1  \n",
       "102079  “It is the  Tata  tag we aim for, who doesn’t ...         1  \n",
       "102820  Reuters Bombay Stock Exchange Related FII acti...         1  \n",
       "\n",
       "[3503 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df_24k[balanced_df_24k['relevant'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "text = balanced_df['content'][0]\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# nlp = spacy.load('en_core_web_trf') may be better in office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = balanced_df['full_content'][0]\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UN ORG\n",
      "Antonio Guterres PERSON\n",
      "Monday DATE\n",
      "Himalayan NORP\n",
      "Guterres PERSON\n",
      "Everest LOC\n",
      "Nepal GPE\n",
      "nearly a third CARDINAL\n",
      "just over three decades DATE\n",
      "Himalayas LOC\n",
      "Nepal GPE\n",
      "65 percent PERCENT\n",
      "the last decade DATE\n",
      "Guterres PERSON\n",
      "four-day DATE\n",
      "Nepal GPE\n",
      "Himalayan NORP\n",
      "Hindu NORP\n",
      "around 240 million CARDINAL\n",
      "1.65 billion CARDINAL\n",
      "South Asian NORP\n",
      "Southeast Asian NORP\n",
      "10 CARDINAL\n",
      "Ganges NORP\n",
      "Indus GPE\n",
      "Yellow GPE\n",
      "Mekong GPE\n",
      "Irrawaddy GPE\n",
      "billions CARDINAL\n",
      "today DATE\n",
      "Guterres PERSON\n",
      "Syangboche village GPE\n",
      "Everest LOC\n",
      "nearly 1.2 degrees Celsius QUANTITY\n",
      "the mid-1800s DATE\n",
      "1.5 degrees QUANTITY\n",
      "Guterres PERSON\n",
      "first ORDINAL\n",
      "Himalayan NORP\n",
      "Indus GPE\n",
      "Ganges ORG\n",
      "Brahmaputra ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a seperate column for the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12                                    Taj Mahal_384 2nd_391\n",
       "75                                                India_384\n",
       "76               Offbeat Himachal getaways_384 Dussehra_380\n",
       "126       first_396 Israel_384 Kathmandu_384 the wee hou...\n",
       "128                             Indian_381 the next day_391\n",
       "                                ...                        \n",
       "105352    BRYN MAWR TRUST Co_383 TE Connectivity Ltd._38...\n",
       "105353    AngloGold Ashanti plc_383 NYSE_383 3.8%_393 Mo...\n",
       "105357    Malaga_384 Nov 29_391 2023_391 Europe_385 Wedn...\n",
       "105359    Pharming Group_383 Get Free Report_387 Monday_...\n",
       "105360    Aquis Exchange_383 Canaccord Genuity Group_383...\n",
       "Name: entities, Length: 24260, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities_plus_labels = [f\"{ent}_{ent.label}\" for ent in doc.ents]\n",
    "    return \" \".join(entities_plus_labels)\n",
    "\n",
    "\n",
    "balanced_df_24k['entities'] = balanced_df_24k['content'].apply(extract_entities)\n",
    "balanced_df_24k['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevant\n",
       "0    12454\n",
       "1     2102\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(balanced_df_24k, test_size=0.4, random_state=42, stratify=balanced_df_24k['relevant'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['relevant'])\n",
    "\n",
    "train_df['relevant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevant\n",
       "0    4152\n",
       "1     700\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['relevant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())  # This should return True if CUDA is available "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Bert\n",
    "with entities incorporated into training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Batch 10/910 - Loss: 0.0890\n",
      "  Batch 20/910 - Loss: 0.0025\n",
      "  Batch 30/910 - Loss: 0.0155\n",
      "  Batch 40/910 - Loss: 0.0195\n",
      "  Batch 50/910 - Loss: 0.0390\n",
      "  Batch 60/910 - Loss: 0.0139\n",
      "  Batch 70/910 - Loss: 0.1055\n",
      "  Batch 80/910 - Loss: 0.0071\n",
      "  Batch 90/910 - Loss: 0.0100\n",
      "  Batch 100/910 - Loss: 0.0069\n",
      "  Batch 110/910 - Loss: 0.0079\n",
      "  Batch 120/910 - Loss: 0.0166\n",
      "  Batch 130/910 - Loss: 0.0329\n",
      "  Batch 140/910 - Loss: 0.0710\n",
      "  Batch 150/910 - Loss: 0.0492\n",
      "  Batch 160/910 - Loss: 0.0238\n",
      "  Batch 170/910 - Loss: 0.0237\n",
      "  Batch 180/910 - Loss: 0.0027\n",
      "  Batch 190/910 - Loss: 0.0306\n",
      "  Batch 200/910 - Loss: 0.0310\n",
      "  Batch 210/910 - Loss: 0.0974\n",
      "  Batch 220/910 - Loss: 0.0271\n",
      "  Batch 230/910 - Loss: 0.0020\n",
      "  Batch 240/910 - Loss: 0.0220\n",
      "  Batch 250/910 - Loss: 0.1192\n",
      "  Batch 260/910 - Loss: 0.0314\n",
      "  Batch 270/910 - Loss: 0.0170\n",
      "  Batch 280/910 - Loss: 0.0217\n",
      "  Batch 290/910 - Loss: 0.0293\n",
      "  Batch 300/910 - Loss: 0.0550\n",
      "  Batch 310/910 - Loss: 0.1255\n",
      "  Batch 320/910 - Loss: 0.0065\n",
      "  Batch 330/910 - Loss: 0.0078\n",
      "  Batch 340/910 - Loss: 0.0125\n",
      "  Batch 350/910 - Loss: 0.0525\n",
      "  Batch 360/910 - Loss: 0.0135\n",
      "  Batch 370/910 - Loss: 0.0050\n",
      "  Batch 380/910 - Loss: 0.0597\n",
      "  Batch 390/910 - Loss: 0.0407\n",
      "  Batch 400/910 - Loss: 0.0095\n",
      "  Batch 410/910 - Loss: 0.0162\n",
      "  Batch 420/910 - Loss: 0.0079\n",
      "  Batch 430/910 - Loss: 0.0142\n",
      "  Batch 440/910 - Loss: 0.0144\n",
      "  Batch 450/910 - Loss: 0.0185\n",
      "  Batch 460/910 - Loss: 0.0644\n",
      "  Batch 470/910 - Loss: 0.0477\n",
      "  Batch 480/910 - Loss: 0.0084\n",
      "  Batch 490/910 - Loss: 0.0828\n",
      "  Batch 500/910 - Loss: 0.0188\n",
      "  Batch 510/910 - Loss: 0.0629\n",
      "  Batch 520/910 - Loss: 0.0018\n",
      "  Batch 530/910 - Loss: 0.0395\n",
      "  Batch 540/910 - Loss: 0.0035\n",
      "  Batch 550/910 - Loss: 0.0171\n",
      "  Batch 560/910 - Loss: 0.1159\n",
      "  Batch 570/910 - Loss: 0.0822\n",
      "  Batch 580/910 - Loss: 0.0022\n",
      "  Batch 590/910 - Loss: 0.0867\n",
      "  Batch 600/910 - Loss: 0.0088\n",
      "  Batch 610/910 - Loss: 0.0461\n",
      "  Batch 620/910 - Loss: 0.0608\n",
      "  Batch 630/910 - Loss: 0.0335\n",
      "  Batch 640/910 - Loss: 0.0849\n",
      "  Batch 650/910 - Loss: 0.0171\n",
      "  Batch 660/910 - Loss: 0.0891\n",
      "  Batch 670/910 - Loss: 0.0364\n",
      "  Batch 680/910 - Loss: 0.0234\n",
      "  Batch 690/910 - Loss: 0.0323\n",
      "  Batch 700/910 - Loss: 0.0555\n",
      "  Batch 710/910 - Loss: 0.0135\n",
      "  Batch 720/910 - Loss: 0.0668\n",
      "  Batch 730/910 - Loss: 0.0135\n",
      "  Batch 740/910 - Loss: 0.0608\n",
      "  Batch 750/910 - Loss: 0.0365\n",
      "  Batch 760/910 - Loss: 0.0037\n",
      "  Batch 770/910 - Loss: 0.0543\n",
      "  Batch 780/910 - Loss: 0.0156\n",
      "  Batch 790/910 - Loss: 0.0691\n",
      "  Batch 800/910 - Loss: 0.0647\n",
      "  Batch 810/910 - Loss: 0.0660\n",
      "  Batch 820/910 - Loss: 0.0413\n",
      "  Batch 830/910 - Loss: 0.0123\n",
      "  Batch 840/910 - Loss: 0.0544\n",
      "  Batch 850/910 - Loss: 0.0357\n",
      "  Batch 860/910 - Loss: 0.0127\n",
      "  Batch 870/910 - Loss: 0.0484\n",
      "  Batch 880/910 - Loss: 0.0065\n",
      "  Batch 890/910 - Loss: 0.0024\n",
      "  Batch 900/910 - Loss: 0.1902\n",
      "  Batch 910/910 - Loss: 0.1222\n",
      "Training Loss: 0.0423\n",
      "Validation Loss: 0.0775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.10      0.18      4149\n",
      "         1.0       0.16      1.00      0.27       703\n",
      "\n",
      "    accuracy                           0.23      4852\n",
      "   macro avg       0.58      0.55      0.22      4852\n",
      "weighted avg       0.87      0.23      0.19      4852\n",
      "\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 2/20\n",
      "  Batch 10/910 - Loss: 0.0202\n",
      "  Batch 20/910 - Loss: 0.0197\n",
      "  Batch 30/910 - Loss: 0.0040\n",
      "  Batch 40/910 - Loss: 0.0083\n",
      "  Batch 50/910 - Loss: 0.0095\n",
      "  Batch 60/910 - Loss: 0.0038\n",
      "  Batch 70/910 - Loss: 0.0566\n",
      "  Batch 80/910 - Loss: 0.1045\n",
      "  Batch 90/910 - Loss: 0.0031\n",
      "  Batch 100/910 - Loss: 0.0027\n",
      "  Batch 110/910 - Loss: 0.0430\n",
      "  Batch 120/910 - Loss: 0.0385\n",
      "  Batch 130/910 - Loss: 0.0614\n",
      "  Batch 140/910 - Loss: 0.0253\n",
      "  Batch 150/910 - Loss: 0.0047\n",
      "  Batch 160/910 - Loss: 0.0608\n",
      "  Batch 170/910 - Loss: 0.0479\n",
      "  Batch 180/910 - Loss: 0.0088\n",
      "  Batch 190/910 - Loss: 0.0594\n",
      "  Batch 200/910 - Loss: 0.0050\n",
      "  Batch 210/910 - Loss: 0.0023\n",
      "  Batch 220/910 - Loss: 0.0018\n",
      "  Batch 230/910 - Loss: 0.0032\n",
      "  Batch 240/910 - Loss: 0.0369\n",
      "  Batch 250/910 - Loss: 0.0142\n",
      "  Batch 260/910 - Loss: 0.0755\n",
      "  Batch 270/910 - Loss: 0.0066\n",
      "  Batch 280/910 - Loss: 0.1042\n",
      "  Batch 290/910 - Loss: 0.0037\n",
      "  Batch 300/910 - Loss: 0.0632\n",
      "  Batch 310/910 - Loss: 0.0606\n",
      "  Batch 320/910 - Loss: 0.0399\n",
      "  Batch 330/910 - Loss: 0.0021\n",
      "  Batch 340/910 - Loss: 0.0084\n",
      "  Batch 350/910 - Loss: 0.0104\n",
      "  Batch 360/910 - Loss: 0.0092\n",
      "  Batch 370/910 - Loss: 0.0334\n",
      "  Batch 380/910 - Loss: 0.0856\n",
      "  Batch 390/910 - Loss: 0.0011\n",
      "  Batch 400/910 - Loss: 0.0101\n",
      "  Batch 410/910 - Loss: 0.0348\n",
      "  Batch 420/910 - Loss: 0.0546\n",
      "  Batch 430/910 - Loss: 0.0224\n",
      "  Batch 440/910 - Loss: 0.0055\n",
      "  Batch 450/910 - Loss: 0.0523\n",
      "  Batch 460/910 - Loss: 0.0666\n",
      "  Batch 470/910 - Loss: 0.0015\n",
      "  Batch 480/910 - Loss: 0.0117\n",
      "  Batch 490/910 - Loss: 0.0220\n",
      "  Batch 500/910 - Loss: 0.0178\n",
      "  Batch 510/910 - Loss: 0.0078\n",
      "  Batch 520/910 - Loss: 0.0029\n",
      "  Batch 530/910 - Loss: 0.0062\n",
      "  Batch 540/910 - Loss: 0.0186\n",
      "  Batch 550/910 - Loss: 0.0258\n",
      "  Batch 560/910 - Loss: 0.1092\n",
      "  Batch 570/910 - Loss: 0.0177\n",
      "  Batch 580/910 - Loss: 0.0053\n",
      "  Batch 590/910 - Loss: 0.0375\n",
      "  Batch 600/910 - Loss: 0.1473\n",
      "  Batch 610/910 - Loss: 0.0494\n",
      "  Batch 620/910 - Loss: 0.0020\n",
      "  Batch 630/910 - Loss: 0.0195\n",
      "  Batch 640/910 - Loss: 0.0123\n",
      "  Batch 650/910 - Loss: 0.0116\n",
      "  Batch 660/910 - Loss: 0.1331\n",
      "  Batch 670/910 - Loss: 0.0579\n",
      "  Batch 680/910 - Loss: 0.0589\n",
      "  Batch 690/910 - Loss: 0.0146\n",
      "  Batch 700/910 - Loss: 0.0008\n",
      "  Batch 710/910 - Loss: 0.0162\n",
      "  Batch 720/910 - Loss: 0.0079\n",
      "  Batch 730/910 - Loss: 0.0049\n",
      "  Batch 740/910 - Loss: 0.0091\n",
      "  Batch 750/910 - Loss: 0.0211\n",
      "  Batch 760/910 - Loss: 0.0265\n",
      "  Batch 770/910 - Loss: 0.0474\n",
      "  Batch 780/910 - Loss: 0.0685\n",
      "  Batch 790/910 - Loss: 0.0239\n",
      "  Batch 800/910 - Loss: 0.0057\n",
      "  Batch 810/910 - Loss: 0.0281\n",
      "  Batch 820/910 - Loss: 0.0812\n",
      "  Batch 830/910 - Loss: 0.0301\n",
      "  Batch 840/910 - Loss: 0.0053\n",
      "  Batch 850/910 - Loss: 0.0022\n",
      "  Batch 860/910 - Loss: 0.0020\n",
      "  Batch 870/910 - Loss: 0.0081\n",
      "  Batch 880/910 - Loss: 0.0555\n",
      "  Batch 890/910 - Loss: 0.0104\n",
      "  Batch 900/910 - Loss: 0.0810\n",
      "  Batch 910/910 - Loss: 0.0562\n",
      "Training Loss: 0.0305\n",
      "Validation Loss: 0.0781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.01      0.02      4149\n",
      "         1.0       0.15      1.00      0.26       703\n",
      "\n",
      "    accuracy                           0.15      4852\n",
      "   macro avg       0.57      0.51      0.14      4852\n",
      "weighted avg       0.88      0.15      0.05      4852\n",
      "\n",
      "Epoch 3/20\n",
      "  Batch 10/910 - Loss: 0.0016\n",
      "  Batch 20/910 - Loss: 0.0058\n",
      "  Batch 30/910 - Loss: 0.0112\n",
      "  Batch 40/910 - Loss: 0.0028\n",
      "  Batch 50/910 - Loss: 0.0011\n",
      "  Batch 60/910 - Loss: 0.0006\n",
      "  Batch 70/910 - Loss: 0.0015\n",
      "  Batch 80/910 - Loss: 0.0034\n",
      "  Batch 90/910 - Loss: 0.0009\n",
      "  Batch 100/910 - Loss: 0.0011\n",
      "  Batch 110/910 - Loss: 0.0027\n",
      "  Batch 120/910 - Loss: 0.0600\n",
      "  Batch 130/910 - Loss: 0.0051\n",
      "  Batch 140/910 - Loss: 0.0016\n",
      "  Batch 150/910 - Loss: 0.0010\n",
      "  Batch 160/910 - Loss: 0.0602\n",
      "  Batch 170/910 - Loss: 0.0028\n",
      "  Batch 180/910 - Loss: 0.0012\n",
      "  Batch 190/910 - Loss: 0.0013\n",
      "  Batch 200/910 - Loss: 0.0648\n",
      "  Batch 210/910 - Loss: 0.0200\n",
      "  Batch 220/910 - Loss: 0.0040\n",
      "  Batch 230/910 - Loss: 0.0094\n",
      "  Batch 240/910 - Loss: 0.0606\n",
      "  Batch 250/910 - Loss: 0.0025\n",
      "  Batch 260/910 - Loss: 0.0011\n",
      "  Batch 270/910 - Loss: 0.0622\n",
      "  Batch 280/910 - Loss: 0.0011\n",
      "  Batch 290/910 - Loss: 0.0010\n",
      "  Batch 300/910 - Loss: 0.0582\n",
      "  Batch 310/910 - Loss: 0.0310\n",
      "  Batch 320/910 - Loss: 0.0643\n",
      "  Batch 330/910 - Loss: 0.0352\n",
      "  Batch 340/910 - Loss: 0.0664\n",
      "  Batch 350/910 - Loss: 0.0039\n",
      "  Batch 360/910 - Loss: 0.0367\n",
      "  Batch 370/910 - Loss: 0.0072\n",
      "  Batch 380/910 - Loss: 0.0020\n",
      "  Batch 390/910 - Loss: 0.0018\n",
      "  Batch 400/910 - Loss: 0.0037\n",
      "  Batch 410/910 - Loss: 0.0588\n",
      "  Batch 420/910 - Loss: 0.0010\n",
      "  Batch 430/910 - Loss: 0.0026\n",
      "  Batch 440/910 - Loss: 0.0031\n",
      "  Batch 450/910 - Loss: 0.0057\n",
      "  Batch 460/910 - Loss: 0.0052\n",
      "  Batch 470/910 - Loss: 0.0153\n",
      "  Batch 480/910 - Loss: 0.0095\n",
      "  Batch 490/910 - Loss: 0.0155\n",
      "  Batch 500/910 - Loss: 0.0148\n",
      "  Batch 510/910 - Loss: 0.0526\n",
      "  Batch 520/910 - Loss: 0.0017\n",
      "  Batch 530/910 - Loss: 0.0739\n",
      "  Batch 540/910 - Loss: 0.0078\n",
      "  Batch 550/910 - Loss: 0.0008\n",
      "  Batch 560/910 - Loss: 0.0010\n",
      "  Batch 570/910 - Loss: 0.0350\n",
      "  Batch 580/910 - Loss: 0.0032\n",
      "  Batch 590/910 - Loss: 0.0016\n",
      "  Batch 600/910 - Loss: 0.0046\n",
      "  Batch 610/910 - Loss: 0.0016\n",
      "  Batch 620/910 - Loss: 0.0097\n",
      "  Batch 630/910 - Loss: 0.0049\n",
      "  Batch 640/910 - Loss: 0.0584\n",
      "  Batch 650/910 - Loss: 0.0394\n",
      "  Batch 660/910 - Loss: 0.1147\n",
      "  Batch 670/910 - Loss: 0.0264\n",
      "  Batch 680/910 - Loss: 0.0608\n",
      "  Batch 690/910 - Loss: 0.0025\n",
      "  Batch 700/910 - Loss: 0.0008\n",
      "  Batch 710/910 - Loss: 0.0028\n",
      "  Batch 720/910 - Loss: 0.0004\n",
      "  Batch 730/910 - Loss: 0.0009\n",
      "  Batch 740/910 - Loss: 0.0620\n",
      "  Batch 750/910 - Loss: 0.0278\n",
      "  Batch 760/910 - Loss: 0.0048\n",
      "  Batch 770/910 - Loss: 0.0104\n",
      "  Batch 780/910 - Loss: 0.0613\n",
      "  Batch 790/910 - Loss: 0.0050\n",
      "  Batch 800/910 - Loss: 0.0029\n",
      "  Batch 810/910 - Loss: 0.0028\n",
      "  Batch 820/910 - Loss: 0.0088\n",
      "  Batch 830/910 - Loss: 0.1698\n",
      "  Batch 840/910 - Loss: 0.0095\n",
      "  Batch 850/910 - Loss: 0.0040\n",
      "  Batch 860/910 - Loss: 0.0073\n",
      "  Batch 870/910 - Loss: 0.0016\n",
      "  Batch 880/910 - Loss: 0.1060\n",
      "  Batch 890/910 - Loss: 0.0005\n",
      "  Batch 900/910 - Loss: 0.0030\n",
      "  Batch 910/910 - Loss: 0.0010\n",
      "Training Loss: 0.0232\n",
      "Validation Loss: 0.0821\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.65      0.79      4149\n",
      "         1.0       0.32      0.97      0.48       703\n",
      "\n",
      "    accuracy                           0.70      4852\n",
      "   macro avg       0.66      0.81      0.63      4852\n",
      "weighted avg       0.89      0.70      0.74      4852\n",
      "\n",
      "Epoch 4/20\n",
      "  Batch 10/910 - Loss: 0.0038\n",
      "  Batch 20/910 - Loss: 0.0050\n",
      "  Batch 30/910 - Loss: 0.0029\n",
      "  Batch 40/910 - Loss: 0.0149\n",
      "  Batch 50/910 - Loss: 0.0010\n",
      "  Batch 60/910 - Loss: 0.0022\n",
      "  Batch 70/910 - Loss: 0.0531\n",
      "  Batch 80/910 - Loss: 0.0024\n",
      "  Batch 90/910 - Loss: 0.0579\n",
      "  Batch 100/910 - Loss: 0.0030\n",
      "  Batch 110/910 - Loss: 0.0042\n",
      "  Batch 120/910 - Loss: 0.0032\n",
      "  Batch 130/910 - Loss: 0.0597\n",
      "  Batch 140/910 - Loss: 0.0047\n",
      "  Batch 150/910 - Loss: 0.0542\n",
      "  Batch 160/910 - Loss: 0.0009\n",
      "  Batch 170/910 - Loss: 0.0640\n",
      "  Batch 180/910 - Loss: 0.0004\n",
      "  Batch 190/910 - Loss: 0.0019\n",
      "  Batch 200/910 - Loss: 0.0016\n",
      "  Batch 210/910 - Loss: 0.0026\n",
      "  Batch 220/910 - Loss: 0.0030\n",
      "  Batch 230/910 - Loss: 0.0009\n",
      "  Batch 240/910 - Loss: 0.0010\n",
      "  Batch 250/910 - Loss: 0.0050\n",
      "  Batch 260/910 - Loss: 0.0033\n",
      "  Batch 270/910 - Loss: 0.0019\n",
      "  Batch 280/910 - Loss: 0.0387\n",
      "  Batch 290/910 - Loss: 0.0031\n",
      "  Batch 300/910 - Loss: 0.0031\n",
      "  Batch 310/910 - Loss: 0.0041\n",
      "  Batch 320/910 - Loss: 0.0010\n",
      "  Batch 330/910 - Loss: 0.0010\n",
      "  Batch 340/910 - Loss: 0.0019\n",
      "  Batch 350/910 - Loss: 0.0140\n",
      "  Batch 360/910 - Loss: 0.0897\n",
      "  Batch 370/910 - Loss: 0.0638\n",
      "  Batch 380/910 - Loss: 0.0038\n",
      "  Batch 390/910 - Loss: 0.0010\n",
      "  Batch 400/910 - Loss: 0.0090\n",
      "  Batch 410/910 - Loss: 0.0562\n",
      "  Batch 420/910 - Loss: 0.0007\n",
      "  Batch 430/910 - Loss: 0.0216\n",
      "  Batch 440/910 - Loss: 0.0064\n",
      "  Batch 450/910 - Loss: 0.0337\n",
      "  Batch 460/910 - Loss: 0.0131\n",
      "  Batch 470/910 - Loss: 0.0158\n",
      "  Batch 480/910 - Loss: 0.0122\n",
      "  Batch 490/910 - Loss: 0.0014\n",
      "  Batch 500/910 - Loss: 0.0028\n",
      "  Batch 510/910 - Loss: 0.0024\n",
      "  Batch 520/910 - Loss: 0.0579\n",
      "  Batch 530/910 - Loss: 0.0005\n",
      "  Batch 540/910 - Loss: 0.0012\n",
      "  Batch 550/910 - Loss: 0.0004\n",
      "  Batch 560/910 - Loss: 0.0029\n",
      "  Batch 570/910 - Loss: 0.0026\n",
      "  Batch 580/910 - Loss: 0.0187\n",
      "  Batch 590/910 - Loss: 0.0034\n",
      "  Batch 600/910 - Loss: 0.0559\n",
      "  Batch 610/910 - Loss: 0.0029\n",
      "  Batch 620/910 - Loss: 0.0032\n",
      "  Batch 630/910 - Loss: 0.0008\n",
      "  Batch 640/910 - Loss: 0.0374\n",
      "  Batch 650/910 - Loss: 0.0005\n",
      "  Batch 660/910 - Loss: 0.0324\n",
      "  Batch 670/910 - Loss: 0.0021\n",
      "  Batch 680/910 - Loss: 0.0021\n",
      "  Batch 690/910 - Loss: 0.0064\n",
      "  Batch 700/910 - Loss: 0.0011\n",
      "  Batch 710/910 - Loss: 0.0475\n",
      "  Batch 720/910 - Loss: 0.0137\n",
      "  Batch 730/910 - Loss: 0.0629\n",
      "  Batch 740/910 - Loss: 0.0613\n",
      "  Batch 750/910 - Loss: 0.0039\n",
      "  Batch 760/910 - Loss: 0.0018\n",
      "  Batch 770/910 - Loss: 0.0004\n",
      "  Batch 780/910 - Loss: 0.0081\n",
      "  Batch 790/910 - Loss: 0.0010\n",
      "  Batch 800/910 - Loss: 0.0018\n",
      "  Batch 810/910 - Loss: 0.0554\n",
      "  Batch 820/910 - Loss: 0.0042\n",
      "  Batch 830/910 - Loss: 0.0068\n",
      "  Batch 840/910 - Loss: 0.0027\n",
      "  Batch 850/910 - Loss: 0.0059\n",
      "  Batch 860/910 - Loss: 0.0023\n",
      "  Batch 870/910 - Loss: 0.0032\n",
      "  Batch 880/910 - Loss: 0.0999\n",
      "  Batch 890/910 - Loss: 0.0505\n",
      "  Batch 900/910 - Loss: 0.0169\n",
      "  Batch 910/910 - Loss: 0.0365\n",
      "Training Loss: 0.0186\n",
      "Validation Loss: 0.0766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.27      0.42      4149\n",
      "         1.0       0.18      0.95      0.31       703\n",
      "\n",
      "    accuracy                           0.37      4852\n",
      "   macro avg       0.58      0.61      0.36      4852\n",
      "weighted avg       0.86      0.37      0.41      4852\n",
      "\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 5/20\n",
      "  Batch 10/910 - Loss: 0.0134\n",
      "  Batch 20/910 - Loss: 0.0357\n",
      "  Batch 30/910 - Loss: 0.0022\n",
      "  Batch 40/910 - Loss: 0.0062\n",
      "  Batch 50/910 - Loss: 0.0025\n",
      "  Batch 60/910 - Loss: 0.0035\n",
      "  Batch 70/910 - Loss: 0.0004\n",
      "  Batch 80/910 - Loss: 0.0615\n",
      "  Batch 90/910 - Loss: 0.0075\n",
      "  Batch 100/910 - Loss: 0.0061\n",
      "  Batch 110/910 - Loss: 0.0250\n",
      "  Batch 120/910 - Loss: 0.0012\n",
      "  Batch 130/910 - Loss: 0.0573\n",
      "  Batch 140/910 - Loss: 0.0050\n",
      "  Batch 150/910 - Loss: 0.0031\n",
      "  Batch 160/910 - Loss: 0.0247\n",
      "  Batch 170/910 - Loss: 0.0032\n",
      "  Batch 180/910 - Loss: 0.0030\n",
      "  Batch 190/910 - Loss: 0.0019\n",
      "  Batch 200/910 - Loss: 0.0008\n",
      "  Batch 210/910 - Loss: 0.0041\n",
      "  Batch 220/910 - Loss: 0.0025\n",
      "  Batch 230/910 - Loss: 0.0056\n",
      "  Batch 240/910 - Loss: 0.0014\n",
      "  Batch 250/910 - Loss: 0.0039\n",
      "  Batch 260/910 - Loss: 0.0033\n",
      "  Batch 270/910 - Loss: 0.0012\n",
      "  Batch 280/910 - Loss: 0.0048\n",
      "  Batch 290/910 - Loss: 0.0025\n",
      "  Batch 300/910 - Loss: 0.0008\n",
      "  Batch 310/910 - Loss: 0.0038\n",
      "  Batch 320/910 - Loss: 0.0623\n",
      "  Batch 330/910 - Loss: 0.0040\n",
      "  Batch 340/910 - Loss: 0.0007\n",
      "  Batch 350/910 - Loss: 0.0007\n",
      "  Batch 360/910 - Loss: 0.0592\n",
      "  Batch 370/910 - Loss: 0.0016\n",
      "  Batch 380/910 - Loss: 0.0017\n",
      "  Batch 390/910 - Loss: 0.1023\n",
      "  Batch 400/910 - Loss: 0.0018\n",
      "  Batch 410/910 - Loss: 0.0045\n",
      "  Batch 420/910 - Loss: 0.0026\n",
      "  Batch 430/910 - Loss: 0.0010\n",
      "  Batch 440/910 - Loss: 0.0966\n",
      "  Batch 450/910 - Loss: 0.0480\n",
      "  Batch 460/910 - Loss: 0.0290\n",
      "  Batch 470/910 - Loss: 0.0009\n",
      "  Batch 480/910 - Loss: 0.0016\n",
      "  Batch 490/910 - Loss: 0.0052\n",
      "  Batch 500/910 - Loss: 0.0013\n",
      "  Batch 510/910 - Loss: 0.0028\n",
      "  Batch 520/910 - Loss: 0.0037\n",
      "  Batch 530/910 - Loss: 0.0741\n",
      "  Batch 540/910 - Loss: 0.0258\n",
      "  Batch 550/910 - Loss: 0.0012\n",
      "  Batch 560/910 - Loss: 0.0116\n",
      "  Batch 570/910 - Loss: 0.0059\n",
      "  Batch 580/910 - Loss: 0.0257\n",
      "  Batch 590/910 - Loss: 0.0150\n",
      "  Batch 600/910 - Loss: 0.0026\n",
      "  Batch 610/910 - Loss: 0.0777\n",
      "  Batch 620/910 - Loss: 0.0026\n",
      "  Batch 630/910 - Loss: 0.0030\n",
      "  Batch 640/910 - Loss: 0.0034\n",
      "  Batch 650/910 - Loss: 0.1032\n",
      "  Batch 660/910 - Loss: 0.0547\n",
      "  Batch 670/910 - Loss: 0.0010\n",
      "  Batch 680/910 - Loss: 0.0437\n",
      "  Batch 690/910 - Loss: 0.0043\n",
      "  Batch 700/910 - Loss: 0.0024\n",
      "  Batch 710/910 - Loss: 0.0008\n",
      "  Batch 720/910 - Loss: 0.0013\n",
      "  Batch 730/910 - Loss: 0.0021\n",
      "  Batch 740/910 - Loss: 0.0097\n",
      "  Batch 750/910 - Loss: 0.0041\n",
      "  Batch 760/910 - Loss: 0.0060\n",
      "  Batch 770/910 - Loss: 0.0032\n",
      "  Batch 780/910 - Loss: 0.0022\n",
      "  Batch 790/910 - Loss: 0.0014\n",
      "  Batch 800/910 - Loss: 0.0008\n",
      "  Batch 810/910 - Loss: 0.0088\n",
      "  Batch 820/910 - Loss: 0.0020\n",
      "  Batch 830/910 - Loss: 0.0711\n",
      "  Batch 840/910 - Loss: 0.0008\n",
      "  Batch 850/910 - Loss: 0.0033\n",
      "  Batch 860/910 - Loss: 0.0776\n",
      "  Batch 870/910 - Loss: 0.0787\n",
      "  Batch 880/910 - Loss: 0.0015\n",
      "  Batch 890/910 - Loss: 0.0060\n",
      "  Batch 900/910 - Loss: 0.0045\n",
      "  Batch 910/910 - Loss: 0.0028\n",
      "Training Loss: 0.0177\n",
      "Validation Loss: 0.0784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.89      0.93      4149\n",
      "         1.0       0.56      0.83      0.67       703\n",
      "\n",
      "    accuracy                           0.88      4852\n",
      "   macro avg       0.76      0.86      0.80      4852\n",
      "weighted avg       0.91      0.88      0.89      4852\n",
      "\n",
      "Epoch 6/20\n",
      "  Batch 10/910 - Loss: 0.0025\n",
      "  Batch 20/910 - Loss: 0.0028\n",
      "  Batch 30/910 - Loss: 0.0027\n",
      "  Batch 40/910 - Loss: 0.0027\n",
      "  Batch 50/910 - Loss: 0.0089\n",
      "  Batch 60/910 - Loss: 0.0048\n",
      "  Batch 70/910 - Loss: 0.0016\n",
      "  Batch 80/910 - Loss: 0.0074\n",
      "  Batch 90/910 - Loss: 0.0027\n",
      "  Batch 100/910 - Loss: 0.0028\n",
      "  Batch 110/910 - Loss: 0.0224\n",
      "  Batch 120/910 - Loss: 0.0068\n",
      "  Batch 130/910 - Loss: 0.0008\n",
      "  Batch 140/910 - Loss: 0.0021\n",
      "  Batch 150/910 - Loss: 0.0007\n",
      "  Batch 160/910 - Loss: 0.0009\n",
      "  Batch 170/910 - Loss: 0.0005\n",
      "  Batch 180/910 - Loss: 0.0007\n",
      "  Batch 190/910 - Loss: 0.0019\n",
      "  Batch 200/910 - Loss: 0.0017\n",
      "  Batch 210/910 - Loss: 0.0010\n",
      "  Batch 220/910 - Loss: 0.0074\n",
      "  Batch 230/910 - Loss: 0.0018\n",
      "  Batch 240/910 - Loss: 0.0008\n",
      "  Batch 250/910 - Loss: 0.0036\n",
      "  Batch 260/910 - Loss: 0.0006\n",
      "  Batch 270/910 - Loss: 0.0006\n",
      "  Batch 280/910 - Loss: 0.0007\n",
      "  Batch 290/910 - Loss: 0.0021\n",
      "  Batch 300/910 - Loss: 0.0032\n",
      "  Batch 310/910 - Loss: 0.0024\n",
      "  Batch 320/910 - Loss: 0.0037\n",
      "  Batch 330/910 - Loss: 0.0037\n",
      "  Batch 340/910 - Loss: 0.0028\n",
      "  Batch 350/910 - Loss: 0.0016\n",
      "  Batch 360/910 - Loss: 0.0034\n",
      "  Batch 370/910 - Loss: 0.0002\n",
      "  Batch 380/910 - Loss: 0.0221\n",
      "  Batch 390/910 - Loss: 0.0359\n",
      "  Batch 400/910 - Loss: 0.0739\n",
      "  Batch 410/910 - Loss: 0.0041\n",
      "  Batch 420/910 - Loss: 0.0019\n",
      "  Batch 430/910 - Loss: 0.0113\n",
      "  Batch 440/910 - Loss: 0.0099\n",
      "  Batch 450/910 - Loss: 0.0584\n",
      "  Batch 460/910 - Loss: 0.0083\n",
      "  Batch 470/910 - Loss: 0.0615\n",
      "  Batch 480/910 - Loss: 0.0111\n",
      "  Batch 490/910 - Loss: 0.0632\n",
      "  Batch 500/910 - Loss: 0.0305\n",
      "  Batch 510/910 - Loss: 0.0089\n",
      "  Batch 520/910 - Loss: 0.0025\n",
      "  Batch 530/910 - Loss: 0.0073\n",
      "  Batch 540/910 - Loss: 0.0010\n",
      "  Batch 550/910 - Loss: 0.0072\n",
      "  Batch 560/910 - Loss: 0.0616\n",
      "  Batch 570/910 - Loss: 0.0034\n",
      "  Batch 580/910 - Loss: 0.0005\n",
      "  Batch 590/910 - Loss: 0.0692\n",
      "  Batch 600/910 - Loss: 0.0212\n",
      "  Batch 610/910 - Loss: 0.0020\n",
      "  Batch 620/910 - Loss: 0.0583\n",
      "  Batch 630/910 - Loss: 0.0153\n",
      "  Batch 640/910 - Loss: 0.0092\n",
      "  Batch 650/910 - Loss: 0.0651\n",
      "  Batch 660/910 - Loss: 0.0007\n",
      "  Batch 670/910 - Loss: 0.0007\n",
      "  Batch 680/910 - Loss: 0.0031\n",
      "  Batch 690/910 - Loss: 0.0007\n",
      "  Batch 700/910 - Loss: 0.0005\n",
      "  Batch 710/910 - Loss: 0.0318\n",
      "  Batch 720/910 - Loss: 0.0106\n",
      "  Batch 730/910 - Loss: 0.0067\n",
      "  Batch 740/910 - Loss: 0.0036\n",
      "  Batch 750/910 - Loss: 0.0020\n",
      "  Batch 760/910 - Loss: 0.0023\n",
      "  Batch 770/910 - Loss: 0.0491\n",
      "  Batch 780/910 - Loss: 0.0137\n",
      "  Batch 790/910 - Loss: 0.0067\n",
      "  Batch 800/910 - Loss: 0.0015\n",
      "  Batch 810/910 - Loss: 0.0376\n",
      "  Batch 820/910 - Loss: 0.0532\n",
      "  Batch 830/910 - Loss: 0.0026\n",
      "  Batch 840/910 - Loss: 0.0130\n",
      "  Batch 850/910 - Loss: 0.0023\n",
      "  Batch 860/910 - Loss: 0.0061\n",
      "  Batch 870/910 - Loss: 0.0047\n",
      "  Batch 880/910 - Loss: 0.0050\n",
      "  Batch 890/910 - Loss: 0.0460\n",
      "  Batch 900/910 - Loss: 0.0022\n",
      "  Batch 910/910 - Loss: 0.0166\n",
      "Training Loss: 0.0147\n",
      "Validation Loss: 0.0792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.75      0.85      4149\n",
      "         1.0       0.39      0.94      0.55       703\n",
      "\n",
      "    accuracy                           0.77      4852\n",
      "   macro avg       0.69      0.84      0.70      4852\n",
      "weighted avg       0.90      0.77      0.81      4852\n",
      "\n",
      "Epoch 7/20\n",
      "  Batch 10/910 - Loss: 0.0040\n",
      "  Batch 20/910 - Loss: 0.0023\n",
      "  Batch 30/910 - Loss: 0.0457\n",
      "  Batch 40/910 - Loss: 0.0014\n",
      "  Batch 50/910 - Loss: 0.0010\n",
      "  Batch 60/910 - Loss: 0.0301\n",
      "  Batch 70/910 - Loss: 0.0017\n",
      "  Batch 80/910 - Loss: 0.0038\n",
      "  Batch 90/910 - Loss: 0.0019\n",
      "  Batch 100/910 - Loss: 0.0010\n",
      "  Batch 110/910 - Loss: 0.0004\n",
      "  Batch 120/910 - Loss: 0.0068\n",
      "  Batch 130/910 - Loss: 0.0013\n",
      "  Batch 140/910 - Loss: 0.0005\n",
      "  Batch 150/910 - Loss: 0.0050\n",
      "  Batch 160/910 - Loss: 0.0007\n",
      "  Batch 170/910 - Loss: 0.0005\n",
      "  Batch 180/910 - Loss: 0.0012\n",
      "  Batch 190/910 - Loss: 0.0013\n",
      "  Batch 200/910 - Loss: 0.0036\n",
      "  Batch 210/910 - Loss: 0.0008\n",
      "  Batch 220/910 - Loss: 0.0021\n",
      "  Batch 230/910 - Loss: 0.0006\n",
      "  Batch 240/910 - Loss: 0.1108\n",
      "  Batch 250/910 - Loss: 0.0020\n",
      "  Batch 260/910 - Loss: 0.0106\n",
      "  Batch 270/910 - Loss: 0.0020\n",
      "  Batch 280/910 - Loss: 0.0006\n",
      "  Batch 290/910 - Loss: 0.0151\n",
      "  Batch 300/910 - Loss: 0.0013\n",
      "  Batch 310/910 - Loss: 0.0048\n",
      "  Batch 320/910 - Loss: 0.0028\n",
      "  Batch 330/910 - Loss: 0.0610\n",
      "  Batch 340/910 - Loss: 0.0062\n",
      "  Batch 350/910 - Loss: 0.0027\n",
      "  Batch 360/910 - Loss: 0.0006\n",
      "  Batch 370/910 - Loss: 0.0018\n",
      "  Batch 380/910 - Loss: 0.0164\n",
      "  Batch 390/910 - Loss: 0.0104\n",
      "  Batch 400/910 - Loss: 0.0764\n",
      "  Batch 410/910 - Loss: 0.0028\n",
      "  Batch 420/910 - Loss: 0.0032\n",
      "  Batch 430/910 - Loss: 0.0449\n",
      "  Batch 440/910 - Loss: 0.0240\n",
      "  Batch 450/910 - Loss: 0.0009\n",
      "  Batch 460/910 - Loss: 0.0643\n",
      "  Batch 470/910 - Loss: 0.0015\n",
      "  Batch 480/910 - Loss: 0.0016\n",
      "  Batch 490/910 - Loss: 0.0022\n",
      "  Batch 500/910 - Loss: 0.0006\n",
      "  Batch 510/910 - Loss: 0.0243\n",
      "  Batch 520/910 - Loss: 0.0005\n",
      "  Batch 530/910 - Loss: 0.0032\n",
      "  Batch 540/910 - Loss: 0.0009\n",
      "  Batch 550/910 - Loss: 0.0175\n",
      "  Batch 560/910 - Loss: 0.0008\n",
      "  Batch 570/910 - Loss: 0.0003\n",
      "  Batch 580/910 - Loss: 0.0628\n",
      "  Batch 590/910 - Loss: 0.0004\n",
      "  Batch 600/910 - Loss: 0.0006\n",
      "  Batch 610/910 - Loss: 0.0052\n",
      "  Batch 620/910 - Loss: 0.0010\n",
      "  Batch 630/910 - Loss: 0.0003\n",
      "  Batch 640/910 - Loss: 0.0265\n",
      "  Batch 650/910 - Loss: 0.0394\n",
      "  Batch 660/910 - Loss: 0.0013\n",
      "  Batch 670/910 - Loss: 0.0017\n",
      "  Batch 680/910 - Loss: 0.0015\n",
      "  Batch 690/910 - Loss: 0.0008\n",
      "  Batch 700/910 - Loss: 0.0009\n",
      "  Batch 710/910 - Loss: 0.0328\n",
      "  Batch 720/910 - Loss: 0.0172\n",
      "  Batch 730/910 - Loss: 0.0045\n",
      "  Batch 740/910 - Loss: 0.0021\n",
      "  Batch 750/910 - Loss: 0.0009\n",
      "  Batch 760/910 - Loss: 0.0030\n",
      "  Batch 770/910 - Loss: 0.0364\n",
      "  Batch 780/910 - Loss: 0.0031\n",
      "  Batch 790/910 - Loss: 0.0499\n",
      "  Batch 800/910 - Loss: 0.0017\n",
      "  Batch 810/910 - Loss: 0.0006\n",
      "  Batch 820/910 - Loss: 0.0005\n",
      "  Batch 830/910 - Loss: 0.0022\n",
      "  Batch 840/910 - Loss: 0.0008\n",
      "  Batch 850/910 - Loss: 0.0524\n",
      "  Batch 860/910 - Loss: 0.0009\n",
      "  Batch 870/910 - Loss: 0.0010\n",
      "  Batch 880/910 - Loss: 0.0014\n",
      "  Batch 890/910 - Loss: 0.0047\n",
      "  Batch 900/910 - Loss: 0.0974\n",
      "  Batch 910/910 - Loss: 0.0005\n",
      "Training Loss: 0.0124\n",
      "Epoch 00007: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Validation Loss: 0.0826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.71      0.83      4149\n",
      "         1.0       0.36      0.95      0.52       703\n",
      "\n",
      "    accuracy                           0.74      4852\n",
      "   macro avg       0.67      0.83      0.67      4852\n",
      "weighted avg       0.90      0.74      0.78      4852\n",
      "\n",
      "Epoch 8/20\n",
      "  Batch 10/910 - Loss: 0.0615\n",
      "  Batch 20/910 - Loss: 0.0010\n",
      "  Batch 30/910 - Loss: 0.0010\n",
      "  Batch 40/910 - Loss: 0.0595\n",
      "  Batch 50/910 - Loss: 0.0007\n",
      "  Batch 60/910 - Loss: 0.0005\n",
      "  Batch 70/910 - Loss: 0.0009\n",
      "  Batch 80/910 - Loss: 0.0008\n",
      "  Batch 90/910 - Loss: 0.0288\n",
      "  Batch 100/910 - Loss: 0.0013\n",
      "  Batch 110/910 - Loss: 0.0009\n",
      "  Batch 120/910 - Loss: 0.0004\n",
      "  Batch 130/910 - Loss: 0.0343\n",
      "  Batch 140/910 - Loss: 0.0023\n",
      "  Batch 150/910 - Loss: 0.0018\n",
      "  Batch 160/910 - Loss: 0.0152\n",
      "  Batch 170/910 - Loss: 0.0011\n",
      "  Batch 180/910 - Loss: 0.0024\n",
      "  Batch 190/910 - Loss: 0.0042\n",
      "  Batch 200/910 - Loss: 0.0045\n",
      "  Batch 210/910 - Loss: 0.0005\n",
      "  Batch 220/910 - Loss: 0.0541\n",
      "  Batch 230/910 - Loss: 0.0173\n",
      "  Batch 240/910 - Loss: 0.0010\n",
      "  Batch 250/910 - Loss: 0.0137\n",
      "  Batch 260/910 - Loss: 0.0028\n",
      "  Batch 270/910 - Loss: 0.0015\n",
      "  Batch 280/910 - Loss: 0.0024\n",
      "  Batch 290/910 - Loss: 0.0005\n",
      "  Batch 300/910 - Loss: 0.0002\n",
      "  Batch 310/910 - Loss: 0.0009\n",
      "  Batch 320/910 - Loss: 0.0016\n",
      "  Batch 330/910 - Loss: 0.0005\n",
      "  Batch 340/910 - Loss: 0.0527\n",
      "  Batch 350/910 - Loss: 0.0007\n",
      "  Batch 360/910 - Loss: 0.0006\n",
      "  Batch 370/910 - Loss: 0.0006\n",
      "  Batch 380/910 - Loss: 0.0008\n",
      "  Batch 390/910 - Loss: 0.0637\n",
      "  Batch 400/910 - Loss: 0.0034\n",
      "  Batch 410/910 - Loss: 0.0007\n",
      "  Batch 420/910 - Loss: 0.0005\n",
      "  Batch 430/910 - Loss: 0.0651\n",
      "  Batch 440/910 - Loss: 0.0013\n",
      "  Batch 450/910 - Loss: 0.0630\n",
      "  Batch 460/910 - Loss: 0.0003\n",
      "  Batch 470/910 - Loss: 0.0013\n",
      "  Batch 480/910 - Loss: 0.0004\n",
      "  Batch 490/910 - Loss: 0.0010\n",
      "  Batch 500/910 - Loss: 0.0005\n",
      "  Batch 510/910 - Loss: 0.0024\n",
      "  Batch 520/910 - Loss: 0.0604\n",
      "  Batch 530/910 - Loss: 0.0032\n",
      "  Batch 540/910 - Loss: 0.0015\n",
      "  Batch 550/910 - Loss: 0.0013\n",
      "  Batch 560/910 - Loss: 0.0015\n",
      "  Batch 570/910 - Loss: 0.0679\n",
      "  Batch 580/910 - Loss: 0.0594\n",
      "  Batch 590/910 - Loss: 0.0007\n",
      "  Batch 600/910 - Loss: 0.0004\n",
      "  Batch 610/910 - Loss: 0.0010\n",
      "  Batch 620/910 - Loss: 0.0003\n",
      "  Batch 630/910 - Loss: 0.0006\n",
      "  Batch 640/910 - Loss: 0.0009\n",
      "  Batch 650/910 - Loss: 0.0011\n",
      "  Batch 660/910 - Loss: 0.0506\n",
      "  Batch 670/910 - Loss: 0.0033\n",
      "  Batch 680/910 - Loss: 0.0005\n",
      "  Batch 690/910 - Loss: 0.0048\n",
      "  Batch 700/910 - Loss: 0.0007\n",
      "  Batch 710/910 - Loss: 0.0007\n",
      "  Batch 720/910 - Loss: 0.0013\n",
      "  Batch 730/910 - Loss: 0.0003\n",
      "  Batch 740/910 - Loss: 0.0010\n",
      "  Batch 750/910 - Loss: 0.0008\n",
      "  Batch 760/910 - Loss: 0.0575\n",
      "  Batch 770/910 - Loss: 0.0008\n",
      "  Batch 780/910 - Loss: 0.0009\n",
      "  Batch 790/910 - Loss: 0.0006\n",
      "  Batch 800/910 - Loss: 0.0007\n",
      "  Batch 810/910 - Loss: 0.0007\n",
      "  Batch 820/910 - Loss: 0.0742\n",
      "  Batch 830/910 - Loss: 0.0011\n",
      "  Batch 840/910 - Loss: 0.0014\n",
      "  Batch 850/910 - Loss: 0.0018\n",
      "  Batch 860/910 - Loss: 0.0003\n",
      "  Batch 870/910 - Loss: 0.0004\n",
      "  Batch 880/910 - Loss: 0.0002\n",
      "  Batch 890/910 - Loss: 0.0106\n",
      "  Batch 900/910 - Loss: 0.0555\n",
      "  Batch 910/910 - Loss: 0.0530\n",
      "Training Loss: 0.0081\n",
      "Validation Loss: 0.0817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.18      0.31      4149\n",
      "         1.0       0.17      0.99      0.29       703\n",
      "\n",
      "    accuracy                           0.30      4852\n",
      "   macro avg       0.58      0.58      0.30      4852\n",
      "weighted avg       0.87      0.30      0.31      4852\n",
      "\n",
      "Epoch 9/20\n",
      "  Batch 10/910 - Loss: 0.0031\n",
      "  Batch 20/910 - Loss: 0.0035\n",
      "  Batch 30/910 - Loss: 0.0031\n",
      "  Batch 40/910 - Loss: 0.0005\n",
      "  Batch 50/910 - Loss: 0.0006\n",
      "  Batch 60/910 - Loss: 0.0012\n",
      "  Batch 70/910 - Loss: 0.0002\n",
      "  Batch 80/910 - Loss: 0.0005\n",
      "  Batch 90/910 - Loss: 0.0006\n",
      "  Batch 100/910 - Loss: 0.0459\n",
      "  Batch 110/910 - Loss: 0.0007\n",
      "  Batch 120/910 - Loss: 0.0575\n",
      "  Batch 130/910 - Loss: 0.0021\n",
      "  Batch 140/910 - Loss: 0.0002\n",
      "  Batch 150/910 - Loss: 0.0003\n",
      "  Batch 160/910 - Loss: 0.0035\n",
      "  Batch 170/910 - Loss: 0.0010\n",
      "  Batch 180/910 - Loss: 0.0011\n",
      "  Batch 190/910 - Loss: 0.0003\n",
      "  Batch 200/910 - Loss: 0.0007\n",
      "  Batch 210/910 - Loss: 0.0055\n",
      "  Batch 220/910 - Loss: 0.0031\n",
      "  Batch 230/910 - Loss: 0.0005\n",
      "  Batch 240/910 - Loss: 0.0009\n",
      "  Batch 250/910 - Loss: 0.0003\n",
      "  Batch 260/910 - Loss: 0.0011\n",
      "  Batch 270/910 - Loss: 0.0008\n",
      "  Batch 280/910 - Loss: 0.0008\n",
      "  Batch 290/910 - Loss: 0.0005\n",
      "  Batch 300/910 - Loss: 0.0006\n",
      "  Batch 310/910 - Loss: 0.0011\n",
      "  Batch 320/910 - Loss: 0.0011\n",
      "  Batch 330/910 - Loss: 0.0006\n",
      "  Batch 340/910 - Loss: 0.0191\n",
      "  Batch 350/910 - Loss: 0.0007\n",
      "  Batch 360/910 - Loss: 0.0007\n",
      "  Batch 370/910 - Loss: 0.0003\n",
      "  Batch 380/910 - Loss: 0.0009\n",
      "  Batch 390/910 - Loss: 0.0002\n",
      "  Batch 400/910 - Loss: 0.0010\n",
      "  Batch 410/910 - Loss: 0.0015\n",
      "  Batch 420/910 - Loss: 0.0005\n",
      "  Batch 430/910 - Loss: 0.0575\n",
      "  Batch 440/910 - Loss: 0.0038\n",
      "  Batch 450/910 - Loss: 0.0003\n",
      "  Batch 460/910 - Loss: 0.0004\n",
      "  Batch 470/910 - Loss: 0.0034\n",
      "  Batch 480/910 - Loss: 0.0004\n",
      "  Batch 490/910 - Loss: 0.0013\n",
      "  Batch 500/910 - Loss: 0.0004\n",
      "  Batch 510/910 - Loss: 0.0005\n",
      "  Batch 520/910 - Loss: 0.0004\n",
      "  Batch 530/910 - Loss: 0.0012\n",
      "  Batch 540/910 - Loss: 0.0004\n",
      "  Batch 550/910 - Loss: 0.0014\n",
      "  Batch 560/910 - Loss: 0.0012\n",
      "  Batch 570/910 - Loss: 0.0019\n",
      "  Batch 580/910 - Loss: 0.0031\n",
      "  Batch 590/910 - Loss: 0.0005\n",
      "  Batch 600/910 - Loss: 0.0010\n",
      "  Batch 610/910 - Loss: 0.0025\n",
      "  Batch 620/910 - Loss: 0.0029\n",
      "  Batch 630/910 - Loss: 0.0008\n",
      "  Batch 640/910 - Loss: 0.0012\n",
      "  Batch 650/910 - Loss: 0.0006\n",
      "  Batch 660/910 - Loss: 0.0006\n",
      "  Batch 670/910 - Loss: 0.0013\n",
      "  Batch 680/910 - Loss: 0.0004\n",
      "  Batch 690/910 - Loss: 0.0019\n",
      "  Batch 700/910 - Loss: 0.0025\n",
      "  Batch 710/910 - Loss: 0.0005\n",
      "  Batch 720/910 - Loss: 0.0007\n",
      "  Batch 730/910 - Loss: 0.0004\n",
      "  Batch 740/910 - Loss: 0.0008\n",
      "  Batch 750/910 - Loss: 0.0005\n",
      "  Batch 760/910 - Loss: 0.0009\n",
      "  Batch 770/910 - Loss: 0.0005\n",
      "  Batch 780/910 - Loss: 0.0009\n",
      "  Batch 790/910 - Loss: 0.0023\n",
      "  Batch 800/910 - Loss: 0.0275\n",
      "  Batch 810/910 - Loss: 0.0006\n",
      "  Batch 820/910 - Loss: 0.0005\n",
      "  Batch 830/910 - Loss: 0.0002\n",
      "  Batch 840/910 - Loss: 0.0537\n",
      "  Batch 850/910 - Loss: 0.0012\n",
      "  Batch 860/910 - Loss: 0.0009\n",
      "  Batch 870/910 - Loss: 0.0003\n",
      "  Batch 880/910 - Loss: 0.0015\n",
      "  Batch 890/910 - Loss: 0.0003\n",
      "  Batch 900/910 - Loss: 0.0017\n",
      "  Batch 910/910 - Loss: 0.0004\n",
      "Training Loss: 0.0049\n",
      "Validation Loss: 0.0835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.51      0.67      4149\n",
      "         1.0       0.24      0.92      0.38       703\n",
      "\n",
      "    accuracy                           0.57      4852\n",
      "   macro avg       0.61      0.72      0.53      4852\n",
      "weighted avg       0.87      0.57      0.63      4852\n",
      "\n",
      "Epoch 10/20\n",
      "  Batch 10/910 - Loss: 0.0021\n",
      "  Batch 20/910 - Loss: 0.0323\n",
      "  Batch 30/910 - Loss: 0.0012\n",
      "  Batch 40/910 - Loss: 0.0029\n",
      "  Batch 50/910 - Loss: 0.0006\n",
      "  Batch 60/910 - Loss: 0.0008\n",
      "  Batch 70/910 - Loss: 0.0006\n",
      "  Batch 80/910 - Loss: 0.0006\n",
      "  Batch 90/910 - Loss: 0.0012\n",
      "  Batch 100/910 - Loss: 0.0004\n",
      "  Batch 110/910 - Loss: 0.0027\n",
      "  Batch 120/910 - Loss: 0.0007\n",
      "  Batch 130/910 - Loss: 0.0008\n",
      "  Batch 140/910 - Loss: 0.0009\n",
      "  Batch 150/910 - Loss: 0.0003\n",
      "  Batch 160/910 - Loss: 0.0003\n",
      "  Batch 170/910 - Loss: 0.0005\n",
      "  Batch 180/910 - Loss: 0.0002\n",
      "  Batch 190/910 - Loss: 0.0003\n",
      "  Batch 200/910 - Loss: 0.0004\n",
      "  Batch 210/910 - Loss: 0.0006\n",
      "  Batch 220/910 - Loss: 0.0006\n",
      "  Batch 230/910 - Loss: 0.0006\n",
      "  Batch 240/910 - Loss: 0.0012\n",
      "  Batch 250/910 - Loss: 0.0006\n",
      "  Batch 260/910 - Loss: 0.0003\n",
      "  Batch 270/910 - Loss: 0.0006\n",
      "  Batch 280/910 - Loss: 0.0474\n",
      "  Batch 290/910 - Loss: 0.0009\n",
      "  Batch 300/910 - Loss: 0.0004\n",
      "  Batch 310/910 - Loss: 0.0004\n",
      "  Batch 320/910 - Loss: 0.0607\n",
      "  Batch 330/910 - Loss: 0.0005\n",
      "  Batch 340/910 - Loss: 0.0017\n",
      "  Batch 350/910 - Loss: 0.0164\n",
      "  Batch 360/910 - Loss: 0.0006\n",
      "  Batch 370/910 - Loss: 0.0013\n",
      "  Batch 380/910 - Loss: 0.0011\n",
      "  Batch 390/910 - Loss: 0.0007\n",
      "  Batch 400/910 - Loss: 0.0004\n",
      "  Batch 410/910 - Loss: 0.0007\n",
      "  Batch 420/910 - Loss: 0.0008\n",
      "  Batch 430/910 - Loss: 0.0057\n",
      "  Batch 440/910 - Loss: 0.0124\n",
      "  Batch 450/910 - Loss: 0.0002\n",
      "  Batch 460/910 - Loss: 0.0012\n",
      "  Batch 470/910 - Loss: 0.0007\n",
      "  Batch 480/910 - Loss: 0.0009\n",
      "  Batch 490/910 - Loss: 0.0003\n",
      "  Batch 500/910 - Loss: 0.0048\n",
      "  Batch 510/910 - Loss: 0.0023\n",
      "  Batch 520/910 - Loss: 0.0003\n",
      "  Batch 530/910 - Loss: 0.0005\n",
      "  Batch 540/910 - Loss: 0.0003\n",
      "  Batch 550/910 - Loss: 0.0022\n",
      "  Batch 560/910 - Loss: 0.0007\n",
      "  Batch 570/910 - Loss: 0.0003\n",
      "  Batch 580/910 - Loss: 0.0026\n",
      "  Batch 590/910 - Loss: 0.0003\n",
      "  Batch 600/910 - Loss: 0.0003\n",
      "  Batch 610/910 - Loss: 0.0008\n",
      "  Batch 620/910 - Loss: 0.0016\n",
      "  Batch 630/910 - Loss: 0.0002\n",
      "  Batch 640/910 - Loss: 0.0523\n",
      "  Batch 650/910 - Loss: 0.0005\n",
      "  Batch 660/910 - Loss: 0.0010\n",
      "  Batch 670/910 - Loss: 0.0003\n",
      "  Batch 680/910 - Loss: 0.0017\n",
      "  Batch 690/910 - Loss: 0.0021\n",
      "  Batch 700/910 - Loss: 0.0009\n",
      "  Batch 710/910 - Loss: 0.0010\n",
      "  Batch 720/910 - Loss: 0.0011\n",
      "  Batch 730/910 - Loss: 0.0003\n",
      "  Batch 740/910 - Loss: 0.0004\n",
      "  Batch 750/910 - Loss: 0.0017\n",
      "  Batch 760/910 - Loss: 0.0006\n",
      "  Batch 770/910 - Loss: 0.0507\n",
      "  Batch 780/910 - Loss: 0.0008\n",
      "  Batch 790/910 - Loss: 0.0004\n",
      "  Batch 800/910 - Loss: 0.0004\n",
      "  Batch 810/910 - Loss: 0.0003\n",
      "  Batch 820/910 - Loss: 0.0395\n",
      "  Batch 830/910 - Loss: 0.0004\n",
      "  Batch 840/910 - Loss: 0.0003\n",
      "  Batch 850/910 - Loss: 0.0219\n",
      "  Batch 860/910 - Loss: 0.0009\n",
      "  Batch 870/910 - Loss: 0.0012\n",
      "  Batch 880/910 - Loss: 0.0057\n",
      "  Batch 890/910 - Loss: 0.0034\n",
      "  Batch 900/910 - Loss: 0.0003\n",
      "  Batch 910/910 - Loss: 0.0013\n",
      "Training Loss: 0.0044\n",
      "Epoch 00010: reducing learning rate of group 0 to 8.0000e-07.\n",
      "Validation Loss: 0.0829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.56      0.71      4149\n",
      "         1.0       0.26      0.90      0.40       703\n",
      "\n",
      "    accuracy                           0.61      4852\n",
      "   macro avg       0.61      0.73      0.55      4852\n",
      "weighted avg       0.87      0.61      0.66      4852\n",
      "\n",
      "Epoch 11/20\n",
      "  Batch 10/910 - Loss: 0.0003\n",
      "  Batch 20/910 - Loss: 0.0061\n",
      "  Batch 30/910 - Loss: 0.0011\n",
      "  Batch 40/910 - Loss: 0.0008\n",
      "  Batch 50/910 - Loss: 0.0014\n",
      "  Batch 60/910 - Loss: 0.0004\n",
      "  Batch 70/910 - Loss: 0.0011\n",
      "  Batch 80/910 - Loss: 0.0005\n",
      "  Batch 90/910 - Loss: 0.0009\n",
      "  Batch 100/910 - Loss: 0.0005\n",
      "  Batch 110/910 - Loss: 0.0004\n",
      "  Batch 120/910 - Loss: 0.0003\n",
      "  Batch 130/910 - Loss: 0.0535\n",
      "  Batch 140/910 - Loss: 0.0003\n",
      "  Batch 150/910 - Loss: 0.0005\n",
      "  Batch 160/910 - Loss: 0.0007\n",
      "  Batch 170/910 - Loss: 0.0006\n",
      "  Batch 180/910 - Loss: 0.0004\n",
      "  Batch 190/910 - Loss: 0.0568\n",
      "  Batch 200/910 - Loss: 0.0004\n",
      "  Batch 210/910 - Loss: 0.0004\n",
      "  Batch 220/910 - Loss: 0.0012\n",
      "  Batch 230/910 - Loss: 0.0007\n",
      "  Batch 240/910 - Loss: 0.0016\n",
      "  Batch 250/910 - Loss: 0.0014\n",
      "  Batch 260/910 - Loss: 0.0015\n",
      "  Batch 270/910 - Loss: 0.0044\n",
      "  Batch 280/910 - Loss: 0.0015\n",
      "  Batch 290/910 - Loss: 0.0002\n",
      "  Batch 300/910 - Loss: 0.0005\n",
      "  Batch 310/910 - Loss: 0.0005\n",
      "  Batch 320/910 - Loss: 0.0006\n",
      "  Batch 330/910 - Loss: 0.0028\n",
      "  Batch 340/910 - Loss: 0.0009\n",
      "  Batch 350/910 - Loss: 0.0004\n",
      "  Batch 360/910 - Loss: 0.0056\n",
      "  Batch 370/910 - Loss: 0.0013\n",
      "  Batch 380/910 - Loss: 0.0021\n",
      "  Batch 390/910 - Loss: 0.0003\n",
      "  Batch 400/910 - Loss: 0.0413\n",
      "  Batch 410/910 - Loss: 0.0002\n",
      "  Batch 420/910 - Loss: 0.0015\n",
      "  Batch 430/910 - Loss: 0.0002\n",
      "  Batch 440/910 - Loss: 0.0007\n",
      "  Batch 450/910 - Loss: 0.0005\n",
      "  Batch 460/910 - Loss: 0.0003\n",
      "  Batch 470/910 - Loss: 0.0015\n",
      "  Batch 480/910 - Loss: 0.0011\n",
      "  Batch 490/910 - Loss: 0.0003\n",
      "  Batch 500/910 - Loss: 0.0014\n",
      "  Batch 510/910 - Loss: 0.0002\n",
      "  Batch 520/910 - Loss: 0.0006\n",
      "  Batch 530/910 - Loss: 0.0022\n",
      "  Batch 540/910 - Loss: 0.0586\n",
      "  Batch 550/910 - Loss: 0.0003\n",
      "  Batch 560/910 - Loss: 0.0005\n",
      "  Batch 570/910 - Loss: 0.0006\n",
      "  Batch 580/910 - Loss: 0.0003\n",
      "  Batch 590/910 - Loss: 0.0003\n",
      "  Batch 600/910 - Loss: 0.0326\n",
      "  Batch 610/910 - Loss: 0.0003\n",
      "  Batch 620/910 - Loss: 0.0004\n",
      "  Batch 630/910 - Loss: 0.0004\n",
      "  Batch 640/910 - Loss: 0.0003\n",
      "  Batch 650/910 - Loss: 0.0002\n",
      "  Batch 660/910 - Loss: 0.0003\n",
      "  Batch 670/910 - Loss: 0.0003\n",
      "  Batch 680/910 - Loss: 0.0003\n",
      "  Batch 690/910 - Loss: 0.0015\n",
      "  Batch 700/910 - Loss: 0.0004\n",
      "  Batch 710/910 - Loss: 0.0005\n",
      "  Batch 720/910 - Loss: 0.0008\n",
      "  Batch 730/910 - Loss: 0.0002\n",
      "  Batch 740/910 - Loss: 0.0008\n",
      "  Batch 750/910 - Loss: 0.0004\n",
      "  Batch 760/910 - Loss: 0.0007\n",
      "  Batch 770/910 - Loss: 0.0003\n",
      "  Batch 780/910 - Loss: 0.0001\n",
      "  Batch 790/910 - Loss: 0.0003\n",
      "  Batch 800/910 - Loss: 0.0003\n",
      "  Batch 810/910 - Loss: 0.0002\n",
      "  Batch 820/910 - Loss: 0.0008\n",
      "  Batch 830/910 - Loss: 0.0009\n",
      "  Batch 840/910 - Loss: 0.0015\n",
      "  Batch 850/910 - Loss: 0.0473\n",
      "  Batch 860/910 - Loss: 0.0007\n",
      "  Batch 870/910 - Loss: 0.0007\n",
      "  Batch 880/910 - Loss: 0.0005\n",
      "  Batch 890/910 - Loss: 0.0005\n",
      "  Batch 900/910 - Loss: 0.0006\n",
      "  Batch 910/910 - Loss: 0.0763\n",
      "Training Loss: 0.0035\n",
      "Validation Loss: 0.0824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.34      0.50      4149\n",
      "         1.0       0.19      0.92      0.32       703\n",
      "\n",
      "    accuracy                           0.42      4852\n",
      "   macro avg       0.58      0.63      0.41      4852\n",
      "weighted avg       0.85      0.42      0.47      4852\n",
      "\n",
      "Epoch 12/20\n",
      "  Batch 10/910 - Loss: 0.0564\n",
      "  Batch 20/910 - Loss: 0.0013\n",
      "  Batch 30/910 - Loss: 0.0472\n",
      "  Batch 40/910 - Loss: 0.0013\n",
      "  Batch 50/910 - Loss: 0.0404\n",
      "  Batch 60/910 - Loss: 0.0006\n",
      "  Batch 70/910 - Loss: 0.0006\n",
      "  Batch 80/910 - Loss: 0.0002\n",
      "  Batch 90/910 - Loss: 0.0003\n",
      "  Batch 100/910 - Loss: 0.0001\n",
      "  Batch 110/910 - Loss: 0.0023\n",
      "  Batch 120/910 - Loss: 0.0004\n",
      "  Batch 130/910 - Loss: 0.0019\n",
      "  Batch 140/910 - Loss: 0.0001\n",
      "  Batch 150/910 - Loss: 0.0006\n",
      "  Batch 160/910 - Loss: 0.0009\n",
      "  Batch 170/910 - Loss: 0.0005\n",
      "  Batch 180/910 - Loss: 0.0006\n",
      "  Batch 190/910 - Loss: 0.0027\n",
      "  Batch 200/910 - Loss: 0.0004\n",
      "  Batch 210/910 - Loss: 0.0003\n",
      "  Batch 220/910 - Loss: 0.0002\n",
      "  Batch 230/910 - Loss: 0.0007\n",
      "  Batch 240/910 - Loss: 0.0002\n",
      "  Batch 250/910 - Loss: 0.0010\n",
      "  Batch 260/910 - Loss: 0.0004\n",
      "  Batch 270/910 - Loss: 0.0003\n",
      "  Batch 280/910 - Loss: 0.0007\n",
      "  Batch 290/910 - Loss: 0.0009\n",
      "  Batch 300/910 - Loss: 0.0005\n",
      "  Batch 310/910 - Loss: 0.0003\n",
      "  Batch 320/910 - Loss: 0.0009\n",
      "  Batch 330/910 - Loss: 0.0006\n",
      "  Batch 340/910 - Loss: 0.0010\n",
      "  Batch 350/910 - Loss: 0.0004\n",
      "  Batch 360/910 - Loss: 0.0008\n",
      "  Batch 370/910 - Loss: 0.0005\n",
      "  Batch 380/910 - Loss: 0.0078\n",
      "  Batch 390/910 - Loss: 0.0011\n",
      "  Batch 400/910 - Loss: 0.0003\n",
      "  Batch 410/910 - Loss: 0.0010\n",
      "  Batch 420/910 - Loss: 0.0008\n",
      "  Batch 430/910 - Loss: 0.0007\n",
      "  Batch 440/910 - Loss: 0.0596\n",
      "  Batch 450/910 - Loss: 0.0003\n",
      "  Batch 460/910 - Loss: 0.0008\n",
      "  Batch 470/910 - Loss: 0.0013\n",
      "  Batch 480/910 - Loss: 0.0004\n",
      "  Batch 490/910 - Loss: 0.0023\n",
      "  Batch 500/910 - Loss: 0.0006\n",
      "  Batch 510/910 - Loss: 0.0001\n",
      "  Batch 520/910 - Loss: 0.0007\n",
      "  Batch 530/910 - Loss: 0.0009\n",
      "  Batch 540/910 - Loss: 0.0618\n",
      "  Batch 550/910 - Loss: 0.0035\n",
      "  Batch 560/910 - Loss: 0.0007\n",
      "  Batch 570/910 - Loss: 0.0002\n",
      "  Batch 580/910 - Loss: 0.0004\n",
      "  Batch 590/910 - Loss: 0.0004\n",
      "  Batch 600/910 - Loss: 0.0003\n",
      "  Batch 610/910 - Loss: 0.0005\n",
      "  Batch 620/910 - Loss: 0.0002\n",
      "  Batch 630/910 - Loss: 0.0008\n",
      "  Batch 640/910 - Loss: 0.0006\n",
      "  Batch 650/910 - Loss: 0.0005\n",
      "  Batch 660/910 - Loss: 0.0005\n",
      "  Batch 670/910 - Loss: 0.0003\n",
      "  Batch 680/910 - Loss: 0.0018\n",
      "  Batch 690/910 - Loss: 0.0004\n",
      "  Batch 700/910 - Loss: 0.0007\n",
      "  Batch 710/910 - Loss: 0.0016\n",
      "  Batch 720/910 - Loss: 0.0007\n",
      "  Batch 730/910 - Loss: 0.0003\n",
      "  Batch 740/910 - Loss: 0.0003\n",
      "  Batch 750/910 - Loss: 0.0011\n",
      "  Batch 760/910 - Loss: 0.0006\n",
      "  Batch 770/910 - Loss: 0.0006\n",
      "  Batch 780/910 - Loss: 0.0645\n",
      "  Batch 790/910 - Loss: 0.0004\n",
      "  Batch 800/910 - Loss: 0.0448\n",
      "  Batch 810/910 - Loss: 0.0010\n",
      "  Batch 820/910 - Loss: 0.0002\n",
      "  Batch 830/910 - Loss: 0.0004\n",
      "  Batch 840/910 - Loss: 0.0004\n",
      "  Batch 850/910 - Loss: 0.0008\n",
      "  Batch 860/910 - Loss: 0.0008\n",
      "  Batch 870/910 - Loss: 0.0008\n",
      "  Batch 880/910 - Loss: 0.0003\n",
      "  Batch 890/910 - Loss: 0.0003\n",
      "  Batch 900/910 - Loss: 0.0010\n",
      "  Batch 910/910 - Loss: 0.0003\n",
      "Training Loss: 0.0032\n",
      "Validation Loss: 0.0838\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.52      0.68      4149\n",
      "         1.0       0.24      0.90      0.38       703\n",
      "\n",
      "    accuracy                           0.57      4852\n",
      "   macro avg       0.60      0.71      0.53      4852\n",
      "weighted avg       0.86      0.57      0.63      4852\n",
      "\n",
      "Epoch 13/20\n",
      "  Batch 10/910 - Loss: 0.0009\n",
      "  Batch 20/910 - Loss: 0.0006\n",
      "  Batch 30/910 - Loss: 0.0010\n",
      "  Batch 40/910 - Loss: 0.0005\n",
      "  Batch 50/910 - Loss: 0.0002\n",
      "  Batch 60/910 - Loss: 0.0004\n",
      "  Batch 70/910 - Loss: 0.0006\n",
      "  Batch 80/910 - Loss: 0.0004\n",
      "  Batch 90/910 - Loss: 0.0004\n",
      "  Batch 100/910 - Loss: 0.0006\n",
      "  Batch 110/910 - Loss: 0.0007\n",
      "  Batch 120/910 - Loss: 0.0002\n",
      "  Batch 130/910 - Loss: 0.0083\n",
      "  Batch 140/910 - Loss: 0.0005\n",
      "  Batch 150/910 - Loss: 0.0231\n",
      "  Batch 160/910 - Loss: 0.0004\n",
      "  Batch 170/910 - Loss: 0.0007\n",
      "  Batch 180/910 - Loss: 0.0005\n",
      "  Batch 190/910 - Loss: 0.0007\n",
      "  Batch 200/910 - Loss: 0.0005\n",
      "  Batch 210/910 - Loss: 0.0005\n",
      "  Batch 220/910 - Loss: 0.0009\n",
      "  Batch 230/910 - Loss: 0.0005\n",
      "  Batch 240/910 - Loss: 0.0003\n",
      "  Batch 250/910 - Loss: 0.0008\n",
      "  Batch 260/910 - Loss: 0.0007\n",
      "  Batch 270/910 - Loss: 0.0003\n",
      "  Batch 280/910 - Loss: 0.0006\n",
      "  Batch 290/910 - Loss: 0.0003\n",
      "  Batch 300/910 - Loss: 0.0003\n",
      "  Batch 310/910 - Loss: 0.0003\n",
      "  Batch 320/910 - Loss: 0.0007\n",
      "  Batch 330/910 - Loss: 0.0004\n",
      "  Batch 340/910 - Loss: 0.0003\n",
      "  Batch 350/910 - Loss: 0.0004\n",
      "  Batch 360/910 - Loss: 0.0004\n",
      "  Batch 370/910 - Loss: 0.0013\n",
      "  Batch 380/910 - Loss: 0.0006\n",
      "  Batch 390/910 - Loss: 0.0011\n",
      "  Batch 400/910 - Loss: 0.0002\n",
      "  Batch 410/910 - Loss: 0.0003\n",
      "  Batch 420/910 - Loss: 0.0003\n",
      "  Batch 430/910 - Loss: 0.0003\n",
      "  Batch 440/910 - Loss: 0.0003\n",
      "  Batch 450/910 - Loss: 0.0002\n",
      "  Batch 460/910 - Loss: 0.0007\n",
      "  Batch 470/910 - Loss: 0.0002\n",
      "  Batch 480/910 - Loss: 0.0003\n",
      "  Batch 490/910 - Loss: 0.0010\n",
      "  Batch 500/910 - Loss: 0.0009\n",
      "  Batch 510/910 - Loss: 0.0017\n",
      "  Batch 520/910 - Loss: 0.0004\n",
      "  Batch 530/910 - Loss: 0.0007\n",
      "  Batch 540/910 - Loss: 0.0009\n",
      "  Batch 550/910 - Loss: 0.0002\n",
      "  Batch 560/910 - Loss: 0.0006\n",
      "  Batch 570/910 - Loss: 0.0007\n",
      "  Batch 580/910 - Loss: 0.0007\n",
      "  Batch 590/910 - Loss: 0.0003\n",
      "  Batch 600/910 - Loss: 0.0005\n",
      "  Batch 610/910 - Loss: 0.0004\n",
      "  Batch 620/910 - Loss: 0.0065\n",
      "  Batch 630/910 - Loss: 0.0002\n",
      "  Batch 640/910 - Loss: 0.0004\n",
      "  Batch 650/910 - Loss: 0.0003\n",
      "  Batch 660/910 - Loss: 0.0005\n",
      "  Batch 670/910 - Loss: 0.0003\n",
      "  Batch 680/910 - Loss: 0.0011\n",
      "  Batch 690/910 - Loss: 0.0004\n",
      "  Batch 700/910 - Loss: 0.0035\n",
      "  Batch 710/910 - Loss: 0.0006\n",
      "  Batch 720/910 - Loss: 0.0005\n",
      "  Batch 730/910 - Loss: 0.0005\n",
      "  Batch 740/910 - Loss: 0.0008\n",
      "  Batch 750/910 - Loss: 0.0335\n",
      "  Batch 760/910 - Loss: 0.0005\n",
      "  Batch 770/910 - Loss: 0.0004\n",
      "  Batch 780/910 - Loss: 0.0004\n",
      "  Batch 790/910 - Loss: 0.0007\n",
      "  Batch 800/910 - Loss: 0.0005\n",
      "  Batch 810/910 - Loss: 0.0008\n",
      "  Batch 820/910 - Loss: 0.0004\n",
      "  Batch 830/910 - Loss: 0.0003\n",
      "  Batch 840/910 - Loss: 0.0001\n",
      "  Batch 850/910 - Loss: 0.0006\n",
      "  Batch 860/910 - Loss: 0.0003\n",
      "  Batch 870/910 - Loss: 0.0011\n",
      "  Batch 880/910 - Loss: 0.0011\n",
      "  Batch 890/910 - Loss: 0.0003\n",
      "  Batch 900/910 - Loss: 0.0008\n",
      "  Batch 910/910 - Loss: 0.0004\n",
      "Training Loss: 0.0031\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.6000e-07.\n",
      "Validation Loss: 0.0835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.52      0.67      4149\n",
      "         1.0       0.24      0.90      0.38       703\n",
      "\n",
      "    accuracy                           0.57      4852\n",
      "   macro avg       0.60      0.71      0.53      4852\n",
      "weighted avg       0.86      0.57      0.63      4852\n",
      "\n",
      "Epoch 14/20\n",
      "  Batch 10/910 - Loss: 0.0011\n",
      "  Batch 20/910 - Loss: 0.0020\n",
      "  Batch 30/910 - Loss: 0.0006\n",
      "  Batch 40/910 - Loss: 0.0005\n",
      "  Batch 50/910 - Loss: 0.0006\n",
      "  Batch 60/910 - Loss: 0.0004\n",
      "  Batch 70/910 - Loss: 0.0012\n",
      "  Batch 80/910 - Loss: 0.0004\n",
      "  Batch 90/910 - Loss: 0.0003\n",
      "  Batch 100/910 - Loss: 0.0005\n",
      "  Batch 110/910 - Loss: 0.0004\n",
      "  Batch 120/910 - Loss: 0.0580\n",
      "  Batch 130/910 - Loss: 0.0006\n",
      "  Batch 140/910 - Loss: 0.0006\n",
      "  Batch 150/910 - Loss: 0.0008\n",
      "  Batch 160/910 - Loss: 0.0005\n",
      "  Batch 170/910 - Loss: 0.0007\n",
      "  Batch 180/910 - Loss: 0.0004\n",
      "  Batch 190/910 - Loss: 0.0005\n",
      "  Batch 200/910 - Loss: 0.0005\n",
      "  Batch 210/910 - Loss: 0.0003\n",
      "  Batch 220/910 - Loss: 0.0014\n",
      "  Batch 230/910 - Loss: 0.0004\n",
      "  Batch 240/910 - Loss: 0.0002\n",
      "  Batch 250/910 - Loss: 0.0003\n",
      "  Batch 260/910 - Loss: 0.0005\n",
      "  Batch 270/910 - Loss: 0.0017\n",
      "  Batch 280/910 - Loss: 0.0011\n",
      "  Batch 290/910 - Loss: 0.0005\n",
      "  Batch 300/910 - Loss: 0.0003\n",
      "  Batch 310/910 - Loss: 0.0002\n",
      "  Batch 320/910 - Loss: 0.0007\n",
      "  Batch 330/910 - Loss: 0.0010\n",
      "  Batch 340/910 - Loss: 0.0004\n",
      "  Batch 350/910 - Loss: 0.0003\n",
      "  Batch 360/910 - Loss: 0.0005\n",
      "  Batch 370/910 - Loss: 0.0007\n",
      "  Batch 380/910 - Loss: 0.0010\n",
      "  Batch 390/910 - Loss: 0.0007\n",
      "  Batch 400/910 - Loss: 0.0002\n",
      "  Batch 410/910 - Loss: 0.0002\n",
      "  Batch 420/910 - Loss: 0.0546\n",
      "  Batch 430/910 - Loss: 0.0003\n",
      "  Batch 440/910 - Loss: 0.0003\n",
      "  Batch 450/910 - Loss: 0.0007\n",
      "  Batch 460/910 - Loss: 0.0397\n",
      "  Batch 470/910 - Loss: 0.0003\n",
      "  Batch 480/910 - Loss: 0.0603\n",
      "  Batch 490/910 - Loss: 0.0011\n",
      "  Batch 500/910 - Loss: 0.0009\n",
      "  Batch 510/910 - Loss: 0.0004\n",
      "  Batch 520/910 - Loss: 0.0003\n",
      "  Batch 530/910 - Loss: 0.0007\n",
      "  Batch 540/910 - Loss: 0.0003\n",
      "  Batch 550/910 - Loss: 0.0003\n",
      "  Batch 560/910 - Loss: 0.0005\n",
      "  Batch 570/910 - Loss: 0.0002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     80\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 82\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m batch_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:428\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[1;32m--> 428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    430\u001b[0m ):\n\u001b[0;32m    431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m            used to compute the attr::tensors.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "balanced_df_24k['relevant'] = balanced_df_24k['relevant'].astype(int)  # Ensure labels are integers\n",
    "\n",
    "# Tokenizer and dataset preparation\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        combined_text = '[CLS]' + row['title'] + '[SEP]' + row['content'] + \"[SEP]\" + row['entities'] + '[SEP]' # Replace 'content' with 'article_content' if working with real data\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            # row['content'],\n",
    "            combined_text,  \n",
    "            None,\n",
    "            add_special_tokens=False, # set to false since they are added manually\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(row['relevant'], dtype=torch.float), \n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(train_df, tokenizer, max_length=128)\n",
    "val_dataset = TextDataset(val_df, tokenizer, max_length=128)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['relevant']), y=train_df['relevant'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "bert_model.to(device)\n",
    "\n",
    "# load from checkpoint\n",
    "checkpoint = torch.load('best_bert_model_epoch_1_BCEWithLogitsLoss.pt')\n",
    "bert_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Loss function and optimizer\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Scheduler: ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.2, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "current_epoch = 1\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(current_epoch, num_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    bert_model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device),\n",
    "        }\n",
    "\n",
    "        outputs = bert_model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    bert_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device),\n",
    "            }\n",
    "            outputs = bert_model(**inputs)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            # val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            preds = torch.sigmoid(logits).cpu().numpy() > 0.5  # threshold = 0.5 for  now\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(inputs['labels'].cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(classification_report(val_labels, val_preds))\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved. Saving model.\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': bert_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss\n",
    "    }, f\"best_bert_model_epoch_{epoch}_BCEWithLogitsLoss.pt\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to find a better threshold value for Bert\n",
    "We will use the val_df to decide. For now I am maximizing f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "bert_model.eval()\n",
    "y_probs, y_true = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device),\n",
    "        }\n",
    "        outputs = bert_model(**inputs)\n",
    "        val_loss += outputs.loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.sigmoid(logits).cpu().numpy()\n",
    "        y_probs.extend(preds)\n",
    "        y_true.extend(inputs['labels'].cpu().numpy())\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(recall, precision, color='b', label=\"Precision-Recall curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Compute F1 scores \n",
    "f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the performance with different threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(test_df, tokenizer, max_length=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "bert_model.eval()\n",
    "y_preds, y_true = [], []\n",
    "with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device),\n",
    "            }\n",
    "            outputs = bert_model(**inputs)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.sigmoid(logits).cpu().numpy() > 0.55 # testing threshold = 0.65 for now\n",
    "            y_preds.extend(preds)\n",
    "            y_true.extend(inputs['labels'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.96      0.93      4158\n",
      "         1.0       0.64      0.42      0.51       694\n",
      "\n",
      "    accuracy                           0.88      4852\n",
      "   macro avg       0.77      0.69      0.72      4852\n",
      "weighted avg       0.87      0.88      0.87      4852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: torch in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tanxe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Bert\n",
    "add a scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20\n",
      "  Batch 10/819 - Loss: 0.1362\n",
      "  Batch 20/819 - Loss: 0.0845\n",
      "  Batch 30/819 - Loss: 0.0592\n",
      "  Batch 40/819 - Loss: 0.0715\n",
      "  Batch 50/819 - Loss: 0.2071\n",
      "  Batch 60/819 - Loss: 0.1036\n",
      "  Batch 70/819 - Loss: 0.1277\n",
      "  Batch 80/819 - Loss: 0.0911\n",
      "  Batch 90/819 - Loss: 0.1165\n",
      "  Batch 100/819 - Loss: 0.1842\n",
      "  Batch 110/819 - Loss: 0.0982\n",
      "  Batch 120/819 - Loss: 0.1139\n",
      "  Batch 130/819 - Loss: 0.1173\n",
      "  Batch 140/819 - Loss: 0.0763\n",
      "  Batch 150/819 - Loss: 0.0658\n",
      "  Batch 160/819 - Loss: 0.1407\n",
      "  Batch 170/819 - Loss: 0.0840\n",
      "  Batch 180/819 - Loss: 0.1104\n",
      "  Batch 190/819 - Loss: 0.1104\n",
      "  Batch 200/819 - Loss: 0.0875\n",
      "  Batch 210/819 - Loss: 0.0400\n",
      "  Batch 220/819 - Loss: 0.0503\n",
      "  Batch 230/819 - Loss: 0.1097\n",
      "  Batch 240/819 - Loss: 0.1908\n",
      "  Batch 250/819 - Loss: 0.0308\n",
      "  Batch 260/819 - Loss: 0.1012\n",
      "  Batch 270/819 - Loss: 0.1203\n",
      "  Batch 280/819 - Loss: 0.1105\n",
      "  Batch 290/819 - Loss: 0.1238\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 105\u001b[0m\n\u001b[0;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Train each model on its own subset\u001b[39;00m\n\u001b[0;32m    104\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m    108\u001b[0m }\n\u001b[0;32m    110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    111\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of models in the ensemble\n",
    "ensemble_size = 5\n",
    "\n",
    "# Tokenizer and dataset preparation\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row['content'],  # Replace 'content' with 'article_content' if working with real data\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(row['relevant'], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = TextDataset(train_df, tokenizer, max_length=128)\n",
    "val_dataset = TextDataset(val_df, tokenizer, max_length=128)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['relevant']), y=train_df['relevant'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Split the dataset into subsets for each model\n",
    "\n",
    "# subset_datasets = []\n",
    "# subset_sizes = len(train_dataset) // ensemble_size\n",
    "\n",
    "# for i in range(ensemble_size):\n",
    "#     start_idx = i * subset_sizes\n",
    "#     end_idx = (i + 1) * subset_sizes if i < ensemble_size - 1 else len(train_dataset)\n",
    "    \n",
    "#     subset = torch.utils.data.Subset(train_dataset, range(start_idx, end_idx))\n",
    "#     subset_datasets.append(subset)\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=ensemble_size, test_size=None, random_state=42)\n",
    "\n",
    "# List to hold the subsets for each model\n",
    "subset_datasets = []\n",
    "\n",
    "# Perform stratified splitting\n",
    "for train_idx, _ in splitter.split(train_df, train_df['relevant']):\n",
    "    subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "    subset_datasets.append(subset)\n",
    "\n",
    "# Initialize multiple BERT models for the ensemble\n",
    "ensemble_models = []\n",
    "for i in range(ensemble_size):\n",
    "    bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "    bert_model.to(device)\n",
    "    ensemble_models.append(bert_model)\n",
    "\n",
    "# Optimizer for the ensemble models\n",
    "optimizer = AdamW([param for model in ensemble_models for param in model.parameters()], lr=2e-5)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "\n",
    "# Validation DataLoader\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "current_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(current_epoch, num_epochs):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    \n",
    "    # Train each model in the ensemble on a different subset of the data\n",
    "    for i, model in enumerate(ensemble_models):\n",
    "        model.train()\n",
    "        train_subset_dataloader = DataLoader(subset_datasets[i], batch_size=16, shuffle=True)\n",
    "        train_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_subset_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Train each model on its own subset\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device),\n",
    "            }\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0 or batch_idx == len(train_subset_dataloader) - 1:\n",
    "                print(f\"  Batch {batch_idx + 1}/{len(train_subset_dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss /= len(train_subset_dataloader)\n",
    "        print(f\"Training Loss for model {i + 1}: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch_preds = []\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device),\n",
    "            }\n",
    "            for model in ensemble_models:\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                batch_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "\n",
    "            # Average the predictions from all models\n",
    "            batch_preds = torch.stack([torch.tensor(pred) for pred in batch_preds])\n",
    "            avg_preds = torch.mean(batch_preds, dim=0)\n",
    "            preds = (avg_preds.squeeze() > 0.5).numpy()  # threshold\n",
    "\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(inputs['labels'].cpu().numpy())\n",
    "\n",
    "            # Calculate validation loss for each batch and accumulate it\n",
    "            batch_loss = 0\n",
    "            for model in ensemble_models:\n",
    "                outputs = model(**inputs)\n",
    "                batch_loss += loss_fn(outputs.logits, inputs['labels'])\n",
    "            batch_loss /= len(ensemble_models)  # Average the loss\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "    # Calculate validation loss and metrics\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(classification_report(val_labels, val_preds))\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved. Saving model.\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': [model.state_dict() for model in ensemble_models],\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, f\"best_ensemble_model_epoch_{epoch}_BCEWithLogitsLoss.pt\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
