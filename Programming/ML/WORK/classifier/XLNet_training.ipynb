{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>full_content</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89541</td>\n",
       "      <td>International Business Times</td>\n",
       "      <td>UN Chief Urges World To 'Stop The Madness' Of ...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89542</td>\n",
       "      <td>Prtimes.jp</td>\n",
       "      <td>RANDEBOOよりワンランク上の大人っぽさが漂うニットとベストが新登場。</td>\n",
       "      <td>[株式会社Ainer]\\nRANDEBOO（ランデブー）では2023年7月18日(火)より公...</td>\n",
       "      <td>RANDEBOO2023718()WEB2023 Autumn Winter \\n\"Nepa...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89543</td>\n",
       "      <td>VOA News</td>\n",
       "      <td>UN Chief Urges World to 'Stop the Madness' of ...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>Kathmandu, Nepal  UN Secretary-General Antonio...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89545</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Sikkim warning: Hydroelectricity push must be ...</td>\n",
       "      <td>Ecologists caution against the adverse effects...</td>\n",
       "      <td>At least 14 persons lost their lives and more ...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>At least 14 persons lost their lives and more ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89547</td>\n",
       "      <td>The Times of Israel</td>\n",
       "      <td>200 foreigners, dual nationals cut down in Ham...</td>\n",
       "      <td>France lost 35 citizens, Thailand 33, US 31, U...</td>\n",
       "      <td>Scores of foreign citizens were killed, taken ...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105370</th>\n",
       "      <td>781108</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Have done no wrong, only did party work, says ...</td>\n",
       "      <td>The High Court today allowed Shivakumar to wit...</td>\n",
       "      <td>Karnataka Deputy Chief Minister D K Shivakumar...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Karnataka Deputy Chief Minister D K Shivakumar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105371</th>\n",
       "      <td>781129</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>FC Barcelona Guarantees $77.6 Million Champion...</td>\n",
       "      <td>FC Barcelona have guaranteed at least $77.6 mi...</td>\n",
       "      <td>FC Barcelona have guaranteed at least $767.6 m...</td>\n",
       "      <td>Home</td>\n",
       "      <td>FC Barcelona have guaranteed at least $767.6 m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105372</th>\n",
       "      <td>781235</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Three hospitals ignored her gravely ill fiancé...</td>\n",
       "      <td>Forty years ago, Sarah Lubarsky came home from...</td>\n",
       "      <td>The photo from David and Sarah Lubarsky's wedd...</td>\n",
       "      <td>Home</td>\n",
       "      <td>The photo from David and Sarah Lubarsky's wedd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105373</th>\n",
       "      <td>781240</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Kerber’s Farm: Bringing Farm To Table To Manha...</td>\n",
       "      <td>A farmstand in Long Island, Kerber’s Farms has...</td>\n",
       "      <td>Kerbers Farm: Bringing Farm To Table To Manhat...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Kerber’s Farm: Bringing Farm To Table To Manha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105374</th>\n",
       "      <td>781308</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Tips For Investing In Short-Term Rentals In Dubai</td>\n",
       "      <td>By exploring your options and keeping a few be...</td>\n",
       "      <td>Cofounder at UpperKey. Passionate about proper...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Cofounder at UpperKey. Passionate about proper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105375 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id                   source_name  \\\n",
       "0            89541  International Business Times   \n",
       "1            89542                    Prtimes.jp   \n",
       "2            89543                      VOA News   \n",
       "3            89545            The Indian Express   \n",
       "4            89547           The Times of Israel   \n",
       "...            ...                           ...   \n",
       "105370      781108            The Indian Express   \n",
       "105371      781129                        Forbes   \n",
       "105372      781235                           NPR   \n",
       "105373      781240                        Forbes   \n",
       "105374      781308                        Forbes   \n",
       "\n",
       "                                                    title  \\\n",
       "0       UN Chief Urges World To 'Stop The Madness' Of ...   \n",
       "1                   RANDEBOOよりワンランク上の大人っぽさが漂うニットとベストが新登場。   \n",
       "2       UN Chief Urges World to 'Stop the Madness' of ...   \n",
       "3       Sikkim warning: Hydroelectricity push must be ...   \n",
       "4       200 foreigners, dual nationals cut down in Ham...   \n",
       "...                                                   ...   \n",
       "105370  Have done no wrong, only did party work, says ...   \n",
       "105371  FC Barcelona Guarantees $77.6 Million Champion...   \n",
       "105372  Three hospitals ignored her gravely ill fiancé...   \n",
       "105373  Kerber’s Farm: Bringing Farm To Table To Manha...   \n",
       "105374  Tips For Investing In Short-Term Rentals In Dubai   \n",
       "\n",
       "                                              description  \\\n",
       "0       UN Secretary-General Antonio Guterres urged th...   \n",
       "1       [株式会社Ainer]\\nRANDEBOO（ランデブー）では2023年7月18日(火)より公...   \n",
       "2       UN Secretary-General Antonio Guterres urged th...   \n",
       "3       Ecologists caution against the adverse effects...   \n",
       "4       France lost 35 citizens, Thailand 33, US 31, U...   \n",
       "...                                                   ...   \n",
       "105370  The High Court today allowed Shivakumar to wit...   \n",
       "105371  FC Barcelona have guaranteed at least $77.6 mi...   \n",
       "105372  Forty years ago, Sarah Lubarsky came home from...   \n",
       "105373  A farmstand in Long Island, Kerber’s Farms has...   \n",
       "105374  By exploring your options and keeping a few be...   \n",
       "\n",
       "                                                  content category  \\\n",
       "0       UN Secretary-General Antonio Guterres urged th...    Nepal   \n",
       "1       RANDEBOO2023718()WEB2023 Autumn Winter \\n\"Nepa...    Nepal   \n",
       "2       Kathmandu, Nepal  UN Secretary-General Antonio...    Nepal   \n",
       "3       At least 14 persons lost their lives and more ...    Nepal   \n",
       "4       Scores of foreign citizens were killed, taken ...    Nepal   \n",
       "...                                                   ...      ...   \n",
       "105370  Karnataka Deputy Chief Minister D K Shivakumar...     Home   \n",
       "105371  FC Barcelona have guaranteed at least $767.6 m...     Home   \n",
       "105372  The photo from David and Sarah Lubarsky's wedd...     Home   \n",
       "105373  Kerbers Farm: Bringing Farm To Table To Manhat...     Home   \n",
       "105374  Cofounder at UpperKey. Passionate about proper...     Home   \n",
       "\n",
       "                                             full_content  relevant  \n",
       "0       UN Secretary-General Antonio Guterres urged th...         0  \n",
       "1                                                     NaN         0  \n",
       "2                                                     NaN         0  \n",
       "3       At least 14 persons lost their lives and more ...         0  \n",
       "4                                                     NaN         0  \n",
       "...                                                   ...       ...  \n",
       "105370  Karnataka Deputy Chief Minister D K Shivakumar...         0  \n",
       "105371  FC Barcelona have guaranteed at least $767.6 m...         0  \n",
       "105372  The photo from David and Sarah Lubarsky's wedd...         0  \n",
       "105373  Kerber’s Farm: Bringing Farm To Table To Manha...         0  \n",
       "105374  Cofounder at UpperKey. Passionate about proper...         0  \n",
       "\n",
       "[105375 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'C:\\\\Users\\\\tanxe\\\\Programming\\\\ML\\\\WORK\\\\classifier\\\\data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(columns=['source_id', 'author', 'published_at', 'url_to_image', 'url' ])\n",
    "filtered_df = df\n",
    "filtered_df['relevant'] = filtered_df['category'].apply(lambda x: 1 if x == 'Stock' or x == 'Finance' else 0)\n",
    "df_cleaned = filtered_df.dropna(subset=['content'])\n",
    "balanced_df = df_cleaned\n",
    "balanced_df\n",
    "\n",
    "# filtered_df = df[df['source_name'].isin(['GlobeNewswire', 'The Times of India'])]\n",
    "\n",
    "# filtered_df['relevant'] = filtered_df['category'].apply(lambda x: 1 if x == 'COVID' else 0)\n",
    "# df_cleaned = filtered_df.dropna(subset=['full_content'])\n",
    "# df_relevant_zero = df_cleaned[df_cleaned['relevant'] == 0]\n",
    "# df_relevant_one = df_cleaned[df_cleaned['relevant'] == 1]\n",
    "# df_sampled = df_relevant_zero.sample(n=1400, random_state=42)\n",
    "# balanced_df = pd.concat([df_sampled, df_relevant_one], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanxe\\AppData\\Local\\Temp\\ipykernel_2056\\2424080986.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  balanced_df_24k['relevant'] = balanced_df_24k['category'].apply(lambda x: 1 if x == 'Stock' else 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "relevant\n",
       "0    20757\n",
       "1     3503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df_24k = balanced_df[balanced_df['source_name'].isin([\"ETF Daily News\", \"The Times of India\"])]\n",
    "balanced_df_24k['relevant'] = balanced_df_24k['category'].apply(lambda x: 1 if x == 'Stock' else 0)\n",
    "balanced_df_24k = balanced_df_24k.dropna(subset=['content'])\n",
    "balanced_df_24k['relevant'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "text = balanced_df['full_content'][0]\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# nlp = spacy.load('en_core_web_trf') may be better in the office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = balanced_df['full_content'][0]\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UN ORG\n",
      "Antonio Guterres PERSON\n",
      "Monday DATE\n",
      "Himalayan NORP\n",
      "Guterres PERSON\n",
      "Everest LOC\n",
      "Nepal GPE\n",
      "nearly a third CARDINAL\n",
      "just over three decades DATE\n",
      "Himalayas LOC\n",
      "Nepal GPE\n",
      "65 percent PERCENT\n",
      "the last decade DATE\n",
      "Guterres PERSON\n",
      "four-day DATE\n",
      "Nepal GPE\n",
      "Himalayan NORP\n",
      "Hindu NORP\n",
      "around 240 million CARDINAL\n",
      "1.65 billion CARDINAL\n",
      "South Asian NORP\n",
      "Southeast Asian NORP\n",
      "10 CARDINAL\n",
      "Ganges NORP\n",
      "Indus GPE\n",
      "Yellow GPE\n",
      "Mekong GPE\n",
      "Irrawaddy GPE\n",
      "billions CARDINAL\n",
      "today DATE\n",
      "Guterres PERSON\n",
      "Syangboche village GPE\n",
      "Everest LOC\n",
      "nearly 1.2 degrees Celsius QUANTITY\n",
      "the mid-1800s DATE\n",
      "1.5 degrees QUANTITY\n",
      "Guterres PERSON\n",
      "first ORDINAL\n",
      "Himalayan NORP\n",
      "Indus GPE\n",
      "Ganges ORG\n",
      "Brahmaputra ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a seperate column for the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities_plus_labels = [f\"{ent}_{ent.label}\" for ent in doc.ents]\n",
    "    return \" \".join(entities_plus_labels)\n",
    "\n",
    "\n",
    "balanced_df_24k['entities'] = balanced_df_24k['content'].apply(extract_entities)\n",
    "balanced_df_24k['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>full_content</th>\n",
       "      <th>relevant</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3109</th>\n",
       "      <td>94343</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>These 9 commodity stocks hit 52-week high on T...</td>\n",
       "      <td>During Thursday's trading session, the Sensex ...</td>\n",
       "      <td>Nov 02, 2023, 07:22:41 PM IST\\nDuring Thursday...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Nov 02_391 2023_391 07:22:41 PM_392 Thursday_3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57910</th>\n",
       "      <td>133924</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Fundamental Radar: Varun Beverages poised to b...</td>\n",
       "      <td>Varun Beverages Ltd is the second-largest fran...</td>\n",
       "      <td>SynopsisVarun Beverages Ltd is the second-larg...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>SynopsisVarun Beverages Ltd_383 second_396 Pep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57935</th>\n",
       "      <td>134021</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Stock market update: Mining stocks up as marke...</td>\n",
       "      <td>The 30-share BSE Sensex was  up  425.32 points...</td>\n",
       "      <td>NEW DELHI: Mining stocks were trading higher o...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Getty Images Nifty moved in a tight range of 8...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEW DELHI_384 Friday_391 10:09AM_397 Oriental_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57936</th>\n",
       "      <td>134022</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Stock market update: Fertilisers stocks up as ...</td>\n",
       "      <td>The 30-share BSE Sensex was  up  441.31 points...</td>\n",
       "      <td>NEW DELHI: Fertilisers stocks were trading hig...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Getty Images NEW DELHI: Fertilisers stocks wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEW DELHI_384 Fertilisers_380 Friday_391 Bohra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57937</th>\n",
       "      <td>134023</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>InterGlobe stock price up 0.08 per cent as Sen...</td>\n",
       "      <td>As of 30-Sep-2023, promoters held 38.02 per ce...</td>\n",
       "      <td>Shares of InterGlobe Aviation Ltd. rose 0.08 p...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Reuters On an immediate basis, 15,770/52,500 a...</td>\n",
       "      <td>1</td>\n",
       "      <td>InterGlobe Aviation Ltd._383 0.08 per cent_394...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102076</th>\n",
       "      <td>693939</td>\n",
       "      <td>ETF Daily News</td>\n",
       "      <td>Universal (NYSE:UVV) vs. British American Toba...</td>\n",
       "      <td>Universal (NYSE:UVV – Get Free Report) and Bri...</td>\n",
       "      <td>Universal (NYSE:UVV – Get Free Report) and Bri...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Universal (NYSE:UVV–Get Free Report) and Briti...</td>\n",
       "      <td>1</td>\n",
       "      <td>NYSE_383 UVV_383 British American Tobacco_383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102077</th>\n",
       "      <td>693944</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Do we have enough retail money in debt markets?</td>\n",
       "      <td>​​For example, as per the monthly data release...</td>\n",
       "      <td>Generally, the retail investors are late to th...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>IANS INSIGHTS  \\n    \\t                    Rea...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102078</th>\n",
       "      <td>693947</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>FII action, OPEC+ meet among top 10 factors to...</td>\n",
       "      <td>Meena expects the market to experience some di...</td>\n",
       "      <td>Indian frontline indices S&amp;amp;P BSE Sensex an...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>ETMarkets.com Indian frontline indices S&amp;P BSE...</td>\n",
       "      <td>1</td>\n",
       "      <td>Indian_381 BSE Sensex_383 Nifty50_383 Friday_3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102079</th>\n",
       "      <td>693954</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>For workers at this iPhone plant, Tata means a...</td>\n",
       "      <td>At the Narasapura facility, the recent takeove...</td>\n",
       "      <td>It is the Tata tag we aim for, who doesnt want...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>“It is the  Tata  tag we aim for, who doesn’t ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tata_383 Tata_383 iPhone_383 Narasapura_384 Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102820</th>\n",
       "      <td>711912</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Riding on the digitization of Indian capital m...</td>\n",
       "      <td>\"The BSE derivatives market share jumped to 14...</td>\n",
       "      <td>Stating Indian exchanges are benefiting from f...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Reuters Bombay Stock Exchange Related FII acti...</td>\n",
       "      <td>1</td>\n",
       "      <td>Indian_381 innovations &amp;amp_383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3503 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id         source_name  \\\n",
       "3109         94343  The Times of India   \n",
       "57910       133924  The Times of India   \n",
       "57935       134021  The Times of India   \n",
       "57936       134022  The Times of India   \n",
       "57937       134023  The Times of India   \n",
       "...            ...                 ...   \n",
       "102076      693939      ETF Daily News   \n",
       "102077      693944  The Times of India   \n",
       "102078      693947  The Times of India   \n",
       "102079      693954  The Times of India   \n",
       "102820      711912  The Times of India   \n",
       "\n",
       "                                                    title  \\\n",
       "3109    These 9 commodity stocks hit 52-week high on T...   \n",
       "57910   Fundamental Radar: Varun Beverages poised to b...   \n",
       "57935   Stock market update: Mining stocks up as marke...   \n",
       "57936   Stock market update: Fertilisers stocks up as ...   \n",
       "57937   InterGlobe stock price up 0.08 per cent as Sen...   \n",
       "...                                                   ...   \n",
       "102076  Universal (NYSE:UVV) vs. British American Toba...   \n",
       "102077    Do we have enough retail money in debt markets?   \n",
       "102078  FII action, OPEC+ meet among top 10 factors to...   \n",
       "102079  For workers at this iPhone plant, Tata means a...   \n",
       "102820  Riding on the digitization of Indian capital m...   \n",
       "\n",
       "                                              description  \\\n",
       "3109    During Thursday's trading session, the Sensex ...   \n",
       "57910   Varun Beverages Ltd is the second-largest fran...   \n",
       "57935   The 30-share BSE Sensex was  up  425.32 points...   \n",
       "57936   The 30-share BSE Sensex was  up  441.31 points...   \n",
       "57937   As of 30-Sep-2023, promoters held 38.02 per ce...   \n",
       "...                                                   ...   \n",
       "102076  Universal (NYSE:UVV – Get Free Report) and Bri...   \n",
       "102077  ​​For example, as per the monthly data release...   \n",
       "102078  Meena expects the market to experience some di...   \n",
       "102079  At the Narasapura facility, the recent takeove...   \n",
       "102820  \"The BSE derivatives market share jumped to 14...   \n",
       "\n",
       "                                                  content category  \\\n",
       "3109    Nov 02, 2023, 07:22:41 PM IST\\nDuring Thursday...    Stock   \n",
       "57910   SynopsisVarun Beverages Ltd is the second-larg...    Stock   \n",
       "57935   NEW DELHI: Mining stocks were trading higher o...    Stock   \n",
       "57936   NEW DELHI: Fertilisers stocks were trading hig...    Stock   \n",
       "57937   Shares of InterGlobe Aviation Ltd. rose 0.08 p...    Stock   \n",
       "...                                                   ...      ...   \n",
       "102076  Universal (NYSE:UVV – Get Free Report) and Bri...    Stock   \n",
       "102077  Generally, the retail investors are late to th...    Stock   \n",
       "102078  Indian frontline indices S&amp;P BSE Sensex an...    Stock   \n",
       "102079  It is the Tata tag we aim for, who doesnt want...    Stock   \n",
       "102820  Stating Indian exchanges are benefiting from f...    Stock   \n",
       "\n",
       "                                             full_content  relevant  \\\n",
       "3109                                                  NaN         1   \n",
       "57910                                                 NaN         1   \n",
       "57935   Getty Images Nifty moved in a tight range of 8...         1   \n",
       "57936   Getty Images NEW DELHI: Fertilisers stocks wer...         1   \n",
       "57937   Reuters On an immediate basis, 15,770/52,500 a...         1   \n",
       "...                                                   ...       ...   \n",
       "102076  Universal (NYSE:UVV–Get Free Report) and Briti...         1   \n",
       "102077  IANS INSIGHTS  \\n    \\t                    Rea...         1   \n",
       "102078  ETMarkets.com Indian frontline indices S&P BSE...         1   \n",
       "102079  “It is the  Tata  tag we aim for, who doesn’t ...         1   \n",
       "102820  Reuters Bombay Stock Exchange Related FII acti...         1   \n",
       "\n",
       "                                                 entities  \n",
       "3109    Nov 02_391 2023_391 07:22:41 PM_392 Thursday_3...  \n",
       "57910   SynopsisVarun Beverages Ltd_383 second_396 Pep...  \n",
       "57935   NEW DELHI_384 Friday_391 10:09AM_397 Oriental_...  \n",
       "57936   NEW DELHI_384 Fertilisers_380 Friday_391 Bohra...  \n",
       "57937   InterGlobe Aviation Ltd._383 0.08 per cent_394...  \n",
       "...                                                   ...  \n",
       "102076      NYSE_383 UVV_383 British American Tobacco_383  \n",
       "102077                                                     \n",
       "102078  Indian_381 BSE Sensex_383 Nifty50_383 Friday_3...  \n",
       "102079  Tata_383 Tata_383 iPhone_383 Narasapura_384 Ka...  \n",
       "102820                    Indian_381 innovations &amp_383  \n",
       "\n",
       "[3503 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df_24k[balanced_df_24k['relevant'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevant\n",
       "0    12454\n",
       "1     2102\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(balanced_df_24k, test_size=0.4, random_state=42, stratify=balanced_df_24k['relevant'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['relevant'])\n",
    "\n",
    "train_df['relevant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # This should return True if CUDA is available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "  Batch 10/910 - Loss: 0.0533\n",
      "  Batch 20/910 - Loss: 0.0889\n",
      "  Batch 30/910 - Loss: 0.1251\n",
      "  Batch 40/910 - Loss: 0.0281\n",
      "  Batch 50/910 - Loss: 0.1214\n",
      "  Batch 60/910 - Loss: 0.1508\n",
      "  Batch 70/910 - Loss: 0.0908\n",
      "  Batch 80/910 - Loss: 0.1146\n",
      "  Batch 90/910 - Loss: 0.0708\n",
      "  Batch 100/910 - Loss: 0.0452\n",
      "  Batch 110/910 - Loss: 0.1092\n",
      "  Batch 120/910 - Loss: 0.0251\n",
      "  Batch 130/910 - Loss: 0.0380\n",
      "  Batch 140/910 - Loss: 0.0845\n",
      "  Batch 150/910 - Loss: 0.0900\n",
      "  Batch 160/910 - Loss: 0.1914\n",
      "  Batch 170/910 - Loss: 0.1182\n",
      "  Batch 180/910 - Loss: 0.0841\n",
      "  Batch 190/910 - Loss: 0.0364\n",
      "  Batch 200/910 - Loss: 0.0855\n",
      "  Batch 210/910 - Loss: 0.1513\n",
      "  Batch 220/910 - Loss: 0.0954\n",
      "  Batch 230/910 - Loss: 0.1615\n",
      "  Batch 240/910 - Loss: 0.1022\n",
      "  Batch 250/910 - Loss: 0.0980\n",
      "  Batch 260/910 - Loss: 0.1054\n",
      "  Batch 270/910 - Loss: 0.0334\n",
      "  Batch 280/910 - Loss: 0.0500\n",
      "  Batch 290/910 - Loss: 0.1918\n",
      "  Batch 300/910 - Loss: 0.0776\n",
      "  Batch 310/910 - Loss: 0.0527\n",
      "  Batch 320/910 - Loss: 0.0809\n",
      "  Batch 330/910 - Loss: 0.1603\n",
      "  Batch 340/910 - Loss: 0.1258\n",
      "  Batch 350/910 - Loss: 0.0192\n",
      "  Batch 360/910 - Loss: 0.0945\n",
      "  Batch 370/910 - Loss: 0.0175\n",
      "  Batch 380/910 - Loss: 0.0591\n",
      "  Batch 390/910 - Loss: 0.0473\n",
      "  Batch 400/910 - Loss: 0.1455\n",
      "  Batch 410/910 - Loss: 0.1575\n",
      "  Batch 420/910 - Loss: 0.0583\n",
      "  Batch 430/910 - Loss: 0.0373\n",
      "  Batch 440/910 - Loss: 0.0421\n",
      "  Batch 450/910 - Loss: 0.1754\n",
      "  Batch 460/910 - Loss: 0.0968\n",
      "  Batch 470/910 - Loss: 0.0897\n",
      "  Batch 480/910 - Loss: 0.1322\n",
      "  Batch 490/910 - Loss: 0.0762\n",
      "  Batch 500/910 - Loss: 0.0209\n",
      "  Batch 510/910 - Loss: 0.1077\n",
      "  Batch 520/910 - Loss: 0.1284\n",
      "  Batch 530/910 - Loss: 0.0098\n",
      "  Batch 540/910 - Loss: 0.1078\n",
      "  Batch 550/910 - Loss: 0.0513\n",
      "  Batch 560/910 - Loss: 0.0370\n",
      "  Batch 570/910 - Loss: 0.1105\n",
      "  Batch 580/910 - Loss: 0.0275\n",
      "  Batch 590/910 - Loss: 0.1193\n",
      "  Batch 600/910 - Loss: 0.1000\n",
      "  Batch 610/910 - Loss: 0.0844\n",
      "  Batch 620/910 - Loss: 0.1262\n",
      "  Batch 630/910 - Loss: 0.1373\n",
      "  Batch 640/910 - Loss: 0.0541\n",
      "  Batch 650/910 - Loss: 0.1177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     78\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 79\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m batch_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:647\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[0;32m    646\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m--> 647\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    648\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    650\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import XLNetForSequenceClassification\n",
    "\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        encoding = self.tokenizer(\n",
    "            row['content'], # Replace 'content' with 'article_content' if working with real data\n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt' #  return the output in the form of PyTorch tensors\n",
    "            )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(row['relevant'], dtype=torch.float), # for BCEWithLogitsLoss use float\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(train_df, tokenizer, max_length=128)\n",
    "val_dataset = TextDataset(val_df, tokenizer, max_length=128)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['relevant']), y=train_df['relevant'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# load the XLNet model\n",
    "XLNet_model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=1)\n",
    "XLNet_model.to(device)\n",
    "\n",
    "# load from checkpoint\n",
    "checkpoint = torch.load('best_XLNet_model_epoch_3_BCEWithLogitsLoss.pt')\n",
    "XLNet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Loss function and optimizer\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "optimizer = AdamW(XLNet_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "current_epoch = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(current_epoch, num_epochs):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    XLNet_model.train() #switch to training mode\n",
    "    train_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad() # Clear old gradients\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device),\n",
    "        }\n",
    "\n",
    "        outputs = XLNet_model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(train_dataloader) # average training loss\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    XLNet_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device),\n",
    "            }\n",
    "            outputs = XLNet_model(**inputs)\n",
    "            val_loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            preds = torch.sigmoid(logits).cpu().numpy() > 0.5  # threshold = 0.5 for  now\n",
    "            val_preds.extend(preds)           \n",
    "            val_labels.extend(inputs['labels'].cpu().numpy())\n",
    "            \n",
    "    val_loss /= len(val_dataloader) # average validation loss\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(classification_report(val_labels, val_preds))\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved. Saving model.\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': XLNet_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss\n",
    "    }, f\"best_XLNet_model_epoch_{epoch}_BCEWithLogitsLoss.pt\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted XLNet\n",
    "incorporating entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m XLNetTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlnet-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTextDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataframe, tokenizer, max_length):\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe \u001b[38;5;241m=\u001b[39m dataframe\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  \n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        encoding = self.tokenizer(\n",
    "            row['title'], row['content'], row['entities'], # Replace 'content' with 'article_content' if working with real data\n",
    "            add_special_tokens=True,\n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt' #  return the output in the form of PyTorch tensors\n",
    "            )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(row['relevant'], dtype=torch.float), # for BCEWithLogitsLoss use float\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(train_df, tokenizer, max_length=256)\n",
    "val_dataset = TextDataset(val_df, tokenizer, max_length=256)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['relevant']), y=train_df['relevant'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# load the XLNet model\n",
    "XLNet_model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=1)\n",
    "XLNet_model.to(device)\n",
    "\n",
    "# load from checkpoint\n",
    "# checkpoint = torch.load('best_XLNet_model_epoch_3_BCEWithLogitsLoss.pt')\n",
    "# XLNet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Loss function and optimizer\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "optimizer = AdamW(XLNet_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.2, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "current_epoch = 1\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(current_epoch, num_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    XLNet_model.train() #switch to training mode\n",
    "    train_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad() # Clear old gradients\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device),\n",
    "        }\n",
    "\n",
    "        outputs = XLNet_model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(train_dataloader) # average training loss\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    XLNet_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device),\n",
    "            }\n",
    "            outputs = XLNet_model(**inputs)\n",
    "            val_loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            preds = torch.sigmoid(logits).cpu().numpy() > 0.5  # threshold = 0.5 for  now\n",
    "            val_preds.extend(preds)           \n",
    "            val_labels.extend(inputs['labels'].cpu().numpy())\n",
    "            \n",
    "    val_loss /= len(val_dataloader) # average validation loss\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(classification_report(val_labels, val_preds))\n",
    "    # step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved. Saving model.\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': XLNet_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss\n",
    "    }, f\"best_XLNet_model_epoch_{epoch}_BCEWithLogitsLoss.pt\")\n",
    "\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted XLNet\n",
    "Only trained on the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'entities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'entities'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m XLNet_model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m#switch to training mode\u001b[39;00m\n\u001b[0;32m     68\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     70\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Clear old gradients\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     75\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     19\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[index]\n\u001b[0;32m     20\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m---> 21\u001b[0m         \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentities\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;66;03m# Replace 'content' with 'article_content' if working with real data\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m     24\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     25\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length, \n\u001b[0;32m     26\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#  return the output in the form of PyTorch tensors\u001b[39;00m\n\u001b[0;32m     27\u001b[0m         )\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelevant\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), \u001b[38;5;66;03m# for BCEWithLogitsLoss use float\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'entities'"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  \n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        encoding = self.tokenizer(\n",
    "            row['entities'], # Replace 'content' with 'article_content' if working with real data\n",
    "            add_special_tokens=True,\n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt' #  return the output in the form of PyTorch tensors\n",
    "            )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(row['relevant'], dtype=torch.float), # for BCEWithLogitsLoss use float\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(train_df, tokenizer, max_length=256)\n",
    "val_dataset = TextDataset(val_df, tokenizer, max_length=256)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['relevant']), y=train_df['relevant'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# load the XLNet model\n",
    "XLNet_model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=1)\n",
    "XLNet_model.to(device)\n",
    "\n",
    "# load from checkpoint\n",
    "# checkpoint = torch.load('best_XLNet_model_epoch_3_BCEWithLogitsLoss.pt')\n",
    "# XLNet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Loss function and optimizer\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "optimizer = AdamW(XLNet_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.2, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "current_epoch = 1\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(current_epoch, num_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    XLNet_model.train() #switch to training mode\n",
    "    train_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad() # Clear old gradients\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device),\n",
    "        }\n",
    "\n",
    "        outputs = XLNet_model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(train_dataloader) # average training loss\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    XLNet_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device),\n",
    "            }\n",
    "            outputs = XLNet_model(**inputs)\n",
    "            val_loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            preds = torch.sigmoid(logits).cpu().numpy() > 0.5  # threshold = 0.5 for  now\n",
    "            val_preds.extend(preds)           \n",
    "            val_labels.extend(inputs['labels'].cpu().numpy())\n",
    "            \n",
    "    val_loss /= len(val_dataloader) # average validation loss\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(classification_report(val_labels, val_preds))\n",
    "    # step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved. Saving model.\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': XLNet_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss\n",
    "    }, f\"best_XLNet_model_epoch_{epoch}_BCEWithLogitsLoss.pt\")\n",
    "\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Batch 10/910 - Loss: 0.0712\n",
      "  Batch 20/910 - Loss: 0.0379\n",
      "  Batch 30/910 - Loss: 0.0192\n",
      "  Batch 40/910 - Loss: 0.0212\n",
      "  Batch 50/910 - Loss: 0.0293\n",
      "  Batch 60/910 - Loss: 0.0337\n",
      "  Batch 70/910 - Loss: 0.0375\n",
      "  Batch 80/910 - Loss: 0.0245\n",
      "  Batch 90/910 - Loss: 0.0321\n",
      "  Batch 100/910 - Loss: 0.0341\n",
      "  Batch 110/910 - Loss: 0.0362\n",
      "  Batch 120/910 - Loss: 0.0137\n",
      "  Batch 130/910 - Loss: 0.0322\n",
      "  Batch 140/910 - Loss: 0.0434\n",
      "  Batch 150/910 - Loss: 0.0263\n",
      "  Batch 160/910 - Loss: 0.0209\n",
      "  Batch 170/910 - Loss: 0.0440\n",
      "  Batch 180/910 - Loss: 0.0833\n",
      "  Batch 190/910 - Loss: 0.0310\n",
      "  Batch 200/910 - Loss: 0.0259\n",
      "  Batch 210/910 - Loss: 0.0071\n",
      "  Batch 220/910 - Loss: 0.0323\n",
      "  Batch 230/910 - Loss: 0.0218\n",
      "  Batch 240/910 - Loss: 0.0324\n",
      "  Batch 250/910 - Loss: 0.0277\n",
      "  Batch 260/910 - Loss: 0.0429\n",
      "  Batch 270/910 - Loss: 0.0243\n",
      "  Batch 280/910 - Loss: 0.0460\n",
      "  Batch 290/910 - Loss: 0.0368\n",
      "  Batch 300/910 - Loss: 0.0274\n",
      "  Batch 310/910 - Loss: 0.0572\n",
      "  Batch 320/910 - Loss: 0.0210\n",
      "  Batch 330/910 - Loss: 0.0222\n",
      "  Batch 340/910 - Loss: 0.0151\n",
      "  Batch 350/910 - Loss: 0.0313\n",
      "  Batch 360/910 - Loss: 0.0305\n",
      "  Batch 370/910 - Loss: 0.0304\n",
      "  Batch 380/910 - Loss: 0.0509\n",
      "  Batch 390/910 - Loss: 0.0167\n",
      "  Batch 400/910 - Loss: 0.0089\n",
      "  Batch 410/910 - Loss: 0.0236\n",
      "  Batch 420/910 - Loss: 0.0284\n",
      "  Batch 430/910 - Loss: 0.0268\n",
      "  Batch 440/910 - Loss: 0.0191\n",
      "  Batch 450/910 - Loss: 0.0557\n",
      "  Batch 460/910 - Loss: 0.0371\n",
      "  Batch 470/910 - Loss: 0.0169\n",
      "  Batch 480/910 - Loss: 0.0446\n",
      "  Batch 490/910 - Loss: 0.0214\n",
      "  Batch 500/910 - Loss: 0.0279\n",
      "  Batch 510/910 - Loss: 0.0176\n",
      "  Batch 520/910 - Loss: 0.0164\n",
      "  Batch 530/910 - Loss: 0.0144\n",
      "  Batch 540/910 - Loss: 0.0244\n",
      "  Batch 550/910 - Loss: 0.0188\n",
      "  Batch 560/910 - Loss: 0.0107\n",
      "  Batch 570/910 - Loss: 0.0215\n",
      "  Batch 580/910 - Loss: 0.0253\n",
      "  Batch 590/910 - Loss: 0.0103\n",
      "  Batch 600/910 - Loss: 0.0172\n",
      "  Batch 610/910 - Loss: 0.0108\n",
      "  Batch 620/910 - Loss: 0.0565\n",
      "  Batch 630/910 - Loss: 0.0220\n",
      "  Batch 640/910 - Loss: 0.0612\n",
      "  Batch 650/910 - Loss: 0.0222\n",
      "  Batch 660/910 - Loss: 0.0167\n",
      "  Batch 670/910 - Loss: 0.0334\n",
      "  Batch 680/910 - Loss: 0.0643\n",
      "  Batch 690/910 - Loss: 0.0197\n",
      "  Batch 700/910 - Loss: 0.0265\n",
      "  Batch 710/910 - Loss: 0.0258\n",
      "  Batch 720/910 - Loss: 0.0310\n",
      "  Batch 730/910 - Loss: 0.0107\n",
      "  Batch 740/910 - Loss: 0.0507\n",
      "  Batch 750/910 - Loss: 0.0152\n",
      "  Batch 760/910 - Loss: 0.0353\n",
      "  Batch 770/910 - Loss: 0.0134\n",
      "  Batch 780/910 - Loss: 0.0157\n",
      "  Batch 790/910 - Loss: 0.0351\n",
      "  Batch 800/910 - Loss: 0.0472\n",
      "  Batch 810/910 - Loss: 0.0112\n",
      "  Batch 820/910 - Loss: 0.0100\n",
      "  Batch 830/910 - Loss: 0.0077\n",
      "  Batch 840/910 - Loss: 0.0195\n",
      "  Batch 850/910 - Loss: 0.0178\n",
      "  Batch 860/910 - Loss: 0.0209\n",
      "  Batch 870/910 - Loss: 0.0118\n",
      "  Batch 880/910 - Loss: 0.0079\n",
      "  Batch 890/910 - Loss: 0.0456\n",
      "  Batch 900/910 - Loss: 0.0437\n",
      "  Batch 910/910 - Loss: 0.0344\n",
      "Training Loss: 0.0281\n",
      "Validation Loss: 0.0290\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.65      0.78      4151\n",
      "         1.0       0.32      0.97      0.48       701\n",
      "\n",
      "    accuracy                           0.69      4852\n",
      "   macro avg       0.65      0.81      0.63      4852\n",
      "weighted avg       0.89      0.69      0.74      4852\n",
      "\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 2/20\n",
      "  Batch 10/910 - Loss: 0.0127\n",
      "  Batch 20/910 - Loss: 0.0041\n",
      "  Batch 30/910 - Loss: 0.0271\n",
      "  Batch 40/910 - Loss: 0.0320\n",
      "  Batch 50/910 - Loss: 0.0229\n",
      "  Batch 60/910 - Loss: 0.0205\n",
      "  Batch 70/910 - Loss: 0.0085\n",
      "  Batch 80/910 - Loss: 0.0098\n",
      "  Batch 90/910 - Loss: 0.0020\n",
      "  Batch 100/910 - Loss: 0.0288\n",
      "  Batch 110/910 - Loss: 0.0189\n",
      "  Batch 120/910 - Loss: 0.0158\n",
      "  Batch 130/910 - Loss: 0.0441\n",
      "  Batch 140/910 - Loss: 0.0230\n",
      "  Batch 150/910 - Loss: 0.0450\n",
      "  Batch 160/910 - Loss: 0.0235\n",
      "  Batch 170/910 - Loss: 0.0207\n",
      "  Batch 180/910 - Loss: 0.0213\n",
      "  Batch 190/910 - Loss: 0.0359\n",
      "  Batch 200/910 - Loss: 0.0208\n",
      "  Batch 210/910 - Loss: 0.0290\n",
      "  Batch 220/910 - Loss: 0.0164\n",
      "  Batch 230/910 - Loss: 0.0085\n",
      "  Batch 240/910 - Loss: 0.0518\n",
      "  Batch 250/910 - Loss: 0.0083\n",
      "  Batch 260/910 - Loss: 0.0137\n",
      "  Batch 270/910 - Loss: 0.0148\n",
      "  Batch 280/910 - Loss: 0.0169\n",
      "  Batch 290/910 - Loss: 0.0323\n",
      "  Batch 300/910 - Loss: 0.0125\n",
      "  Batch 310/910 - Loss: 0.0196\n",
      "  Batch 320/910 - Loss: 0.0328\n",
      "  Batch 330/910 - Loss: 0.0181\n",
      "  Batch 340/910 - Loss: 0.0275\n",
      "  Batch 350/910 - Loss: 0.0098\n",
      "  Batch 360/910 - Loss: 0.0233\n",
      "  Batch 370/910 - Loss: 0.0142\n",
      "  Batch 380/910 - Loss: 0.0058\n",
      "  Batch 390/910 - Loss: 0.0223\n",
      "  Batch 400/910 - Loss: 0.0041\n",
      "  Batch 410/910 - Loss: 0.0327\n",
      "  Batch 420/910 - Loss: 0.0393\n",
      "  Batch 430/910 - Loss: 0.0272\n",
      "  Batch 440/910 - Loss: 0.0317\n",
      "  Batch 450/910 - Loss: 0.0159\n",
      "  Batch 460/910 - Loss: 0.0351\n",
      "  Batch 470/910 - Loss: 0.0162\n",
      "  Batch 480/910 - Loss: 0.0165\n",
      "  Batch 490/910 - Loss: 0.0181\n",
      "  Batch 500/910 - Loss: 0.0254\n",
      "  Batch 510/910 - Loss: 0.0263\n",
      "  Batch 520/910 - Loss: 0.0211\n",
      "  Batch 530/910 - Loss: 0.0257\n",
      "  Batch 540/910 - Loss: 0.0126\n",
      "  Batch 550/910 - Loss: 0.0350\n",
      "  Batch 560/910 - Loss: 0.0131\n",
      "  Batch 570/910 - Loss: 0.0721\n",
      "  Batch 580/910 - Loss: 0.0131\n",
      "  Batch 590/910 - Loss: 0.0139\n",
      "  Batch 600/910 - Loss: 0.0193\n",
      "  Batch 610/910 - Loss: 0.0219\n",
      "  Batch 620/910 - Loss: 0.0205\n",
      "  Batch 630/910 - Loss: 0.0373\n",
      "  Batch 640/910 - Loss: 0.0252\n",
      "  Batch 650/910 - Loss: 0.0144\n",
      "  Batch 660/910 - Loss: 0.0172\n",
      "  Batch 670/910 - Loss: 0.0115\n",
      "  Batch 680/910 - Loss: 0.0225\n",
      "  Batch 690/910 - Loss: 0.0122\n",
      "  Batch 700/910 - Loss: 0.0343\n",
      "  Batch 710/910 - Loss: 0.0130\n",
      "  Batch 720/910 - Loss: 0.0192\n",
      "  Batch 730/910 - Loss: 0.0309\n",
      "  Batch 740/910 - Loss: 0.0118\n",
      "  Batch 750/910 - Loss: 0.0163\n",
      "  Batch 760/910 - Loss: 0.0317\n",
      "  Batch 770/910 - Loss: 0.0091\n",
      "  Batch 780/910 - Loss: 0.0121\n",
      "  Batch 790/910 - Loss: 0.0234\n",
      "  Batch 800/910 - Loss: 0.0172\n",
      "  Batch 810/910 - Loss: 0.0150\n",
      "  Batch 820/910 - Loss: 0.0172\n",
      "  Batch 830/910 - Loss: 0.0360\n",
      "  Batch 840/910 - Loss: 0.0360\n",
      "  Batch 850/910 - Loss: 0.0101\n",
      "  Batch 860/910 - Loss: 0.0217\n",
      "  Batch 870/910 - Loss: 0.0197\n",
      "  Batch 880/910 - Loss: 0.0190\n",
      "  Batch 890/910 - Loss: 0.0119\n",
      "  Batch 900/910 - Loss: 0.0129\n",
      "  Batch 910/910 - Loss: 0.0324\n",
      "Training Loss: 0.0232\n",
      "Validation Loss: 0.0236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.78      0.87      4151\n",
      "         1.0       0.41      0.93      0.57       701\n",
      "\n",
      "    accuracy                           0.80      4852\n",
      "   macro avg       0.70      0.85      0.72      4852\n",
      "weighted avg       0.90      0.80      0.83      4852\n",
      "\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 3/20\n",
      "  Batch 10/910 - Loss: 0.0193\n",
      "  Batch 20/910 - Loss: 0.0160\n",
      "  Batch 30/910 - Loss: 0.0133\n",
      "  Batch 40/910 - Loss: 0.0055\n",
      "  Batch 50/910 - Loss: 0.0200\n",
      "  Batch 60/910 - Loss: 0.0272\n",
      "  Batch 70/910 - Loss: 0.0435\n",
      "  Batch 80/910 - Loss: 0.0153\n",
      "  Batch 90/910 - Loss: 0.0136\n",
      "  Batch 100/910 - Loss: 0.0095\n",
      "  Batch 110/910 - Loss: 0.0201\n",
      "  Batch 120/910 - Loss: 0.0368\n",
      "  Batch 130/910 - Loss: 0.0111\n",
      "  Batch 140/910 - Loss: 0.0173\n",
      "  Batch 150/910 - Loss: 0.0192\n",
      "  Batch 160/910 - Loss: 0.0147\n",
      "  Batch 170/910 - Loss: 0.0065\n",
      "  Batch 180/910 - Loss: 0.0341\n",
      "  Batch 190/910 - Loss: 0.0303\n",
      "  Batch 200/910 - Loss: 0.0160\n",
      "  Batch 210/910 - Loss: 0.0173\n",
      "  Batch 220/910 - Loss: 0.0147\n",
      "  Batch 230/910 - Loss: 0.0197\n",
      "  Batch 240/910 - Loss: 0.0134\n",
      "  Batch 250/910 - Loss: 0.0343\n",
      "  Batch 260/910 - Loss: 0.0117\n",
      "  Batch 270/910 - Loss: 0.0264\n",
      "  Batch 280/910 - Loss: 0.0170\n",
      "  Batch 290/910 - Loss: 0.0243\n",
      "  Batch 300/910 - Loss: 0.0143\n",
      "  Batch 310/910 - Loss: 0.0323\n",
      "  Batch 320/910 - Loss: 0.0136\n",
      "  Batch 330/910 - Loss: 0.0145\n",
      "  Batch 340/910 - Loss: 0.0170\n",
      "  Batch 350/910 - Loss: 0.0682\n",
      "  Batch 360/910 - Loss: 0.0114\n",
      "  Batch 370/910 - Loss: 0.0196\n",
      "  Batch 380/910 - Loss: 0.0172\n",
      "  Batch 390/910 - Loss: 0.0059\n",
      "  Batch 400/910 - Loss: 0.0561\n",
      "  Batch 410/910 - Loss: 0.0041\n",
      "  Batch 420/910 - Loss: 0.0593\n",
      "  Batch 430/910 - Loss: 0.0243\n",
      "  Batch 440/910 - Loss: 0.0255\n",
      "  Batch 450/910 - Loss: 0.0211\n",
      "  Batch 460/910 - Loss: 0.0183\n",
      "  Batch 470/910 - Loss: 0.0267\n",
      "  Batch 480/910 - Loss: 0.0274\n",
      "  Batch 490/910 - Loss: 0.0200\n",
      "  Batch 500/910 - Loss: 0.0169\n",
      "  Batch 510/910 - Loss: 0.0158\n",
      "  Batch 520/910 - Loss: 0.0046\n",
      "  Batch 530/910 - Loss: 0.0113\n",
      "  Batch 540/910 - Loss: 0.0257\n",
      "  Batch 550/910 - Loss: 0.0175\n",
      "  Batch 560/910 - Loss: 0.0154\n",
      "  Batch 570/910 - Loss: 0.0203\n",
      "  Batch 580/910 - Loss: 0.0645\n",
      "  Batch 590/910 - Loss: 0.0051\n",
      "  Batch 600/910 - Loss: 0.0062\n",
      "  Batch 610/910 - Loss: 0.0244\n",
      "  Batch 620/910 - Loss: 0.0357\n",
      "  Batch 630/910 - Loss: 0.0228\n",
      "  Batch 640/910 - Loss: 0.0177\n",
      "  Batch 650/910 - Loss: 0.0253\n",
      "  Batch 660/910 - Loss: 0.0455\n",
      "  Batch 670/910 - Loss: 0.0221\n",
      "  Batch 680/910 - Loss: 0.0180\n",
      "  Batch 690/910 - Loss: 0.0133\n",
      "  Batch 700/910 - Loss: 0.0650\n",
      "  Batch 710/910 - Loss: 0.0251\n",
      "  Batch 720/910 - Loss: 0.0146\n",
      "  Batch 730/910 - Loss: 0.0099\n",
      "  Batch 740/910 - Loss: 0.0542\n",
      "  Batch 750/910 - Loss: 0.0203\n",
      "  Batch 760/910 - Loss: 0.0070\n",
      "  Batch 770/910 - Loss: 0.0241\n",
      "  Batch 780/910 - Loss: 0.0144\n",
      "  Batch 790/910 - Loss: 0.0351\n",
      "  Batch 800/910 - Loss: 0.0770\n",
      "  Batch 810/910 - Loss: 0.0274\n",
      "  Batch 820/910 - Loss: 0.0159\n",
      "  Batch 830/910 - Loss: 0.0153\n",
      "  Batch 840/910 - Loss: 0.0143\n",
      "  Batch 850/910 - Loss: 0.0857\n",
      "  Batch 860/910 - Loss: 0.0219\n",
      "  Batch 870/910 - Loss: 0.0115\n",
      "  Batch 880/910 - Loss: 0.0240\n",
      "  Batch 890/910 - Loss: 0.0180\n",
      "  Batch 900/910 - Loss: 0.0701\n",
      "  Batch 910/910 - Loss: 0.0304\n",
      "Training Loss: 0.0226\n",
      "Validation Loss: 0.0234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.81      0.89      4151\n",
      "         1.0       0.45      0.89      0.59       701\n",
      "\n",
      "    accuracy                           0.82      4852\n",
      "   macro avg       0.71      0.85      0.74      4852\n",
      "weighted avg       0.90      0.82      0.85      4852\n",
      "\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 4/20\n",
      "  Batch 10/910 - Loss: 0.0255\n",
      "  Batch 20/910 - Loss: 0.0173\n",
      "  Batch 30/910 - Loss: 0.0056\n",
      "  Batch 40/910 - Loss: 0.0114\n",
      "  Batch 50/910 - Loss: 0.0200\n",
      "  Batch 60/910 - Loss: 0.0055\n",
      "  Batch 70/910 - Loss: 0.0158\n",
      "  Batch 80/910 - Loss: 0.0107\n",
      "  Batch 90/910 - Loss: 0.0026\n",
      "  Batch 100/910 - Loss: 0.0153\n",
      "  Batch 110/910 - Loss: 0.0087\n",
      "  Batch 120/910 - Loss: 0.0220\n",
      "  Batch 130/910 - Loss: 0.0088\n",
      "  Batch 140/910 - Loss: 0.0019\n",
      "  Batch 150/910 - Loss: 0.0201\n",
      "  Batch 160/910 - Loss: 0.0314\n",
      "  Batch 170/910 - Loss: 0.0136\n",
      "  Batch 180/910 - Loss: 0.0092\n",
      "  Batch 190/910 - Loss: 0.0068\n",
      "  Batch 200/910 - Loss: 0.0019\n",
      "  Batch 210/910 - Loss: 0.0135\n",
      "  Batch 220/910 - Loss: 0.0047\n",
      "  Batch 230/910 - Loss: 0.0201\n",
      "  Batch 240/910 - Loss: 0.0062\n",
      "  Batch 250/910 - Loss: 0.0120\n",
      "  Batch 260/910 - Loss: 0.0144\n",
      "  Batch 270/910 - Loss: 0.0165\n",
      "  Batch 280/910 - Loss: 0.0222\n",
      "  Batch 290/910 - Loss: 0.0183\n",
      "  Batch 300/910 - Loss: 0.0164\n",
      "  Batch 310/910 - Loss: 0.0251\n",
      "  Batch 320/910 - Loss: 0.0136\n",
      "  Batch 330/910 - Loss: 0.0140\n",
      "  Batch 340/910 - Loss: 0.0152\n",
      "  Batch 350/910 - Loss: 0.0265\n",
      "  Batch 360/910 - Loss: 0.0326\n",
      "  Batch 370/910 - Loss: 0.0126\n",
      "  Batch 380/910 - Loss: 0.0165\n",
      "  Batch 390/910 - Loss: 0.0159\n",
      "  Batch 400/910 - Loss: 0.0102\n",
      "  Batch 410/910 - Loss: 0.0215\n",
      "  Batch 420/910 - Loss: 0.0196\n",
      "  Batch 430/910 - Loss: 0.0218\n",
      "  Batch 440/910 - Loss: 0.0254\n",
      "  Batch 450/910 - Loss: 0.0503\n",
      "  Batch 460/910 - Loss: 0.0264\n",
      "  Batch 470/910 - Loss: 0.0120\n",
      "  Batch 480/910 - Loss: 0.0159\n",
      "  Batch 490/910 - Loss: 0.0167\n",
      "  Batch 500/910 - Loss: 0.1485\n",
      "  Batch 510/910 - Loss: 0.0140\n",
      "  Batch 520/910 - Loss: 0.0169\n",
      "  Batch 530/910 - Loss: 0.0266\n",
      "  Batch 540/910 - Loss: 0.0157\n",
      "  Batch 550/910 - Loss: 0.0134\n",
      "  Batch 560/910 - Loss: 0.0189\n",
      "  Batch 570/910 - Loss: 0.0247\n",
      "  Batch 580/910 - Loss: 0.0253\n",
      "  Batch 590/910 - Loss: 0.0284\n",
      "  Batch 600/910 - Loss: 0.0522\n",
      "  Batch 610/910 - Loss: 0.0088\n",
      "  Batch 620/910 - Loss: 0.0117\n",
      "  Batch 630/910 - Loss: 0.0138\n",
      "  Batch 640/910 - Loss: 0.0247\n",
      "  Batch 650/910 - Loss: 0.0073\n",
      "  Batch 660/910 - Loss: 0.0225\n",
      "  Batch 670/910 - Loss: 0.0122\n",
      "  Batch 680/910 - Loss: 0.0124\n",
      "  Batch 690/910 - Loss: 0.0237\n",
      "  Batch 700/910 - Loss: 0.0150\n",
      "  Batch 710/910 - Loss: 0.0293\n",
      "  Batch 720/910 - Loss: 0.0348\n",
      "  Batch 730/910 - Loss: 0.0066\n",
      "  Batch 740/910 - Loss: 0.0438\n",
      "  Batch 750/910 - Loss: 0.0207\n",
      "  Batch 760/910 - Loss: 0.0121\n",
      "  Batch 770/910 - Loss: 0.0910\n",
      "  Batch 780/910 - Loss: 0.0183\n",
      "  Batch 790/910 - Loss: 0.0229\n",
      "  Batch 800/910 - Loss: 0.0043\n",
      "  Batch 810/910 - Loss: 0.0092\n",
      "  Batch 820/910 - Loss: 0.0135\n",
      "  Batch 830/910 - Loss: 0.0099\n",
      "  Batch 840/910 - Loss: 0.0111\n",
      "  Batch 850/910 - Loss: 0.0445\n",
      "  Batch 860/910 - Loss: 0.0166\n",
      "  Batch 870/910 - Loss: 0.0173\n",
      "  Batch 880/910 - Loss: 0.0209\n",
      "  Batch 890/910 - Loss: 0.0137\n",
      "  Batch 900/910 - Loss: 0.1115\n",
      "  Batch 910/910 - Loss: 0.0120\n",
      "Training Loss: 0.0203\n",
      "Validation Loss: 0.0231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.82      0.89      4151\n",
      "         1.0       0.46      0.89      0.61       701\n",
      "\n",
      "    accuracy                           0.83      4852\n",
      "   macro avg       0.72      0.86      0.75      4852\n",
      "weighted avg       0.90      0.83      0.85      4852\n",
      "\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 5/20\n",
      "  Batch 10/910 - Loss: 0.0119\n",
      "  Batch 20/910 - Loss: 0.0213\n",
      "  Batch 30/910 - Loss: 0.0074\n",
      "  Batch 40/910 - Loss: 0.0101\n",
      "  Batch 50/910 - Loss: 0.0048\n",
      "  Batch 60/910 - Loss: 0.0152\n",
      "  Batch 70/910 - Loss: 0.0144\n",
      "  Batch 80/910 - Loss: 0.0085\n",
      "  Batch 90/910 - Loss: 0.0181\n",
      "  Batch 100/910 - Loss: 0.0043\n",
      "  Batch 110/910 - Loss: 0.0090\n",
      "  Batch 120/910 - Loss: 0.0344\n",
      "  Batch 130/910 - Loss: 0.0123\n",
      "  Batch 140/910 - Loss: 0.0256\n",
      "  Batch 150/910 - Loss: 0.0194\n",
      "  Batch 160/910 - Loss: 0.0162\n",
      "  Batch 170/910 - Loss: 0.0097\n",
      "  Batch 180/910 - Loss: 0.0191\n",
      "  Batch 190/910 - Loss: 0.0147\n",
      "  Batch 200/910 - Loss: 0.0355\n",
      "  Batch 210/910 - Loss: 0.0098\n",
      "  Batch 220/910 - Loss: 0.0091\n",
      "  Batch 230/910 - Loss: 0.0113\n",
      "  Batch 240/910 - Loss: 0.0264\n",
      "  Batch 250/910 - Loss: 0.0152\n",
      "  Batch 260/910 - Loss: 0.0402\n",
      "  Batch 270/910 - Loss: 0.0032\n",
      "  Batch 280/910 - Loss: 0.0281\n",
      "  Batch 290/910 - Loss: 0.0164\n",
      "  Batch 300/910 - Loss: 0.0267\n",
      "  Batch 310/910 - Loss: 0.0248\n",
      "  Batch 320/910 - Loss: 0.0172\n",
      "  Batch 330/910 - Loss: 0.0149\n",
      "  Batch 340/910 - Loss: 0.0136\n",
      "  Batch 350/910 - Loss: 0.0193\n",
      "  Batch 360/910 - Loss: 0.0135\n",
      "  Batch 370/910 - Loss: 0.0213\n",
      "  Batch 380/910 - Loss: 0.0099\n",
      "  Batch 390/910 - Loss: 0.0122\n",
      "  Batch 400/910 - Loss: 0.0186\n",
      "  Batch 410/910 - Loss: 0.0212\n",
      "  Batch 420/910 - Loss: 0.0494\n",
      "  Batch 430/910 - Loss: 0.0187\n",
      "  Batch 440/910 - Loss: 0.0171\n",
      "  Batch 450/910 - Loss: 0.0505\n",
      "  Batch 460/910 - Loss: 0.0025\n",
      "  Batch 470/910 - Loss: 0.0171\n",
      "  Batch 480/910 - Loss: 0.0052\n",
      "  Batch 490/910 - Loss: 0.0072\n",
      "  Batch 500/910 - Loss: 0.0071\n",
      "  Batch 510/910 - Loss: 0.0143\n",
      "  Batch 520/910 - Loss: 0.0083\n",
      "  Batch 530/910 - Loss: 0.0199\n",
      "  Batch 540/910 - Loss: 0.0407\n",
      "  Batch 550/910 - Loss: 0.0053\n",
      "  Batch 560/910 - Loss: 0.0092\n",
      "  Batch 570/910 - Loss: 0.0155\n",
      "  Batch 580/910 - Loss: 0.0102\n",
      "  Batch 590/910 - Loss: 0.0139\n",
      "  Batch 600/910 - Loss: 0.0250\n",
      "  Batch 610/910 - Loss: 0.0102\n",
      "  Batch 620/910 - Loss: 0.0190\n",
      "  Batch 630/910 - Loss: 0.0368\n",
      "  Batch 640/910 - Loss: 0.0146\n",
      "  Batch 650/910 - Loss: 0.0293\n",
      "  Batch 660/910 - Loss: 0.0078\n",
      "  Batch 670/910 - Loss: 0.0015\n",
      "  Batch 680/910 - Loss: 0.0109\n",
      "  Batch 690/910 - Loss: 0.0055\n",
      "  Batch 700/910 - Loss: 0.0214\n",
      "  Batch 710/910 - Loss: 0.0211\n",
      "  Batch 720/910 - Loss: 0.0071\n",
      "  Batch 730/910 - Loss: 0.0088\n",
      "  Batch 740/910 - Loss: 0.0164\n",
      "  Batch 750/910 - Loss: 0.0071\n",
      "  Batch 760/910 - Loss: 0.0082\n",
      "  Batch 770/910 - Loss: 0.0176\n",
      "  Batch 780/910 - Loss: 0.0132\n",
      "  Batch 790/910 - Loss: 0.0235\n",
      "  Batch 800/910 - Loss: 0.0087\n",
      "  Batch 810/910 - Loss: 0.0225\n",
      "  Batch 820/910 - Loss: 0.0061\n",
      "  Batch 830/910 - Loss: 0.0040\n",
      "  Batch 840/910 - Loss: 0.0078\n",
      "  Batch 850/910 - Loss: 0.0124\n",
      "  Batch 860/910 - Loss: 0.0212\n",
      "  Batch 870/910 - Loss: 0.0323\n",
      "  Batch 880/910 - Loss: 0.0039\n",
      "  Batch 890/910 - Loss: 0.0106\n",
      "  Batch 900/910 - Loss: 0.0232\n",
      "  Batch 910/910 - Loss: 0.0218\n",
      "Training Loss: 0.0174\n",
      "Validation Loss: 0.0241\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.83      0.90      4151\n",
      "         1.0       0.48      0.91      0.62       701\n",
      "\n",
      "    accuracy                           0.84      4852\n",
      "   macro avg       0.73      0.87      0.76      4852\n",
      "weighted avg       0.91      0.84      0.86      4852\n",
      "\n",
      "Epoch 6/20\n",
      "  Batch 10/910 - Loss: 0.0082\n",
      "  Batch 20/910 - Loss: 0.0134\n",
      "  Batch 30/910 - Loss: 0.0120\n",
      "  Batch 40/910 - Loss: 0.0028\n",
      "  Batch 50/910 - Loss: 0.0387\n",
      "  Batch 60/910 - Loss: 0.0093\n",
      "  Batch 70/910 - Loss: 0.0167\n",
      "  Batch 80/910 - Loss: 0.0143\n",
      "  Batch 90/910 - Loss: 0.0084\n",
      "  Batch 100/910 - Loss: 0.0040\n",
      "  Batch 110/910 - Loss: 0.0217\n",
      "  Batch 120/910 - Loss: 0.0086\n",
      "  Batch 130/910 - Loss: 0.0036\n",
      "  Batch 140/910 - Loss: 0.0138\n",
      "  Batch 150/910 - Loss: 0.0134\n",
      "  Batch 160/910 - Loss: 0.0044\n",
      "  Batch 170/910 - Loss: 0.0119\n",
      "  Batch 180/910 - Loss: 0.0109\n",
      "  Batch 190/910 - Loss: 0.0101\n",
      "  Batch 200/910 - Loss: 0.0109\n",
      "  Batch 210/910 - Loss: 0.0098\n",
      "  Batch 220/910 - Loss: 0.0114\n",
      "  Batch 230/910 - Loss: 0.0020\n",
      "  Batch 240/910 - Loss: 0.0091\n",
      "  Batch 250/910 - Loss: 0.0122\n",
      "  Batch 260/910 - Loss: 0.0204\n",
      "  Batch 270/910 - Loss: 0.0051\n",
      "  Batch 280/910 - Loss: 0.0200\n",
      "  Batch 290/910 - Loss: 0.0138\n",
      "  Batch 300/910 - Loss: 0.0019\n",
      "  Batch 310/910 - Loss: 0.0130\n",
      "  Batch 320/910 - Loss: 0.0293\n",
      "  Batch 330/910 - Loss: 0.0090\n",
      "  Batch 340/910 - Loss: 0.0352\n",
      "  Batch 350/910 - Loss: 0.0122\n",
      "  Batch 360/910 - Loss: 0.0147\n",
      "  Batch 370/910 - Loss: 0.0047\n",
      "  Batch 380/910 - Loss: 0.0159\n",
      "  Batch 390/910 - Loss: 0.0090\n",
      "  Batch 400/910 - Loss: 0.0071\n",
      "  Batch 410/910 - Loss: 0.0049\n",
      "  Batch 420/910 - Loss: 0.0311\n",
      "  Batch 430/910 - Loss: 0.0078\n",
      "  Batch 440/910 - Loss: 0.0133\n",
      "  Batch 450/910 - Loss: 0.0109\n",
      "  Batch 460/910 - Loss: 0.0124\n",
      "  Batch 470/910 - Loss: 0.0176\n",
      "  Batch 480/910 - Loss: 0.0061\n",
      "  Batch 490/910 - Loss: 0.0092\n",
      "  Batch 500/910 - Loss: 0.0151\n",
      "  Batch 510/910 - Loss: 0.0115\n",
      "  Batch 520/910 - Loss: 0.0242\n",
      "  Batch 530/910 - Loss: 0.0073\n",
      "  Batch 540/910 - Loss: 0.0254\n",
      "  Batch 550/910 - Loss: 0.0054\n",
      "  Batch 560/910 - Loss: 0.0045\n",
      "  Batch 570/910 - Loss: 0.0069\n",
      "  Batch 580/910 - Loss: 0.0123\n",
      "  Batch 590/910 - Loss: 0.0096\n",
      "  Batch 600/910 - Loss: 0.0175\n",
      "  Batch 610/910 - Loss: 0.0293\n",
      "  Batch 620/910 - Loss: 0.0140\n",
      "  Batch 630/910 - Loss: 0.0186\n",
      "  Batch 640/910 - Loss: 0.0175\n",
      "  Batch 650/910 - Loss: 0.0097\n",
      "  Batch 660/910 - Loss: 0.0189\n",
      "  Batch 670/910 - Loss: 0.0102\n",
      "  Batch 680/910 - Loss: 0.0318\n",
      "  Batch 690/910 - Loss: 0.0137\n",
      "  Batch 700/910 - Loss: 0.0256\n",
      "  Batch 710/910 - Loss: 0.0086\n",
      "  Batch 720/910 - Loss: 0.0332\n",
      "  Batch 730/910 - Loss: 0.0165\n",
      "  Batch 740/910 - Loss: 0.0079\n",
      "  Batch 750/910 - Loss: 0.0140\n",
      "  Batch 760/910 - Loss: 0.0072\n",
      "  Batch 770/910 - Loss: 0.0026\n",
      "  Batch 780/910 - Loss: 0.0102\n",
      "  Batch 790/910 - Loss: 0.0086\n",
      "  Batch 800/910 - Loss: 0.0056\n",
      "  Batch 810/910 - Loss: 0.0056\n",
      "  Batch 820/910 - Loss: 0.0159\n",
      "  Batch 830/910 - Loss: 0.0047\n",
      "  Batch 840/910 - Loss: 0.0118\n",
      "  Batch 850/910 - Loss: 0.0180\n",
      "  Batch 860/910 - Loss: 0.0111\n",
      "  Batch 870/910 - Loss: 0.0150\n",
      "  Batch 880/910 - Loss: 0.0254\n",
      "  Batch 890/910 - Loss: 0.0095\n",
      "  Batch 900/910 - Loss: 0.0150\n",
      "  Batch 910/910 - Loss: 0.0056\n",
      "Training Loss: 0.0142\n",
      "Validation Loss: 0.0280\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.91      4151\n",
      "         1.0       0.51      0.87      0.64       701\n",
      "\n",
      "    accuracy                           0.86      4852\n",
      "   macro avg       0.74      0.86      0.78      4852\n",
      "weighted avg       0.91      0.86      0.87      4852\n",
      "\n",
      "Epoch 7/20\n",
      "  Batch 10/910 - Loss: 0.0045\n",
      "  Batch 20/910 - Loss: 0.0050\n",
      "  Batch 30/910 - Loss: 0.0075\n",
      "  Batch 40/910 - Loss: 0.0074\n",
      "  Batch 50/910 - Loss: 0.0116\n",
      "  Batch 60/910 - Loss: 0.0188\n",
      "  Batch 70/910 - Loss: 0.0044\n",
      "  Batch 80/910 - Loss: 0.0038\n",
      "  Batch 90/910 - Loss: 0.0147\n",
      "  Batch 100/910 - Loss: 0.0185\n",
      "  Batch 110/910 - Loss: 0.0095\n",
      "  Batch 120/910 - Loss: 0.0020\n",
      "  Batch 130/910 - Loss: 0.0204\n",
      "  Batch 140/910 - Loss: 0.0008\n",
      "  Batch 150/910 - Loss: 0.0361\n",
      "  Batch 160/910 - Loss: 0.0057\n",
      "  Batch 170/910 - Loss: 0.0182\n",
      "  Batch 180/910 - Loss: 0.0083\n",
      "  Batch 190/910 - Loss: 0.0107\n",
      "  Batch 200/910 - Loss: 0.0093\n",
      "  Batch 210/910 - Loss: 0.0088\n",
      "  Batch 220/910 - Loss: 0.0029\n",
      "  Batch 230/910 - Loss: 0.0316\n",
      "  Batch 240/910 - Loss: 0.0030\n",
      "  Batch 250/910 - Loss: 0.0019\n",
      "  Batch 260/910 - Loss: 0.0149\n",
      "  Batch 270/910 - Loss: 0.0005\n",
      "  Batch 280/910 - Loss: 0.0128\n",
      "  Batch 290/910 - Loss: 0.0051\n",
      "  Batch 300/910 - Loss: 0.0052\n",
      "  Batch 310/910 - Loss: 0.0016\n",
      "  Batch 320/910 - Loss: 0.0145\n",
      "  Batch 330/910 - Loss: 0.0086\n",
      "  Batch 340/910 - Loss: 0.0108\n",
      "  Batch 350/910 - Loss: 0.0052\n",
      "  Batch 360/910 - Loss: 0.0086\n",
      "  Batch 370/910 - Loss: 0.0051\n",
      "  Batch 380/910 - Loss: 0.0010\n",
      "  Batch 390/910 - Loss: 0.0242\n",
      "  Batch 400/910 - Loss: 0.0218\n",
      "  Batch 410/910 - Loss: 0.0193\n",
      "  Batch 420/910 - Loss: 0.0071\n",
      "  Batch 430/910 - Loss: 0.0007\n",
      "  Batch 440/910 - Loss: 0.0115\n",
      "  Batch 450/910 - Loss: 0.0063\n",
      "  Batch 460/910 - Loss: 0.0061\n",
      "  Batch 470/910 - Loss: 0.0085\n",
      "  Batch 480/910 - Loss: 0.0063\n",
      "  Batch 490/910 - Loss: 0.0037\n",
      "  Batch 500/910 - Loss: 0.0221\n",
      "  Batch 510/910 - Loss: 0.0042\n",
      "  Batch 520/910 - Loss: 0.0043\n",
      "  Batch 530/910 - Loss: 0.0193\n",
      "  Batch 540/910 - Loss: 0.0043\n",
      "  Batch 550/910 - Loss: 0.0077\n",
      "  Batch 560/910 - Loss: 0.0099\n",
      "  Batch 570/910 - Loss: 0.0093\n",
      "  Batch 580/910 - Loss: 0.0248\n",
      "  Batch 590/910 - Loss: 0.0122\n",
      "  Batch 600/910 - Loss: 0.0092\n",
      "  Batch 610/910 - Loss: 0.0258\n",
      "  Batch 620/910 - Loss: 0.0048\n",
      "  Batch 630/910 - Loss: 0.0028\n",
      "  Batch 640/910 - Loss: 0.0095\n",
      "  Batch 650/910 - Loss: 0.0015\n",
      "  Batch 660/910 - Loss: 0.0041\n",
      "  Batch 670/910 - Loss: 0.0035\n",
      "  Batch 680/910 - Loss: 0.0155\n",
      "  Batch 690/910 - Loss: 0.0127\n",
      "  Batch 700/910 - Loss: 0.0040\n",
      "  Batch 710/910 - Loss: 0.0338\n",
      "  Batch 720/910 - Loss: 0.0143\n",
      "  Batch 730/910 - Loss: 0.0125\n",
      "  Batch 740/910 - Loss: 0.0142\n",
      "  Batch 750/910 - Loss: 0.0041\n",
      "  Batch 760/910 - Loss: 0.0036\n",
      "  Batch 770/910 - Loss: 0.0125\n",
      "  Batch 780/910 - Loss: 0.0124\n",
      "  Batch 790/910 - Loss: 0.0135\n",
      "  Batch 800/910 - Loss: 0.0140\n",
      "  Batch 810/910 - Loss: 0.0120\n",
      "  Batch 820/910 - Loss: 0.0083\n",
      "  Batch 830/910 - Loss: 0.0080\n",
      "  Batch 840/910 - Loss: 0.0078\n",
      "  Batch 850/910 - Loss: 0.0108\n",
      "  Batch 860/910 - Loss: 0.0158\n",
      "  Batch 870/910 - Loss: 0.0307\n",
      "  Batch 880/910 - Loss: 0.0077\n",
      "  Batch 890/910 - Loss: 0.0036\n",
      "  Batch 900/910 - Loss: 0.0057\n",
      "  Batch 910/910 - Loss: 0.0082\n",
      "Training Loss: 0.0116\n",
      "Validation Loss: 0.0254\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.89      0.92      4151\n",
      "         1.0       0.55      0.79      0.65       701\n",
      "\n",
      "    accuracy                           0.88      4852\n",
      "   macro avg       0.75      0.84      0.79      4852\n",
      "weighted avg       0.90      0.88      0.88      4852\n",
      "\n",
      "Epoch 00007: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 8/20\n",
      "  Batch 10/910 - Loss: 0.0282\n",
      "  Batch 20/910 - Loss: 0.0097\n",
      "  Batch 30/910 - Loss: 0.0043\n",
      "  Batch 40/910 - Loss: 0.0103\n",
      "  Batch 50/910 - Loss: 0.0210\n",
      "  Batch 60/910 - Loss: 0.0039\n",
      "  Batch 70/910 - Loss: 0.0129\n",
      "  Batch 80/910 - Loss: 0.0085\n",
      "  Batch 90/910 - Loss: 0.0375\n",
      "  Batch 100/910 - Loss: 0.0004\n",
      "  Batch 110/910 - Loss: 0.0009\n",
      "  Batch 120/910 - Loss: 0.0019\n",
      "  Batch 130/910 - Loss: 0.0012\n",
      "  Batch 140/910 - Loss: 0.0004\n",
      "  Batch 150/910 - Loss: 0.0038\n",
      "  Batch 160/910 - Loss: 0.0006\n",
      "  Batch 170/910 - Loss: 0.0210\n",
      "  Batch 180/910 - Loss: 0.0098\n",
      "  Batch 190/910 - Loss: 0.0040\n",
      "  Batch 200/910 - Loss: 0.0059\n",
      "  Batch 210/910 - Loss: 0.0021\n",
      "  Batch 220/910 - Loss: 0.0069\n",
      "  Batch 230/910 - Loss: 0.0062\n",
      "  Batch 240/910 - Loss: 0.0107\n",
      "  Batch 250/910 - Loss: 0.0026\n",
      "  Batch 260/910 - Loss: 0.0023\n",
      "  Batch 270/910 - Loss: 0.0037\n",
      "  Batch 280/910 - Loss: 0.0051\n",
      "  Batch 290/910 - Loss: 0.0065\n",
      "  Batch 300/910 - Loss: 0.0113\n",
      "  Batch 310/910 - Loss: 0.0153\n",
      "  Batch 320/910 - Loss: 0.0022\n",
      "  Batch 330/910 - Loss: 0.0209\n",
      "  Batch 340/910 - Loss: 0.0051\n",
      "  Batch 350/910 - Loss: 0.0034\n",
      "  Batch 360/910 - Loss: 0.0013\n",
      "  Batch 370/910 - Loss: 0.0252\n",
      "  Batch 380/910 - Loss: 0.0023\n",
      "  Batch 390/910 - Loss: 0.0030\n",
      "  Batch 400/910 - Loss: 0.0430\n",
      "  Batch 410/910 - Loss: 0.0290\n",
      "  Batch 420/910 - Loss: 0.0013\n",
      "  Batch 430/910 - Loss: 0.0003\n",
      "  Batch 440/910 - Loss: 0.0303\n",
      "  Batch 450/910 - Loss: 0.0005\n",
      "  Batch 460/910 - Loss: 0.0029\n",
      "  Batch 470/910 - Loss: 0.0063\n",
      "  Batch 480/910 - Loss: 0.0029\n",
      "  Batch 490/910 - Loss: 0.0010\n",
      "  Batch 500/910 - Loss: 0.0118\n",
      "  Batch 510/910 - Loss: 0.0137\n",
      "  Batch 520/910 - Loss: 0.0081\n",
      "  Batch 530/910 - Loss: 0.0020\n",
      "  Batch 540/910 - Loss: 0.0013\n",
      "  Batch 550/910 - Loss: 0.0090\n",
      "  Batch 560/910 - Loss: 0.0017\n",
      "  Batch 570/910 - Loss: 0.0061\n",
      "  Batch 580/910 - Loss: 0.0553\n",
      "  Batch 590/910 - Loss: 0.0107\n",
      "  Batch 600/910 - Loss: 0.0003\n",
      "  Batch 610/910 - Loss: 0.0148\n",
      "  Batch 620/910 - Loss: 0.0179\n",
      "  Batch 630/910 - Loss: 0.0012\n",
      "  Batch 640/910 - Loss: 0.0060\n",
      "  Batch 650/910 - Loss: 0.0103\n",
      "  Batch 660/910 - Loss: 0.0082\n",
      "  Batch 670/910 - Loss: 0.0012\n",
      "  Batch 680/910 - Loss: 0.0049\n",
      "  Batch 690/910 - Loss: 0.0009\n",
      "  Batch 700/910 - Loss: 0.0058\n",
      "  Batch 710/910 - Loss: 0.0114\n",
      "  Batch 720/910 - Loss: 0.0024\n",
      "  Batch 730/910 - Loss: 0.0100\n",
      "  Batch 740/910 - Loss: 0.0064\n",
      "  Batch 750/910 - Loss: 0.0048\n",
      "  Batch 760/910 - Loss: 0.0046\n",
      "  Batch 770/910 - Loss: 0.0029\n",
      "  Batch 780/910 - Loss: 0.0013\n",
      "  Batch 790/910 - Loss: 0.0089\n",
      "  Batch 800/910 - Loss: 0.0021\n",
      "  Batch 810/910 - Loss: 0.0120\n",
      "  Batch 820/910 - Loss: 0.0077\n",
      "  Batch 830/910 - Loss: 0.0012\n",
      "  Batch 840/910 - Loss: 0.0048\n",
      "  Batch 850/910 - Loss: 0.0217\n",
      "  Batch 860/910 - Loss: 0.0013\n",
      "  Batch 870/910 - Loss: 0.0014\n",
      "  Batch 880/910 - Loss: 0.0143\n",
      "  Batch 890/910 - Loss: 0.0090\n",
      "  Batch 900/910 - Loss: 0.0060\n",
      "  Batch 910/910 - Loss: 0.0020\n",
      "Training Loss: 0.0071\n",
      "Validation Loss: 0.0417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.92      0.94      4151\n",
      "         1.0       0.61      0.75      0.67       701\n",
      "\n",
      "    accuracy                           0.89      4852\n",
      "   macro avg       0.78      0.83      0.80      4852\n",
      "weighted avg       0.91      0.89      0.90      4852\n",
      "\n",
      "Epoch 9/20\n",
      "  Batch 10/910 - Loss: 0.0020\n",
      "  Batch 20/910 - Loss: 0.0039\n",
      "  Batch 30/910 - Loss: 0.0022\n",
      "  Batch 40/910 - Loss: 0.0010\n",
      "  Batch 50/910 - Loss: 0.0004\n",
      "  Batch 60/910 - Loss: 0.0004\n",
      "  Batch 70/910 - Loss: 0.0091\n",
      "  Batch 80/910 - Loss: 0.0081\n",
      "  Batch 90/910 - Loss: 0.0030\n",
      "  Batch 100/910 - Loss: 0.0011\n",
      "  Batch 110/910 - Loss: 0.0047\n",
      "  Batch 120/910 - Loss: 0.0076\n",
      "  Batch 130/910 - Loss: 0.0059\n",
      "  Batch 140/910 - Loss: 0.0003\n",
      "  Batch 150/910 - Loss: 0.0013\n",
      "  Batch 160/910 - Loss: 0.0015\n",
      "  Batch 170/910 - Loss: 0.0142\n",
      "  Batch 180/910 - Loss: 0.0041\n",
      "  Batch 190/910 - Loss: 0.0027\n",
      "  Batch 200/910 - Loss: 0.0031\n",
      "  Batch 210/910 - Loss: 0.0053\n",
      "  Batch 220/910 - Loss: 0.0101\n",
      "  Batch 230/910 - Loss: 0.0042\n",
      "  Batch 240/910 - Loss: 0.0013\n",
      "  Batch 250/910 - Loss: 0.0039\n",
      "  Batch 260/910 - Loss: 0.0011\n",
      "  Batch 270/910 - Loss: 0.0051\n",
      "  Batch 280/910 - Loss: 0.0029\n",
      "  Batch 290/910 - Loss: 0.0001\n",
      "  Batch 300/910 - Loss: 0.0010\n",
      "  Batch 310/910 - Loss: 0.0097\n",
      "  Batch 320/910 - Loss: 0.0021\n",
      "  Batch 330/910 - Loss: 0.0583\n",
      "  Batch 340/910 - Loss: 0.0063\n",
      "  Batch 350/910 - Loss: 0.0120\n",
      "  Batch 360/910 - Loss: 0.0010\n",
      "  Batch 370/910 - Loss: 0.0008\n",
      "  Batch 380/910 - Loss: 0.0130\n",
      "  Batch 390/910 - Loss: 0.0009\n",
      "  Batch 400/910 - Loss: 0.0020\n",
      "  Batch 410/910 - Loss: 0.0044\n",
      "  Batch 420/910 - Loss: 0.0019\n",
      "  Batch 430/910 - Loss: 0.0102\n",
      "  Batch 440/910 - Loss: 0.0010\n",
      "  Batch 450/910 - Loss: 0.0025\n",
      "  Batch 460/910 - Loss: 0.0133\n",
      "  Batch 470/910 - Loss: 0.0018\n",
      "  Batch 480/910 - Loss: 0.0058\n",
      "  Batch 490/910 - Loss: 0.0029\n",
      "  Batch 500/910 - Loss: 0.0095\n",
      "  Batch 510/910 - Loss: 0.0009\n",
      "  Batch 520/910 - Loss: 0.0006\n",
      "  Batch 530/910 - Loss: 0.0004\n",
      "  Batch 540/910 - Loss: 0.0004\n",
      "  Batch 550/910 - Loss: 0.0169\n",
      "  Batch 560/910 - Loss: 0.0040\n",
      "  Batch 570/910 - Loss: 0.0113\n",
      "  Batch 580/910 - Loss: 0.0083\n",
      "  Batch 590/910 - Loss: 0.0084\n",
      "  Batch 600/910 - Loss: 0.0013\n",
      "  Batch 610/910 - Loss: 0.0012\n",
      "  Batch 620/910 - Loss: 0.0159\n",
      "  Batch 630/910 - Loss: 0.0015\n",
      "  Batch 640/910 - Loss: 0.0020\n",
      "  Batch 650/910 - Loss: 0.0111\n",
      "  Batch 660/910 - Loss: 0.0004\n",
      "  Batch 670/910 - Loss: 0.0156\n",
      "  Batch 680/910 - Loss: 0.0064\n",
      "  Batch 690/910 - Loss: 0.0004\n",
      "  Batch 700/910 - Loss: 0.0004\n",
      "  Batch 710/910 - Loss: 0.0019\n",
      "  Batch 720/910 - Loss: 0.0092\n",
      "  Batch 730/910 - Loss: 0.0020\n",
      "  Batch 740/910 - Loss: 0.0011\n",
      "  Batch 750/910 - Loss: 0.0035\n",
      "  Batch 760/910 - Loss: 0.0033\n",
      "  Batch 770/910 - Loss: 0.0012\n",
      "  Batch 780/910 - Loss: 0.0144\n",
      "  Batch 790/910 - Loss: 0.0034\n",
      "  Batch 800/910 - Loss: 0.0011\n",
      "  Batch 810/910 - Loss: 0.0027\n",
      "  Batch 820/910 - Loss: 0.0014\n",
      "  Batch 830/910 - Loss: 0.0147\n",
      "  Batch 840/910 - Loss: 0.0051\n",
      "  Batch 850/910 - Loss: 0.0027\n",
      "  Batch 860/910 - Loss: 0.0399\n",
      "  Batch 870/910 - Loss: 0.0029\n",
      "  Batch 880/910 - Loss: 0.0111\n",
      "  Batch 890/910 - Loss: 0.0052\n",
      "  Batch 900/910 - Loss: 0.0035\n",
      "  Batch 910/910 - Loss: 0.0002\n",
      "Training Loss: 0.0057\n",
      "Validation Loss: 0.0571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.93      0.94      4151\n",
      "         1.0       0.63      0.73      0.68       701\n",
      "\n",
      "    accuracy                           0.90      4852\n",
      "   macro avg       0.79      0.83      0.81      4852\n",
      "weighted avg       0.91      0.90      0.90      4852\n",
      "\n",
      "Epoch 10/20\n",
      "  Batch 10/910 - Loss: 0.0059\n",
      "  Batch 20/910 - Loss: 0.0024\n",
      "  Batch 30/910 - Loss: 0.0005\n",
      "  Batch 40/910 - Loss: 0.0032\n",
      "  Batch 50/910 - Loss: 0.0109\n",
      "  Batch 60/910 - Loss: 0.0174\n",
      "  Batch 70/910 - Loss: 0.0033\n",
      "  Batch 80/910 - Loss: 0.0013\n",
      "  Batch 90/910 - Loss: 0.0008\n",
      "  Batch 100/910 - Loss: 0.0011\n",
      "  Batch 110/910 - Loss: 0.0007\n",
      "  Batch 120/910 - Loss: 0.0262\n",
      "  Batch 130/910 - Loss: 0.0006\n",
      "  Batch 140/910 - Loss: 0.0371\n",
      "  Batch 150/910 - Loss: 0.0058\n",
      "  Batch 160/910 - Loss: 0.0109\n",
      "  Batch 170/910 - Loss: 0.0010\n",
      "  Batch 180/910 - Loss: 0.0006\n",
      "  Batch 190/910 - Loss: 0.0117\n",
      "  Batch 200/910 - Loss: 0.0123\n",
      "  Batch 210/910 - Loss: 0.0041\n",
      "  Batch 220/910 - Loss: 0.0069\n",
      "  Batch 230/910 - Loss: 0.0054\n",
      "  Batch 240/910 - Loss: 0.0101\n",
      "  Batch 250/910 - Loss: 0.0037\n",
      "  Batch 260/910 - Loss: 0.0011\n",
      "  Batch 270/910 - Loss: 0.0008\n",
      "  Batch 280/910 - Loss: 0.0044\n",
      "  Batch 290/910 - Loss: 0.0005\n",
      "  Batch 300/910 - Loss: 0.0018\n",
      "  Batch 310/910 - Loss: 0.0001\n",
      "  Batch 320/910 - Loss: 0.0017\n",
      "  Batch 330/910 - Loss: 0.0052\n",
      "  Batch 340/910 - Loss: 0.0166\n",
      "  Batch 350/910 - Loss: 0.0005\n",
      "  Batch 360/910 - Loss: 0.0036\n",
      "  Batch 370/910 - Loss: 0.0030\n",
      "  Batch 380/910 - Loss: 0.0060\n",
      "  Batch 390/910 - Loss: 0.0012\n",
      "  Batch 400/910 - Loss: 0.0082\n",
      "  Batch 410/910 - Loss: 0.0177\n",
      "  Batch 420/910 - Loss: 0.0031\n",
      "  Batch 430/910 - Loss: 0.0000\n",
      "  Batch 440/910 - Loss: 0.0056\n",
      "  Batch 450/910 - Loss: 0.0019\n",
      "  Batch 460/910 - Loss: 0.0039\n",
      "  Batch 470/910 - Loss: 0.0052\n",
      "  Batch 480/910 - Loss: 0.0096\n",
      "  Batch 490/910 - Loss: 0.0006\n",
      "  Batch 500/910 - Loss: 0.0008\n",
      "  Batch 510/910 - Loss: 0.0218\n",
      "  Batch 520/910 - Loss: 0.0003\n",
      "  Batch 530/910 - Loss: 0.0015\n",
      "  Batch 540/910 - Loss: 0.0022\n",
      "  Batch 550/910 - Loss: 0.0025\n",
      "  Batch 560/910 - Loss: 0.0034\n",
      "  Batch 570/910 - Loss: 0.0008\n",
      "  Batch 580/910 - Loss: 0.0011\n",
      "  Batch 590/910 - Loss: 0.0086\n",
      "  Batch 600/910 - Loss: 0.0008\n",
      "  Batch 610/910 - Loss: 0.0008\n",
      "  Batch 620/910 - Loss: 0.0038\n",
      "  Batch 630/910 - Loss: 0.0003\n",
      "  Batch 640/910 - Loss: 0.0053\n",
      "  Batch 650/910 - Loss: 0.0067\n",
      "  Batch 660/910 - Loss: 0.0004\n",
      "  Batch 670/910 - Loss: 0.0020\n",
      "  Batch 680/910 - Loss: 0.0030\n",
      "  Batch 690/910 - Loss: 0.0126\n",
      "  Batch 700/910 - Loss: 0.0103\n",
      "  Batch 710/910 - Loss: 0.0007\n",
      "  Batch 720/910 - Loss: 0.0011\n",
      "  Batch 730/910 - Loss: 0.0142\n",
      "  Batch 740/910 - Loss: 0.0004\n",
      "  Batch 750/910 - Loss: 0.0035\n",
      "  Batch 760/910 - Loss: 0.0004\n",
      "  Batch 770/910 - Loss: 0.0039\n",
      "  Batch 780/910 - Loss: 0.0003\n",
      "  Batch 790/910 - Loss: 0.0022\n",
      "  Batch 800/910 - Loss: 0.0014\n",
      "  Batch 810/910 - Loss: 0.0004\n",
      "  Batch 820/910 - Loss: 0.0084\n",
      "  Batch 830/910 - Loss: 0.0030\n",
      "  Batch 840/910 - Loss: 0.0045\n",
      "  Batch 850/910 - Loss: 0.0011\n",
      "  Batch 860/910 - Loss: 0.0078\n",
      "  Batch 870/910 - Loss: 0.0114\n",
      "  Batch 880/910 - Loss: 0.0030\n",
      "  Batch 890/910 - Loss: 0.0165\n",
      "  Batch 900/910 - Loss: 0.0027\n",
      "  Batch 910/910 - Loss: 0.0108\n",
      "Training Loss: 0.0049\n",
      "Validation Loss: 0.0635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.93      0.94      4151\n",
      "         1.0       0.65      0.71      0.68       701\n",
      "\n",
      "    accuracy                           0.90      4852\n",
      "   macro avg       0.80      0.82      0.81      4852\n",
      "weighted avg       0.91      0.90      0.90      4852\n",
      "\n",
      "Epoch 00010: reducing learning rate of group 0 to 8.0000e-07.\n",
      "Epoch 11/20\n",
      "  Batch 10/910 - Loss: 0.0072\n",
      "  Batch 20/910 - Loss: 0.0065\n",
      "  Batch 30/910 - Loss: 0.0012\n",
      "  Batch 40/910 - Loss: 0.0059\n",
      "  Batch 50/910 - Loss: 0.0007\n",
      "  Batch 60/910 - Loss: 0.0100\n",
      "  Batch 70/910 - Loss: 0.0030\n",
      "  Batch 80/910 - Loss: 0.0048\n",
      "  Batch 90/910 - Loss: 0.0008\n",
      "  Batch 100/910 - Loss: 0.0025\n",
      "  Batch 110/910 - Loss: 0.0027\n",
      "  Batch 120/910 - Loss: 0.0096\n",
      "  Batch 130/910 - Loss: 0.0111\n",
      "  Batch 140/910 - Loss: 0.0001\n",
      "  Batch 150/910 - Loss: 0.0006\n",
      "  Batch 160/910 - Loss: 0.0212\n",
      "  Batch 170/910 - Loss: 0.0005\n",
      "  Batch 180/910 - Loss: 0.0057\n",
      "  Batch 190/910 - Loss: 0.0001\n",
      "  Batch 200/910 - Loss: 0.0013\n",
      "  Batch 210/910 - Loss: 0.0000\n",
      "  Batch 220/910 - Loss: 0.0002\n",
      "  Batch 230/910 - Loss: 0.0006\n",
      "  Batch 240/910 - Loss: 0.0006\n",
      "  Batch 250/910 - Loss: 0.0045\n",
      "  Batch 260/910 - Loss: 0.0028\n",
      "  Batch 270/910 - Loss: 0.0035\n",
      "  Batch 280/910 - Loss: 0.0007\n",
      "  Batch 290/910 - Loss: 0.0007\n",
      "  Batch 300/910 - Loss: 0.0002\n",
      "  Batch 310/910 - Loss: 0.0036\n",
      "  Batch 320/910 - Loss: 0.0001\n",
      "  Batch 330/910 - Loss: 0.0006\n",
      "  Batch 340/910 - Loss: 0.0002\n",
      "  Batch 350/910 - Loss: 0.0003\n",
      "  Batch 360/910 - Loss: 0.0007\n",
      "  Batch 370/910 - Loss: 0.0037\n",
      "  Batch 380/910 - Loss: 0.0165\n",
      "  Batch 390/910 - Loss: 0.0006\n",
      "  Batch 400/910 - Loss: 0.0005\n",
      "  Batch 410/910 - Loss: 0.0021\n",
      "  Batch 420/910 - Loss: 0.0005\n",
      "  Batch 430/910 - Loss: 0.0001\n",
      "  Batch 440/910 - Loss: 0.0012\n",
      "  Batch 450/910 - Loss: 0.0042\n",
      "  Batch 460/910 - Loss: 0.0006\n",
      "  Batch 470/910 - Loss: 0.0011\n",
      "  Batch 480/910 - Loss: 0.0025\n",
      "  Batch 490/910 - Loss: 0.0077\n",
      "  Batch 500/910 - Loss: 0.0050\n",
      "  Batch 510/910 - Loss: 0.0001\n",
      "  Batch 520/910 - Loss: 0.0001\n",
      "  Batch 530/910 - Loss: 0.0058\n",
      "  Batch 540/910 - Loss: 0.0019\n",
      "  Batch 550/910 - Loss: 0.0074\n",
      "  Batch 560/910 - Loss: 0.0005\n",
      "  Batch 570/910 - Loss: 0.0001\n",
      "  Batch 580/910 - Loss: 0.0006\n",
      "  Batch 590/910 - Loss: 0.0019\n",
      "  Batch 600/910 - Loss: 0.0013\n",
      "  Batch 610/910 - Loss: 0.0007\n",
      "  Batch 620/910 - Loss: 0.0010\n",
      "  Batch 630/910 - Loss: 0.0000\n",
      "  Batch 640/910 - Loss: 0.0035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 117\u001b[0m\n\u001b[0;32m    111\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    114\u001b[0m }\n\u001b[0;32m    116\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 117\u001b[0m outputs \u001b[38;5;241m=\u001b[39m XLNet_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    118\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    119\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels)\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1543\u001b[0m, in \u001b[0;36mXLNetForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1543\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1544\u001b[0m     input_ids,\n\u001b[0;32m   1545\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1546\u001b[0m     mems\u001b[38;5;241m=\u001b[39mmems,\n\u001b[0;32m   1547\u001b[0m     perm_mask\u001b[38;5;241m=\u001b[39mperm_mask,\n\u001b[0;32m   1548\u001b[0m     target_mapping\u001b[38;5;241m=\u001b[39mtarget_mapping,\n\u001b[0;32m   1549\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1550\u001b[0m     input_mask\u001b[38;5;241m=\u001b[39minput_mask,\n\u001b[0;32m   1551\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1552\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1553\u001b[0m     use_mems\u001b[38;5;241m=\u001b[39muse_mems,\n\u001b[0;32m   1554\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1555\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1556\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1558\u001b[0m )\n\u001b[0;32m   1559\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1561\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_summary(output)\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1233\u001b[0m, in \u001b[0;36mXLNetModel.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m   1231\u001b[0m     hidden_states\u001b[38;5;241m.\u001b[39mappend((output_h, output_g) \u001b[38;5;28;01mif\u001b[39;00m output_g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m output_h)\n\u001b[1;32m-> 1233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_tgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask_g\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1245\u001b[0m output_h, output_g \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:504\u001b[0m, in \u001b[0;36mXLNetLayer.forward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    493\u001b[0m     output_h,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    502\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    503\u001b[0m ):\n\u001b[1;32m--> 504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     output_h, output_g \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:435\u001b[0m, in \u001b[0;36mXLNetRelativeAttention.forward\u001b[1;34m(self, h, g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    432\u001b[0m k_head_r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibh,hnd->ibnd\u001b[39m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# core attention ops\u001b[39;00m\n\u001b[1;32m--> 435\u001b[0m attn_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_attn_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_head_r\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    447\u001b[0m     attn_vec, attn_prob \u001b[38;5;241m=\u001b[39m attn_vec\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:290\u001b[0m, in \u001b[0;36mXLNetRelativeAttention.rel_attn_core\u001b[1;34m(self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    287\u001b[0m attn_score \u001b[38;5;241m=\u001b[39m (ac \u001b[38;5;241m+\u001b[39m bd \u001b[38;5;241m+\u001b[39m ef) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m:\n\u001b[0;32m    291\u001b[0m         attn_score \u001b[38;5;241m=\u001b[39m attn_score \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m65500\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mijbn->bnij\u001b[39m\u001b[38;5;124m\"\u001b[39m, attn_mask)\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  \n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        encoding = self.tokenizer(\n",
    "            # Replace 'content' with 'article_content' if working with real data\n",
    "            # Replace 'title' with 'source' too\n",
    "            row['title'], row['content'], \n",
    "            add_special_tokens=True,\n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt' #  return the output in the form of PyTorch tensors\n",
    "            )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(row['relevant'], dtype=torch.float), # for BCEWithLogitsLoss use float\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(train_df, tokenizer, max_length=256)\n",
    "val_dataset = TextDataset(val_df, tokenizer, max_length=256)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# load the XLNet model\n",
    "XLNet_model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=1)\n",
    "XLNet_model.to(device)\n",
    "\n",
    "# Define a Focal Loss class\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        # Note: When building custom loss functions that modify existing \n",
    "        # ones, it's common practice to use the functional versions as building blocks, then\n",
    "        # wrap them in a class that inherits from nn.Module\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Using the functional version of BCEWithLogitsLoss so that we can get the unreduced\n",
    "        # losses (for the Hadamard product)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        # For targets=1, we use p, for targets=0, we use 1-p\n",
    "        pt = probs * targets + (1 - probs) * (1 - targets)\n",
    "        \n",
    "        # Focusing term\n",
    "        focal_weight = (1 - pt).pow(self.gamma)\n",
    "        \n",
    "        # Apply alpha weighting\n",
    "        if self.alpha is not None:\n",
    "            # Alpha for positive samples, 1-alpha for negative samples\n",
    "            alpha_weight = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            focal_weight = focal_weight * alpha_weight\n",
    "            \n",
    "        # Compute the loss using Hadamard Product\n",
    "        focal_loss = focal_weight * bce_loss\n",
    "        \n",
    "        # Apply reduction\n",
    "        return torch.mean(focal_loss)\n",
    "\n",
    "# Compute a reasonable alpha from class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['relevant']), y=train_df['relevant'])\n",
    "alpha = class_weights[1]  / (class_weights[0] + class_weights[1] )  # Normalize to get alpha between 0 and 1\n",
    "\n",
    "# Initialize focal loss and set up the optimizer\n",
    "loss_fn = FocalLoss(alpha=alpha, gamma=2)  # gamma=2 is a common default value\n",
    "optimizer = AdamW(XLNet_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.2, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "current_epoch = 1\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(current_epoch, num_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    XLNet_model.train()  # Switch to training mode\n",
    "    train_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad() # Clear old gradients\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = XLNet_model(**inputs)\n",
    "        logits = outputs.logits.squeeze(-1)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(train_dataloader) # average training loss\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    XLNet_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "            }\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = XLNet_model(**inputs)\n",
    "            logits = outputs.logits.squeeze(-1)\n",
    "            val_loss += loss_fn(logits, labels).item()\n",
    "            preds = torch.sigmoid(logits).cpu().numpy() > 0.5  # threshold = 0.5 for  now\n",
    "            val_preds.extend(preds)           \n",
    "            val_labels.extend(batch['labels'].cpu().numpy())\n",
    "            \n",
    "    val_loss /= len(val_dataloader) # average validation loss\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(classification_report(val_labels, val_preds))\n",
    "    # step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved. Saving model.\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': XLNet_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, f\"best_XLNet_model_epoch_{epoch}_FocalLoss_gamma{loss_fn.gamma}_alpha{loss_fn.alpha:.2f}.pt\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "XLNet_model.eval()\n",
    "y_probs, y_true = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device),\n",
    "        }\n",
    "        outputs = XLNet_model(**inputs)\n",
    "        val_loss += outputs.loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.sigmoid(logits).cpu().numpy()\n",
    "        y_probs.extend(preds)\n",
    "        y_true.extend(inputs['labels'].cpu().numpy())\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(recall, precision, color='b', label=\"Precision-Recall curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Compute F1 scores \n",
    "f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the performance with different threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(test_df, tokenizer, max_length=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "XLNet_model.eval()\n",
    "y_preds, y_true = [], []\n",
    "with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device),\n",
    "            }\n",
    "            outputs = XLNet_model(**inputs)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.sigmoid(logits).cpu().numpy() > 0.55 # testing threshold = 0.65 for now\n",
    "            y_preds.extend(preds)\n",
    "            y_true.extend(inputs['labels'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93      4158\n",
      "         1.0       0.58      0.52      0.55       694\n",
      "\n",
      "    accuracy                           0.88      4852\n",
      "   macro avg       0.75      0.73      0.74      4852\n",
      "weighted avg       0.87      0.88      0.87      4852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
