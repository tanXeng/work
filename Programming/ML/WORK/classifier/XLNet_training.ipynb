{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>full_content</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89541</td>\n",
       "      <td>International Business Times</td>\n",
       "      <td>UN Chief Urges World To 'Stop The Madness' Of ...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89542</td>\n",
       "      <td>Prtimes.jp</td>\n",
       "      <td>RANDEBOOよりワンランク上の大人っぽさが漂うニットとベストが新登場。</td>\n",
       "      <td>[株式会社Ainer]\\nRANDEBOO（ランデブー）では2023年7月18日(火)より公...</td>\n",
       "      <td>RANDEBOO2023718()WEB2023 Autumn Winter \\n\"Nepa...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89543</td>\n",
       "      <td>VOA News</td>\n",
       "      <td>UN Chief Urges World to 'Stop the Madness' of ...</td>\n",
       "      <td>UN Secretary-General Antonio Guterres urged th...</td>\n",
       "      <td>Kathmandu, Nepal  UN Secretary-General Antonio...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89545</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Sikkim warning: Hydroelectricity push must be ...</td>\n",
       "      <td>Ecologists caution against the adverse effects...</td>\n",
       "      <td>At least 14 persons lost their lives and more ...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>At least 14 persons lost their lives and more ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89547</td>\n",
       "      <td>The Times of Israel</td>\n",
       "      <td>200 foreigners, dual nationals cut down in Ham...</td>\n",
       "      <td>France lost 35 citizens, Thailand 33, US 31, U...</td>\n",
       "      <td>Scores of foreign citizens were killed, taken ...</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105370</th>\n",
       "      <td>781108</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Have done no wrong, only did party work, says ...</td>\n",
       "      <td>The High Court today allowed Shivakumar to wit...</td>\n",
       "      <td>Karnataka Deputy Chief Minister D K Shivakumar...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Karnataka Deputy Chief Minister D K Shivakumar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105371</th>\n",
       "      <td>781129</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>FC Barcelona Guarantees $77.6 Million Champion...</td>\n",
       "      <td>FC Barcelona have guaranteed at least $77.6 mi...</td>\n",
       "      <td>FC Barcelona have guaranteed at least $767.6 m...</td>\n",
       "      <td>Home</td>\n",
       "      <td>FC Barcelona have guaranteed at least $767.6 m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105372</th>\n",
       "      <td>781235</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Three hospitals ignored her gravely ill fiancé...</td>\n",
       "      <td>Forty years ago, Sarah Lubarsky came home from...</td>\n",
       "      <td>The photo from David and Sarah Lubarsky's wedd...</td>\n",
       "      <td>Home</td>\n",
       "      <td>The photo from David and Sarah Lubarsky's wedd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105373</th>\n",
       "      <td>781240</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Kerber’s Farm: Bringing Farm To Table To Manha...</td>\n",
       "      <td>A farmstand in Long Island, Kerber’s Farms has...</td>\n",
       "      <td>Kerbers Farm: Bringing Farm To Table To Manhat...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Kerber’s Farm: Bringing Farm To Table To Manha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105374</th>\n",
       "      <td>781308</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Tips For Investing In Short-Term Rentals In Dubai</td>\n",
       "      <td>By exploring your options and keeping a few be...</td>\n",
       "      <td>Cofounder at UpperKey. Passionate about proper...</td>\n",
       "      <td>Home</td>\n",
       "      <td>Cofounder at UpperKey. Passionate about proper...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105375 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id                   source_name  \\\n",
       "0            89541  International Business Times   \n",
       "1            89542                    Prtimes.jp   \n",
       "2            89543                      VOA News   \n",
       "3            89545            The Indian Express   \n",
       "4            89547           The Times of Israel   \n",
       "...            ...                           ...   \n",
       "105370      781108            The Indian Express   \n",
       "105371      781129                        Forbes   \n",
       "105372      781235                           NPR   \n",
       "105373      781240                        Forbes   \n",
       "105374      781308                        Forbes   \n",
       "\n",
       "                                                    title  \\\n",
       "0       UN Chief Urges World To 'Stop The Madness' Of ...   \n",
       "1                   RANDEBOOよりワンランク上の大人っぽさが漂うニットとベストが新登場。   \n",
       "2       UN Chief Urges World to 'Stop the Madness' of ...   \n",
       "3       Sikkim warning: Hydroelectricity push must be ...   \n",
       "4       200 foreigners, dual nationals cut down in Ham...   \n",
       "...                                                   ...   \n",
       "105370  Have done no wrong, only did party work, says ...   \n",
       "105371  FC Barcelona Guarantees $77.6 Million Champion...   \n",
       "105372  Three hospitals ignored her gravely ill fiancé...   \n",
       "105373  Kerber’s Farm: Bringing Farm To Table To Manha...   \n",
       "105374  Tips For Investing In Short-Term Rentals In Dubai   \n",
       "\n",
       "                                              description  \\\n",
       "0       UN Secretary-General Antonio Guterres urged th...   \n",
       "1       [株式会社Ainer]\\nRANDEBOO（ランデブー）では2023年7月18日(火)より公...   \n",
       "2       UN Secretary-General Antonio Guterres urged th...   \n",
       "3       Ecologists caution against the adverse effects...   \n",
       "4       France lost 35 citizens, Thailand 33, US 31, U...   \n",
       "...                                                   ...   \n",
       "105370  The High Court today allowed Shivakumar to wit...   \n",
       "105371  FC Barcelona have guaranteed at least $77.6 mi...   \n",
       "105372  Forty years ago, Sarah Lubarsky came home from...   \n",
       "105373  A farmstand in Long Island, Kerber’s Farms has...   \n",
       "105374  By exploring your options and keeping a few be...   \n",
       "\n",
       "                                                  content category  \\\n",
       "0       UN Secretary-General Antonio Guterres urged th...    Nepal   \n",
       "1       RANDEBOO2023718()WEB2023 Autumn Winter \\n\"Nepa...    Nepal   \n",
       "2       Kathmandu, Nepal  UN Secretary-General Antonio...    Nepal   \n",
       "3       At least 14 persons lost their lives and more ...    Nepal   \n",
       "4       Scores of foreign citizens were killed, taken ...    Nepal   \n",
       "...                                                   ...      ...   \n",
       "105370  Karnataka Deputy Chief Minister D K Shivakumar...     Home   \n",
       "105371  FC Barcelona have guaranteed at least $767.6 m...     Home   \n",
       "105372  The photo from David and Sarah Lubarsky's wedd...     Home   \n",
       "105373  Kerbers Farm: Bringing Farm To Table To Manhat...     Home   \n",
       "105374  Cofounder at UpperKey. Passionate about proper...     Home   \n",
       "\n",
       "                                             full_content  relevant  \n",
       "0       UN Secretary-General Antonio Guterres urged th...         0  \n",
       "1                                                     NaN         0  \n",
       "2                                                     NaN         0  \n",
       "3       At least 14 persons lost their lives and more ...         0  \n",
       "4                                                     NaN         0  \n",
       "...                                                   ...       ...  \n",
       "105370  Karnataka Deputy Chief Minister D K Shivakumar...         0  \n",
       "105371  FC Barcelona have guaranteed at least $767.6 m...         0  \n",
       "105372  The photo from David and Sarah Lubarsky's wedd...         0  \n",
       "105373  Kerber’s Farm: Bringing Farm To Table To Manha...         0  \n",
       "105374  Cofounder at UpperKey. Passionate about proper...         0  \n",
       "\n",
       "[105375 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'C:\\\\Users\\\\tanxe\\\\Programming\\\\ML\\\\WORK\\\\classifier\\\\data.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(columns=['source_id', 'author', 'published_at', 'url_to_image', 'url' ])\n",
    "filtered_df = df\n",
    "filtered_df['relevant'] = filtered_df['category'].apply(lambda x: 1 if x == 'Stock' or x == 'Finance' else 0)\n",
    "df_cleaned = filtered_df.dropna(subset=['content'])\n",
    "balanced_df = df_cleaned\n",
    "balanced_df\n",
    "\n",
    "# filtered_df = df[df['source_name'].isin(['GlobeNewswire', 'The Times of India'])]\n",
    "\n",
    "# filtered_df['relevant'] = filtered_df['category'].apply(lambda x: 1 if x == 'COVID' else 0)\n",
    "# df_cleaned = filtered_df.dropna(subset=['full_content'])\n",
    "# df_relevant_zero = df_cleaned[df_cleaned['relevant'] == 0]\n",
    "# df_relevant_one = df_cleaned[df_cleaned['relevant'] == 1]\n",
    "# df_sampled = df_relevant_zero.sample(n=1400, random_state=42)\n",
    "# balanced_df = pd.concat([df_sampled, df_relevant_one], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanxe\\AppData\\Local\\Temp\\ipykernel_3520\\2424080986.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  balanced_df_24k['relevant'] = balanced_df_24k['category'].apply(lambda x: 1 if x == 'Stock' else 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "relevant\n",
       "0    20757\n",
       "1     3503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df_24k = balanced_df[balanced_df['source_name'].isin([\"ETF Daily News\", \"The Times of India\"])]\n",
    "balanced_df_24k['relevant'] = balanced_df_24k['category'].apply(lambda x: 1 if x == 'Stock' else 0)\n",
    "balanced_df_24k = balanced_df_24k.dropna(subset=['content'])\n",
    "balanced_df_24k['relevant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>full_content</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3109</th>\n",
       "      <td>94343</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>These 9 commodity stocks hit 52-week high on T...</td>\n",
       "      <td>During Thursday's trading session, the Sensex ...</td>\n",
       "      <td>Nov 02, 2023, 07:22:41 PM IST\\nDuring Thursday...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57910</th>\n",
       "      <td>133924</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Fundamental Radar: Varun Beverages poised to b...</td>\n",
       "      <td>Varun Beverages Ltd is the second-largest fran...</td>\n",
       "      <td>SynopsisVarun Beverages Ltd is the second-larg...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57935</th>\n",
       "      <td>134021</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Stock market update: Mining stocks up as marke...</td>\n",
       "      <td>The 30-share BSE Sensex was  up  425.32 points...</td>\n",
       "      <td>NEW DELHI: Mining stocks were trading higher o...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Getty Images Nifty moved in a tight range of 8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57936</th>\n",
       "      <td>134022</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Stock market update: Fertilisers stocks up as ...</td>\n",
       "      <td>The 30-share BSE Sensex was  up  441.31 points...</td>\n",
       "      <td>NEW DELHI: Fertilisers stocks were trading hig...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Getty Images NEW DELHI: Fertilisers stocks wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57937</th>\n",
       "      <td>134023</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>InterGlobe stock price up 0.08 per cent as Sen...</td>\n",
       "      <td>As of 30-Sep-2023, promoters held 38.02 per ce...</td>\n",
       "      <td>Shares of InterGlobe Aviation Ltd. rose 0.08 p...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Reuters On an immediate basis, 15,770/52,500 a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102076</th>\n",
       "      <td>693939</td>\n",
       "      <td>ETF Daily News</td>\n",
       "      <td>Universal (NYSE:UVV) vs. British American Toba...</td>\n",
       "      <td>Universal (NYSE:UVV – Get Free Report) and Bri...</td>\n",
       "      <td>Universal (NYSE:UVV – Get Free Report) and Bri...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Universal (NYSE:UVV–Get Free Report) and Briti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102077</th>\n",
       "      <td>693944</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Do we have enough retail money in debt markets?</td>\n",
       "      <td>​​For example, as per the monthly data release...</td>\n",
       "      <td>Generally, the retail investors are late to th...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>IANS INSIGHTS  \\n    \\t                    Rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102078</th>\n",
       "      <td>693947</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>FII action, OPEC+ meet among top 10 factors to...</td>\n",
       "      <td>Meena expects the market to experience some di...</td>\n",
       "      <td>Indian frontline indices S&amp;amp;P BSE Sensex an...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>ETMarkets.com Indian frontline indices S&amp;P BSE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102079</th>\n",
       "      <td>693954</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>For workers at this iPhone plant, Tata means a...</td>\n",
       "      <td>At the Narasapura facility, the recent takeove...</td>\n",
       "      <td>It is the Tata tag we aim for, who doesnt want...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>“It is the  Tata  tag we aim for, who doesn’t ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102820</th>\n",
       "      <td>711912</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Riding on the digitization of Indian capital m...</td>\n",
       "      <td>\"The BSE derivatives market share jumped to 14...</td>\n",
       "      <td>Stating Indian exchanges are benefiting from f...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Reuters Bombay Stock Exchange Related FII acti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3503 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id         source_name  \\\n",
       "3109         94343  The Times of India   \n",
       "57910       133924  The Times of India   \n",
       "57935       134021  The Times of India   \n",
       "57936       134022  The Times of India   \n",
       "57937       134023  The Times of India   \n",
       "...            ...                 ...   \n",
       "102076      693939      ETF Daily News   \n",
       "102077      693944  The Times of India   \n",
       "102078      693947  The Times of India   \n",
       "102079      693954  The Times of India   \n",
       "102820      711912  The Times of India   \n",
       "\n",
       "                                                    title  \\\n",
       "3109    These 9 commodity stocks hit 52-week high on T...   \n",
       "57910   Fundamental Radar: Varun Beverages poised to b...   \n",
       "57935   Stock market update: Mining stocks up as marke...   \n",
       "57936   Stock market update: Fertilisers stocks up as ...   \n",
       "57937   InterGlobe stock price up 0.08 per cent as Sen...   \n",
       "...                                                   ...   \n",
       "102076  Universal (NYSE:UVV) vs. British American Toba...   \n",
       "102077    Do we have enough retail money in debt markets?   \n",
       "102078  FII action, OPEC+ meet among top 10 factors to...   \n",
       "102079  For workers at this iPhone plant, Tata means a...   \n",
       "102820  Riding on the digitization of Indian capital m...   \n",
       "\n",
       "                                              description  \\\n",
       "3109    During Thursday's trading session, the Sensex ...   \n",
       "57910   Varun Beverages Ltd is the second-largest fran...   \n",
       "57935   The 30-share BSE Sensex was  up  425.32 points...   \n",
       "57936   The 30-share BSE Sensex was  up  441.31 points...   \n",
       "57937   As of 30-Sep-2023, promoters held 38.02 per ce...   \n",
       "...                                                   ...   \n",
       "102076  Universal (NYSE:UVV – Get Free Report) and Bri...   \n",
       "102077  ​​For example, as per the monthly data release...   \n",
       "102078  Meena expects the market to experience some di...   \n",
       "102079  At the Narasapura facility, the recent takeove...   \n",
       "102820  \"The BSE derivatives market share jumped to 14...   \n",
       "\n",
       "                                                  content category  \\\n",
       "3109    Nov 02, 2023, 07:22:41 PM IST\\nDuring Thursday...    Stock   \n",
       "57910   SynopsisVarun Beverages Ltd is the second-larg...    Stock   \n",
       "57935   NEW DELHI: Mining stocks were trading higher o...    Stock   \n",
       "57936   NEW DELHI: Fertilisers stocks were trading hig...    Stock   \n",
       "57937   Shares of InterGlobe Aviation Ltd. rose 0.08 p...    Stock   \n",
       "...                                                   ...      ...   \n",
       "102076  Universal (NYSE:UVV – Get Free Report) and Bri...    Stock   \n",
       "102077  Generally, the retail investors are late to th...    Stock   \n",
       "102078  Indian frontline indices S&amp;P BSE Sensex an...    Stock   \n",
       "102079  It is the Tata tag we aim for, who doesnt want...    Stock   \n",
       "102820  Stating Indian exchanges are benefiting from f...    Stock   \n",
       "\n",
       "                                             full_content  relevant  \n",
       "3109                                                  NaN         1  \n",
       "57910                                                 NaN         1  \n",
       "57935   Getty Images Nifty moved in a tight range of 8...         1  \n",
       "57936   Getty Images NEW DELHI: Fertilisers stocks wer...         1  \n",
       "57937   Reuters On an immediate basis, 15,770/52,500 a...         1  \n",
       "...                                                   ...       ...  \n",
       "102076  Universal (NYSE:UVV–Get Free Report) and Briti...         1  \n",
       "102077  IANS INSIGHTS  \\n    \\t                    Rea...         1  \n",
       "102078  ETMarkets.com Indian frontline indices S&P BSE...         1  \n",
       "102079  “It is the  Tata  tag we aim for, who doesn’t ...         1  \n",
       "102820  Reuters Bombay Stock Exchange Related FII acti...         1  \n",
       "\n",
       "[3503 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df_24k[balanced_df_24k['relevant'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevant\n",
       "0    12450\n",
       "1     2106\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(balanced_df_24k, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df['relevant'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # This should return True if CUDA is available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "  Batch 10/910 - Loss: 0.0471\n",
      "  Batch 20/910 - Loss: 0.0953\n",
      "  Batch 30/910 - Loss: 0.0498\n",
      "  Batch 40/910 - Loss: 0.1012\n",
      "  Batch 50/910 - Loss: 0.0361\n",
      "  Batch 60/910 - Loss: 0.0351\n",
      "  Batch 70/910 - Loss: 0.1042\n",
      "  Batch 80/910 - Loss: 0.1007\n",
      "  Batch 90/910 - Loss: 0.0428\n",
      "  Batch 100/910 - Loss: 0.0481\n",
      "  Batch 110/910 - Loss: 0.0825\n",
      "  Batch 120/910 - Loss: 0.0665\n",
      "  Batch 130/910 - Loss: 0.0428\n",
      "  Batch 140/910 - Loss: 0.0936\n",
      "  Batch 150/910 - Loss: 0.0943\n",
      "  Batch 160/910 - Loss: 0.1505\n",
      "  Batch 170/910 - Loss: 0.0314\n",
      "  Batch 180/910 - Loss: 0.0834\n",
      "  Batch 190/910 - Loss: 0.1200\n",
      "  Batch 200/910 - Loss: 0.1305\n",
      "  Batch 210/910 - Loss: 0.1035\n",
      "  Batch 220/910 - Loss: 0.0661\n",
      "  Batch 230/910 - Loss: 0.1401\n",
      "  Batch 240/910 - Loss: 0.1590\n",
      "  Batch 250/910 - Loss: 0.1222\n",
      "  Batch 260/910 - Loss: 0.1267\n",
      "  Batch 270/910 - Loss: 0.0265\n",
      "  Batch 280/910 - Loss: 0.1443\n",
      "  Batch 290/910 - Loss: 0.0056\n",
      "  Batch 300/910 - Loss: 0.0501\n",
      "  Batch 310/910 - Loss: 0.0660\n",
      "  Batch 320/910 - Loss: 0.0790\n",
      "  Batch 330/910 - Loss: 0.0105\n",
      "  Batch 340/910 - Loss: 0.0153\n",
      "  Batch 350/910 - Loss: 0.0886\n",
      "  Batch 360/910 - Loss: 0.1838\n",
      "  Batch 370/910 - Loss: 0.0192\n",
      "  Batch 380/910 - Loss: 0.0240\n",
      "  Batch 390/910 - Loss: 0.0630\n",
      "  Batch 400/910 - Loss: 0.0141\n",
      "  Batch 410/910 - Loss: 0.0190\n",
      "  Batch 420/910 - Loss: 0.1344\n",
      "  Batch 430/910 - Loss: 0.0101\n",
      "  Batch 440/910 - Loss: 0.0257\n",
      "  Batch 450/910 - Loss: 0.1218\n",
      "  Batch 460/910 - Loss: 0.0468\n",
      "  Batch 470/910 - Loss: 0.2290\n",
      "  Batch 480/910 - Loss: 0.0170\n",
      "  Batch 490/910 - Loss: 0.1477\n",
      "  Batch 500/910 - Loss: 0.0306\n",
      "  Batch 510/910 - Loss: 0.0043\n",
      "  Batch 520/910 - Loss: 0.0666\n",
      "  Batch 530/910 - Loss: 0.0886\n",
      "  Batch 540/910 - Loss: 0.1279\n",
      "  Batch 550/910 - Loss: 0.0383\n",
      "  Batch 560/910 - Loss: 0.0863\n",
      "  Batch 570/910 - Loss: 0.0870\n",
      "  Batch 580/910 - Loss: 0.0837\n",
      "  Batch 590/910 - Loss: 0.0169\n",
      "  Batch 600/910 - Loss: 0.0301\n",
      "  Batch 610/910 - Loss: 0.0562\n",
      "  Batch 620/910 - Loss: 0.0929\n",
      "  Batch 630/910 - Loss: 0.1196\n",
      "  Batch 640/910 - Loss: 0.0744\n",
      "  Batch 650/910 - Loss: 0.2026\n",
      "  Batch 660/910 - Loss: 0.0791\n",
      "  Batch 670/910 - Loss: 0.0114\n",
      "  Batch 680/910 - Loss: 0.0087\n",
      "  Batch 690/910 - Loss: 0.0911\n",
      "  Batch 700/910 - Loss: 0.0811\n",
      "  Batch 710/910 - Loss: 0.0711\n",
      "  Batch 720/910 - Loss: 0.0390\n",
      "  Batch 730/910 - Loss: 0.0118\n",
      "  Batch 740/910 - Loss: 0.0438\n",
      "  Batch 750/910 - Loss: 0.1145\n",
      "  Batch 760/910 - Loss: 0.0570\n",
      "  Batch 770/910 - Loss: 0.0670\n",
      "  Batch 780/910 - Loss: 0.0490\n",
      "  Batch 790/910 - Loss: 0.1287\n",
      "  Batch 800/910 - Loss: 0.1017\n",
      "  Batch 810/910 - Loss: 0.0316\n",
      "  Batch 820/910 - Loss: 0.0781\n",
      "  Batch 830/910 - Loss: 0.0620\n",
      "  Batch 840/910 - Loss: 0.1568\n",
      "  Batch 850/910 - Loss: 0.1142\n",
      "  Batch 860/910 - Loss: 0.0501\n",
      "  Batch 870/910 - Loss: 0.0503\n",
      "  Batch 880/910 - Loss: 0.0647\n",
      "  Batch 890/910 - Loss: 0.1163\n",
      "  Batch 900/910 - Loss: 0.0663\n",
      "  Batch 910/910 - Loss: 0.2406\n",
      "Training Loss: 0.0762\n",
      "Validation Loss: 0.1053\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.70      0.81      4149\n",
      "         1.0       0.33      0.85      0.47       703\n",
      "\n",
      "    accuracy                           0.73      4852\n",
      "   macro avg       0.65      0.78      0.64      4852\n",
      "weighted avg       0.87      0.73      0.76      4852\n",
      "\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 6/20\n",
      "  Batch 10/910 - Loss: 0.1411\n",
      "  Batch 20/910 - Loss: 0.0712\n",
      "  Batch 30/910 - Loss: 0.0394\n",
      "  Batch 40/910 - Loss: 0.0377\n",
      "  Batch 50/910 - Loss: 0.2158\n",
      "  Batch 60/910 - Loss: 0.0557\n",
      "  Batch 70/910 - Loss: 0.0491\n",
      "  Batch 80/910 - Loss: 0.1093\n",
      "  Batch 90/910 - Loss: 0.1530\n",
      "  Batch 100/910 - Loss: 0.0785\n",
      "  Batch 110/910 - Loss: 0.0482\n",
      "  Batch 120/910 - Loss: 0.1319\n",
      "  Batch 130/910 - Loss: 0.0266\n",
      "  Batch 140/910 - Loss: 0.0331\n",
      "  Batch 150/910 - Loss: 0.0469\n",
      "  Batch 160/910 - Loss: 0.0953\n",
      "  Batch 170/910 - Loss: 0.1988\n",
      "  Batch 180/910 - Loss: 0.0209\n",
      "  Batch 190/910 - Loss: 0.1614\n",
      "  Batch 200/910 - Loss: 0.0492\n",
      "  Batch 210/910 - Loss: 0.0245\n",
      "  Batch 220/910 - Loss: 0.1568\n",
      "  Batch 230/910 - Loss: 0.0783\n",
      "  Batch 240/910 - Loss: 0.1592\n",
      "  Batch 250/910 - Loss: 0.0086\n",
      "  Batch 260/910 - Loss: 0.0616\n",
      "  Batch 270/910 - Loss: 0.2035\n",
      "  Batch 280/910 - Loss: 0.0256\n",
      "  Batch 290/910 - Loss: 0.0085\n",
      "  Batch 300/910 - Loss: 0.0056\n",
      "  Batch 310/910 - Loss: 0.0378\n",
      "  Batch 320/910 - Loss: 0.0333\n",
      "  Batch 330/910 - Loss: 0.0273\n",
      "  Batch 340/910 - Loss: 0.1343\n",
      "  Batch 350/910 - Loss: 0.0388\n",
      "  Batch 360/910 - Loss: 0.0954\n",
      "  Batch 370/910 - Loss: 0.0136\n",
      "  Batch 380/910 - Loss: 0.0828\n",
      "  Batch 390/910 - Loss: 0.1015\n",
      "  Batch 400/910 - Loss: 0.0177\n",
      "  Batch 410/910 - Loss: 0.0962\n",
      "  Batch 420/910 - Loss: 0.0027\n",
      "  Batch 430/910 - Loss: 0.0185\n",
      "  Batch 440/910 - Loss: 0.1237\n",
      "  Batch 450/910 - Loss: 0.1274\n",
      "  Batch 460/910 - Loss: 0.0876\n",
      "  Batch 470/910 - Loss: 0.1310\n",
      "  Batch 480/910 - Loss: 0.0567\n",
      "  Batch 490/910 - Loss: 0.1152\n",
      "  Batch 500/910 - Loss: 0.0656\n",
      "  Batch 510/910 - Loss: 0.1258\n",
      "  Batch 520/910 - Loss: 0.0849\n",
      "  Batch 530/910 - Loss: 0.0859\n",
      "  Batch 540/910 - Loss: 0.1095\n",
      "  Batch 550/910 - Loss: 0.0173\n",
      "  Batch 560/910 - Loss: 0.0064\n",
      "  Batch 570/910 - Loss: 0.1043\n",
      "  Batch 580/910 - Loss: 0.0742\n",
      "  Batch 590/910 - Loss: 0.0654\n",
      "  Batch 600/910 - Loss: 0.0230\n",
      "  Batch 610/910 - Loss: 0.0120\n",
      "  Batch 620/910 - Loss: 0.1825\n",
      "  Batch 630/910 - Loss: 0.1180\n",
      "  Batch 640/910 - Loss: 0.1194\n",
      "  Batch 650/910 - Loss: 0.0123\n",
      "  Batch 660/910 - Loss: 0.0593\n",
      "  Batch 670/910 - Loss: 0.0354\n",
      "  Batch 680/910 - Loss: 0.1047\n",
      "  Batch 690/910 - Loss: 0.1269\n",
      "  Batch 700/910 - Loss: 0.0320\n",
      "  Batch 710/910 - Loss: 0.1071\n",
      "  Batch 720/910 - Loss: 0.0082\n",
      "  Batch 730/910 - Loss: 0.0603\n",
      "  Batch 740/910 - Loss: 0.0777\n",
      "  Batch 750/910 - Loss: 0.0268\n",
      "  Batch 760/910 - Loss: 0.0803\n",
      "  Batch 770/910 - Loss: 0.0561\n",
      "  Batch 780/910 - Loss: 0.0203\n",
      "  Batch 790/910 - Loss: 0.0066\n",
      "  Batch 800/910 - Loss: 0.1072\n",
      "  Batch 810/910 - Loss: 0.0660\n",
      "  Batch 820/910 - Loss: 0.0756\n",
      "  Batch 830/910 - Loss: 0.0451\n",
      "  Batch 840/910 - Loss: 0.0079\n",
      "  Batch 850/910 - Loss: 0.0312\n",
      "  Batch 860/910 - Loss: 0.2116\n",
      "  Batch 870/910 - Loss: 0.1106\n",
      "  Batch 880/910 - Loss: 0.0298\n",
      "  Batch 890/910 - Loss: 0.0412\n",
      "  Batch 900/910 - Loss: 0.0475\n",
      "  Batch 910/910 - Loss: 0.1305\n",
      "Training Loss: 0.0661\n",
      "Validation Loss: 0.0932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.26      0.41      4149\n",
      "         1.0       0.18      0.99      0.31       703\n",
      "\n",
      "    accuracy                           0.36      4852\n",
      "   macro avg       0.59      0.62      0.36      4852\n",
      "weighted avg       0.88      0.36      0.39      4852\n",
      "\n",
      "Validation loss improved. Saving model.\n",
      "Epoch 7/20\n",
      "  Batch 10/910 - Loss: 0.0652\n",
      "  Batch 20/910 - Loss: 0.0584\n",
      "  Batch 30/910 - Loss: 0.0059\n",
      "  Batch 40/910 - Loss: 0.0777\n",
      "  Batch 50/910 - Loss: 0.0380\n",
      "  Batch 60/910 - Loss: 0.0590\n",
      "  Batch 70/910 - Loss: 0.1603\n",
      "  Batch 80/910 - Loss: 0.0453\n",
      "  Batch 90/910 - Loss: 0.1094\n",
      "  Batch 100/910 - Loss: 0.0588\n",
      "  Batch 110/910 - Loss: 0.0767\n",
      "  Batch 120/910 - Loss: 0.0150\n",
      "  Batch 130/910 - Loss: 0.0270\n",
      "  Batch 140/910 - Loss: 0.0047\n",
      "  Batch 150/910 - Loss: 0.1118\n",
      "  Batch 160/910 - Loss: 0.0381\n",
      "  Batch 170/910 - Loss: 0.0409\n",
      "  Batch 180/910 - Loss: 0.0795\n",
      "  Batch 190/910 - Loss: 0.0716\n",
      "  Batch 200/910 - Loss: 0.0081\n",
      "  Batch 210/910 - Loss: 0.0660\n",
      "  Batch 220/910 - Loss: 0.0110\n",
      "  Batch 230/910 - Loss: 0.0075\n",
      "  Batch 240/910 - Loss: 0.0642\n",
      "  Batch 250/910 - Loss: 0.0856\n",
      "  Batch 260/910 - Loss: 0.0636\n",
      "  Batch 270/910 - Loss: 0.0336\n",
      "  Batch 280/910 - Loss: 0.0275\n",
      "  Batch 290/910 - Loss: 0.0616\n",
      "  Batch 300/910 - Loss: 0.0289\n",
      "  Batch 310/910 - Loss: 0.0620\n",
      "  Batch 320/910 - Loss: 0.0651\n",
      "  Batch 330/910 - Loss: 0.1105\n",
      "  Batch 340/910 - Loss: 0.0105\n",
      "  Batch 350/910 - Loss: 0.1261\n",
      "  Batch 360/910 - Loss: 0.0791\n",
      "  Batch 370/910 - Loss: 0.0261\n",
      "  Batch 380/910 - Loss: 0.0084\n",
      "  Batch 390/910 - Loss: 0.0160\n",
      "  Batch 400/910 - Loss: 0.0189\n",
      "  Batch 410/910 - Loss: 0.0252\n",
      "  Batch 420/910 - Loss: 0.1322\n",
      "  Batch 430/910 - Loss: 0.0598\n",
      "  Batch 440/910 - Loss: 0.0944\n",
      "  Batch 450/910 - Loss: 0.0064\n",
      "  Batch 460/910 - Loss: 0.0094\n",
      "  Batch 470/910 - Loss: 0.0740\n",
      "  Batch 480/910 - Loss: 0.0443\n",
      "  Batch 490/910 - Loss: 0.0129\n",
      "  Batch 500/910 - Loss: 0.0878\n",
      "  Batch 510/910 - Loss: 0.0811\n",
      "  Batch 520/910 - Loss: 0.0745\n",
      "  Batch 530/910 - Loss: 0.1313\n",
      "  Batch 540/910 - Loss: 0.0669\n",
      "  Batch 550/910 - Loss: 0.0381\n",
      "  Batch 560/910 - Loss: 0.0368\n",
      "  Batch 570/910 - Loss: 0.0536\n",
      "  Batch 580/910 - Loss: 0.0283\n",
      "  Batch 590/910 - Loss: 0.0082\n",
      "  Batch 600/910 - Loss: 0.0587\n",
      "  Batch 610/910 - Loss: 0.1182\n",
      "  Batch 620/910 - Loss: 0.0816\n",
      "  Batch 630/910 - Loss: 0.0308\n",
      "  Batch 640/910 - Loss: 0.0781\n",
      "  Batch 650/910 - Loss: 0.1262\n",
      "  Batch 660/910 - Loss: 0.0777\n",
      "  Batch 670/910 - Loss: 0.0862\n",
      "  Batch 680/910 - Loss: 0.0468\n",
      "  Batch 690/910 - Loss: 0.0060\n",
      "  Batch 700/910 - Loss: 0.0270\n",
      "  Batch 710/910 - Loss: 0.0859\n",
      "  Batch 720/910 - Loss: 0.0794\n",
      "  Batch 730/910 - Loss: 0.0864\n",
      "  Batch 740/910 - Loss: 0.0436\n",
      "  Batch 750/910 - Loss: 0.0430\n",
      "  Batch 760/910 - Loss: 0.0273\n",
      "  Batch 770/910 - Loss: 0.0036\n",
      "  Batch 780/910 - Loss: 0.1228\n",
      "  Batch 790/910 - Loss: 0.1145\n",
      "  Batch 800/910 - Loss: 0.1285\n",
      "  Batch 810/910 - Loss: 0.0574\n",
      "  Batch 820/910 - Loss: 0.0528\n",
      "  Batch 830/910 - Loss: 0.0984\n",
      "  Batch 840/910 - Loss: 0.0689\n",
      "  Batch 850/910 - Loss: 0.0760\n",
      "  Batch 860/910 - Loss: 0.0722\n",
      "  Batch 870/910 - Loss: 0.0941\n",
      "  Batch 880/910 - Loss: 0.0649\n",
      "  Batch 890/910 - Loss: 0.0114\n",
      "  Batch 900/910 - Loss: 0.0875\n",
      "  Batch 910/910 - Loss: 0.0336\n",
      "Training Loss: 0.0554\n",
      "Validation Loss: 0.1058\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      4149\n",
      "         1.0       0.14      1.00      0.25       703\n",
      "\n",
      "    accuracy                           0.14      4852\n",
      "   macro avg       0.07      0.50      0.13      4852\n",
      "weighted avg       0.02      0.14      0.04      4852\n",
      "\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/910 - Loss: 0.0591\n",
      "  Batch 20/910 - Loss: 0.0935\n",
      "  Batch 30/910 - Loss: 0.1602\n",
      "  Batch 40/910 - Loss: 0.0592\n",
      "  Batch 50/910 - Loss: 0.0621\n",
      "  Batch 60/910 - Loss: 0.0139\n",
      "  Batch 70/910 - Loss: 0.0769\n",
      "  Batch 80/910 - Loss: 0.0094\n",
      "  Batch 90/910 - Loss: 0.0017\n",
      "  Batch 100/910 - Loss: 0.0046\n",
      "  Batch 110/910 - Loss: 0.0743\n",
      "  Batch 120/910 - Loss: 0.0607\n",
      "  Batch 130/910 - Loss: 0.0547\n",
      "  Batch 140/910 - Loss: 0.0504\n",
      "  Batch 150/910 - Loss: 0.0622\n",
      "  Batch 160/910 - Loss: 0.0843\n",
      "  Batch 170/910 - Loss: 0.0137\n",
      "  Batch 180/910 - Loss: 0.0221\n",
      "  Batch 190/910 - Loss: 0.0881\n",
      "  Batch 200/910 - Loss: 0.0133\n",
      "  Batch 210/910 - Loss: 0.0622\n",
      "  Batch 220/910 - Loss: 0.0872\n",
      "  Batch 230/910 - Loss: 0.0921\n",
      "  Batch 240/910 - Loss: 0.0343\n",
      "  Batch 250/910 - Loss: 0.0036\n",
      "  Batch 260/910 - Loss: 0.0623\n",
      "  Batch 270/910 - Loss: 0.0607\n",
      "  Batch 280/910 - Loss: 0.0047\n",
      "  Batch 290/910 - Loss: 0.0095\n",
      "  Batch 300/910 - Loss: 0.0349\n",
      "  Batch 310/910 - Loss: 0.0589\n",
      "  Batch 320/910 - Loss: 0.2337\n",
      "  Batch 330/910 - Loss: 0.1489\n",
      "  Batch 340/910 - Loss: 0.0581\n",
      "  Batch 350/910 - Loss: 0.0178\n",
      "  Batch 360/910 - Loss: 0.0387\n",
      "  Batch 370/910 - Loss: 0.0105\n",
      "  Batch 380/910 - Loss: 0.0163\n",
      "  Batch 390/910 - Loss: 0.0442\n",
      "  Batch 400/910 - Loss: 0.0739\n",
      "  Batch 410/910 - Loss: 0.0090\n",
      "  Batch 420/910 - Loss: 0.0374\n",
      "  Batch 430/910 - Loss: 0.0974\n",
      "  Batch 440/910 - Loss: 0.0472\n",
      "  Batch 450/910 - Loss: 0.0710\n",
      "  Batch 460/910 - Loss: 0.0687\n",
      "  Batch 470/910 - Loss: 0.0336\n",
      "  Batch 480/910 - Loss: 0.1216\n",
      "  Batch 490/910 - Loss: 0.0641\n",
      "  Batch 500/910 - Loss: 0.0723\n",
      "  Batch 510/910 - Loss: 0.0287\n",
      "  Batch 520/910 - Loss: 0.1638\n",
      "  Batch 530/910 - Loss: 0.0213\n",
      "  Batch 540/910 - Loss: 0.0070\n",
      "  Batch 550/910 - Loss: 0.0171\n",
      "  Batch 560/910 - Loss: 0.0555\n",
      "  Batch 570/910 - Loss: 0.0456\n",
      "  Batch 580/910 - Loss: 0.0705\n",
      "  Batch 590/910 - Loss: 0.0218\n",
      "  Batch 600/910 - Loss: 0.0033\n",
      "  Batch 610/910 - Loss: 0.0084\n",
      "  Batch 620/910 - Loss: 0.0787\n",
      "  Batch 630/910 - Loss: 0.0253\n",
      "  Batch 640/910 - Loss: 0.0123\n",
      "  Batch 650/910 - Loss: 0.1379\n",
      "  Batch 660/910 - Loss: 0.0330\n",
      "  Batch 670/910 - Loss: 0.0376\n",
      "  Batch 680/910 - Loss: 0.0418\n",
      "  Batch 690/910 - Loss: 0.1149\n",
      "  Batch 700/910 - Loss: 0.0339\n",
      "  Batch 710/910 - Loss: 0.0690\n",
      "  Batch 720/910 - Loss: 0.0660\n",
      "  Batch 730/910 - Loss: 0.0284\n",
      "  Batch 740/910 - Loss: 0.0022\n",
      "  Batch 750/910 - Loss: 0.0677\n",
      "  Batch 760/910 - Loss: 0.0588\n",
      "  Batch 770/910 - Loss: 0.0258\n",
      "  Batch 780/910 - Loss: 0.0258\n",
      "  Batch 790/910 - Loss: 0.0025\n",
      "  Batch 800/910 - Loss: 0.0096\n",
      "  Batch 810/910 - Loss: 0.0196\n",
      "  Batch 820/910 - Loss: 0.0904\n",
      "  Batch 830/910 - Loss: 0.0281\n",
      "  Batch 840/910 - Loss: 0.0091\n",
      "  Batch 850/910 - Loss: 0.0561\n",
      "  Batch 860/910 - Loss: 0.0463\n",
      "  Batch 870/910 - Loss: 0.0169\n",
      "  Batch 880/910 - Loss: 0.0243\n",
      "  Batch 890/910 - Loss: 0.0199\n",
      "  Batch 900/910 - Loss: 0.0093\n",
      "  Batch 910/910 - Loss: 0.0217\n",
      "Training Loss: 0.0497\n",
      "Validation Loss: 0.1233\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      4149\n",
      "         1.0       0.14      1.00      0.25       703\n",
      "\n",
      "    accuracy                           0.14      4852\n",
      "   macro avg       0.07      0.50      0.13      4852\n",
      "weighted avg       0.02      0.14      0.04      4852\n",
      "\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/910 - Loss: 0.0853\n",
      "  Batch 20/910 - Loss: 0.0273\n",
      "  Batch 30/910 - Loss: 0.0489\n",
      "  Batch 40/910 - Loss: 0.0258\n",
      "  Batch 50/910 - Loss: 0.0644\n",
      "  Batch 60/910 - Loss: 0.0073\n",
      "  Batch 70/910 - Loss: 0.1230\n",
      "  Batch 80/910 - Loss: 0.0025\n",
      "  Batch 90/910 - Loss: 0.1065\n",
      "  Batch 100/910 - Loss: 0.0696\n",
      "  Batch 110/910 - Loss: 0.1039\n",
      "  Batch 120/910 - Loss: 0.0122\n",
      "  Batch 130/910 - Loss: 0.0770\n",
      "  Batch 140/910 - Loss: 0.0402\n",
      "  Batch 150/910 - Loss: 0.0039\n",
      "  Batch 160/910 - Loss: 0.0431\n",
      "  Batch 170/910 - Loss: 0.0256\n",
      "  Batch 180/910 - Loss: 0.1849\n",
      "  Batch 190/910 - Loss: 0.0729\n",
      "  Batch 200/910 - Loss: 0.0601\n",
      "  Batch 210/910 - Loss: 0.0185\n",
      "  Batch 220/910 - Loss: 0.1460\n",
      "  Batch 230/910 - Loss: 0.1251\n",
      "  Batch 240/910 - Loss: 0.0041\n",
      "  Batch 250/910 - Loss: 0.0023\n",
      "  Batch 260/910 - Loss: 0.0087\n",
      "  Batch 270/910 - Loss: 0.0272\n",
      "  Batch 280/910 - Loss: 0.0722\n",
      "  Batch 290/910 - Loss: 0.0132\n",
      "  Batch 300/910 - Loss: 0.0665\n",
      "  Batch 310/910 - Loss: 0.1097\n",
      "  Batch 320/910 - Loss: 0.0121\n",
      "  Batch 330/910 - Loss: 0.0062\n",
      "  Batch 340/910 - Loss: 0.0254\n",
      "  Batch 350/910 - Loss: 0.1396\n",
      "  Batch 360/910 - Loss: 0.0134\n",
      "  Batch 370/910 - Loss: 0.0717\n",
      "  Batch 380/910 - Loss: 0.0103\n",
      "  Batch 390/910 - Loss: 0.0184\n",
      "  Batch 400/910 - Loss: 0.1242\n",
      "  Batch 410/910 - Loss: 0.0048\n",
      "  Batch 420/910 - Loss: 0.0074\n",
      "  Batch 430/910 - Loss: 0.0497\n",
      "  Batch 440/910 - Loss: 0.0234\n",
      "  Batch 450/910 - Loss: 0.0343\n",
      "  Batch 460/910 - Loss: 0.0218\n",
      "  Batch 470/910 - Loss: 0.0112\n",
      "  Batch 480/910 - Loss: 0.0038\n",
      "  Batch 490/910 - Loss: 0.0577\n",
      "  Batch 500/910 - Loss: 0.0088\n",
      "  Batch 510/910 - Loss: 0.0035\n",
      "  Batch 520/910 - Loss: 0.0050\n",
      "  Batch 530/910 - Loss: 0.0633\n",
      "  Batch 540/910 - Loss: 0.0605\n",
      "  Batch 550/910 - Loss: 0.0956\n",
      "  Batch 560/910 - Loss: 0.0127\n",
      "  Batch 570/910 - Loss: 0.0097\n",
      "  Batch 580/910 - Loss: 0.0056\n",
      "  Batch 590/910 - Loss: 0.0943\n",
      "  Batch 600/910 - Loss: 0.0180\n",
      "  Batch 610/910 - Loss: 0.0596\n",
      "  Batch 620/910 - Loss: 0.0594\n",
      "  Batch 630/910 - Loss: 0.0124\n",
      "  Batch 640/910 - Loss: 0.0686\n",
      "  Batch 650/910 - Loss: 0.0324\n",
      "  Batch 660/910 - Loss: 0.0483\n",
      "  Batch 670/910 - Loss: 0.0957\n",
      "  Batch 680/910 - Loss: 0.0545\n",
      "  Batch 690/910 - Loss: 0.0044\n",
      "  Batch 700/910 - Loss: 0.0684\n",
      "  Batch 710/910 - Loss: 0.0184\n",
      "  Batch 720/910 - Loss: 0.0018\n",
      "  Batch 730/910 - Loss: 0.0053\n",
      "  Batch 740/910 - Loss: 0.0569\n",
      "  Batch 750/910 - Loss: 0.1297\n",
      "  Batch 760/910 - Loss: 0.0125\n",
      "  Batch 770/910 - Loss: 0.0048\n",
      "  Batch 780/910 - Loss: 0.0023\n",
      "  Batch 790/910 - Loss: 0.0029\n",
      "  Batch 800/910 - Loss: 0.0056\n",
      "  Batch 810/910 - Loss: 0.0128\n",
      "  Batch 820/910 - Loss: 0.0193\n",
      "  Batch 830/910 - Loss: 0.0602\n",
      "  Batch 840/910 - Loss: 0.0607\n",
      "  Batch 850/910 - Loss: 0.0056\n",
      "  Batch 860/910 - Loss: 0.0923\n",
      "  Batch 870/910 - Loss: 0.0655\n",
      "  Batch 880/910 - Loss: 0.0856\n",
      "  Batch 890/910 - Loss: 0.0500\n",
      "  Batch 900/910 - Loss: 0.0539\n",
      "  Batch 910/910 - Loss: 0.0160\n",
      "Training Loss: 0.0439\n",
      "Validation Loss: 0.1021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      4149\n",
      "         1.0       0.14      1.00      0.25       703\n",
      "\n",
      "    accuracy                           0.14      4852\n",
      "   macro avg       0.07      0.50      0.13      4852\n",
      "weighted avg       0.02      0.14      0.04      4852\n",
      "\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/910 - Loss: 0.0593\n",
      "  Batch 20/910 - Loss: 0.0056\n",
      "  Batch 30/910 - Loss: 0.0077\n",
      "  Batch 40/910 - Loss: 0.1718\n",
      "  Batch 50/910 - Loss: 0.0194\n",
      "  Batch 60/910 - Loss: 0.0129\n",
      "  Batch 70/910 - Loss: 0.0082\n",
      "  Batch 80/910 - Loss: 0.0140\n",
      "  Batch 90/910 - Loss: 0.0017\n",
      "  Batch 100/910 - Loss: 0.0461\n",
      "  Batch 110/910 - Loss: 0.1227\n",
      "  Batch 120/910 - Loss: 0.0571\n",
      "  Batch 130/910 - Loss: 0.0135\n",
      "  Batch 140/910 - Loss: 0.0105\n",
      "  Batch 150/910 - Loss: 0.0504\n",
      "  Batch 160/910 - Loss: 0.0101\n",
      "  Batch 170/910 - Loss: 0.0511\n",
      "  Batch 180/910 - Loss: 0.0278\n",
      "  Batch 190/910 - Loss: 0.0132\n",
      "  Batch 200/910 - Loss: 0.0533\n",
      "  Batch 210/910 - Loss: 0.0476\n",
      "  Batch 220/910 - Loss: 0.0099\n",
      "  Batch 230/910 - Loss: 0.0135\n",
      "  Batch 240/910 - Loss: 0.0873\n",
      "  Batch 250/910 - Loss: 0.0062\n",
      "  Batch 260/910 - Loss: 0.0032\n",
      "  Batch 270/910 - Loss: 0.0523\n",
      "  Batch 280/910 - Loss: 0.0059\n",
      "  Batch 290/910 - Loss: 0.0316\n",
      "  Batch 300/910 - Loss: 0.0663\n",
      "  Batch 310/910 - Loss: 0.0622\n",
      "  Batch 320/910 - Loss: 0.0583\n",
      "  Batch 330/910 - Loss: 0.0041\n",
      "  Batch 340/910 - Loss: 0.0061\n",
      "  Batch 350/910 - Loss: 0.0112\n",
      "  Batch 360/910 - Loss: 0.0365\n",
      "  Batch 370/910 - Loss: 0.1353\n",
      "  Batch 380/910 - Loss: 0.0649\n",
      "  Batch 390/910 - Loss: 0.0166\n",
      "  Batch 400/910 - Loss: 0.0386\n",
      "  Batch 410/910 - Loss: 0.1197\n",
      "  Batch 420/910 - Loss: 0.0143\n",
      "  Batch 430/910 - Loss: 0.0203\n",
      "  Batch 440/910 - Loss: 0.0689\n",
      "  Batch 450/910 - Loss: 0.0334\n",
      "  Batch 460/910 - Loss: 0.0984\n",
      "  Batch 470/910 - Loss: 0.0537\n",
      "  Batch 480/910 - Loss: 0.0138\n",
      "  Batch 490/910 - Loss: 0.0401\n",
      "  Batch 500/910 - Loss: 0.0084\n",
      "  Batch 510/910 - Loss: 0.0074\n",
      "  Batch 520/910 - Loss: 0.1158\n",
      "  Batch 530/910 - Loss: 0.0023\n",
      "  Batch 540/910 - Loss: 0.0036\n",
      "  Batch 550/910 - Loss: 0.0188\n",
      "  Batch 560/910 - Loss: 0.0191\n",
      "  Batch 570/910 - Loss: 0.0784\n",
      "  Batch 580/910 - Loss: 0.0057\n",
      "  Batch 590/910 - Loss: 0.0021\n",
      "  Batch 600/910 - Loss: 0.0808\n",
      "  Batch 610/910 - Loss: 0.0135\n",
      "  Batch 620/910 - Loss: 0.0404\n",
      "  Batch 630/910 - Loss: 0.0031\n",
      "  Batch 640/910 - Loss: 0.0668\n",
      "  Batch 650/910 - Loss: 0.0203\n",
      "  Batch 660/910 - Loss: 0.0605\n",
      "  Batch 670/910 - Loss: 0.0915\n",
      "  Batch 680/910 - Loss: 0.0479\n",
      "  Batch 690/910 - Loss: 0.0034\n",
      "  Batch 700/910 - Loss: 0.0057\n",
      "  Batch 710/910 - Loss: 0.0057\n",
      "  Batch 720/910 - Loss: 0.1157\n",
      "  Batch 730/910 - Loss: 0.1012\n",
      "  Batch 740/910 - Loss: 0.0118\n",
      "  Batch 750/910 - Loss: 0.0061\n",
      "  Batch 760/910 - Loss: 0.0998\n",
      "  Batch 770/910 - Loss: 0.0588\n",
      "  Batch 780/910 - Loss: 0.0902\n",
      "  Batch 790/910 - Loss: 0.0137\n",
      "  Batch 800/910 - Loss: 0.0162\n",
      "  Batch 810/910 - Loss: 0.0034\n",
      "  Batch 820/910 - Loss: 0.1120\n",
      "  Batch 830/910 - Loss: 0.0022\n",
      "  Batch 840/910 - Loss: 0.0215\n",
      "  Batch 850/910 - Loss: 0.0508\n",
      "  Batch 860/910 - Loss: 0.0614\n",
      "  Batch 870/910 - Loss: 0.0202\n",
      "  Batch 880/910 - Loss: 0.0256\n",
      "  Batch 890/910 - Loss: 0.0421\n",
      "  Batch 900/910 - Loss: 0.0092\n",
      "  Batch 910/910 - Loss: 0.0067\n",
      "Training Loss: 0.0398\n",
      "Validation Loss: 0.1036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.68      0.80      4149\n",
      "         1.0       0.31      0.85      0.46       703\n",
      "\n",
      "    accuracy                           0.71      4852\n",
      "   macro avg       0.64      0.76      0.63      4852\n",
      "weighted avg       0.87      0.71      0.75      4852\n",
      "\n",
      "Epoch 11/20\n",
      "  Batch 10/910 - Loss: 0.0694\n",
      "  Batch 20/910 - Loss: 0.0625\n",
      "  Batch 30/910 - Loss: 0.0151\n",
      "  Batch 40/910 - Loss: 0.0108\n",
      "  Batch 50/910 - Loss: 0.0022\n",
      "  Batch 60/910 - Loss: 0.0020\n",
      "  Batch 70/910 - Loss: 0.0034\n",
      "  Batch 80/910 - Loss: 0.0006\n",
      "  Batch 90/910 - Loss: 0.0030\n",
      "  Batch 100/910 - Loss: 0.0016\n",
      "  Batch 110/910 - Loss: 0.0939\n",
      "  Batch 120/910 - Loss: 0.0225\n",
      "  Batch 130/910 - Loss: 0.0058\n",
      "  Batch 140/910 - Loss: 0.0075\n",
      "  Batch 150/910 - Loss: 0.0107\n",
      "  Batch 160/910 - Loss: 0.0489\n",
      "  Batch 170/910 - Loss: 0.0699\n",
      "  Batch 180/910 - Loss: 0.0125\n",
      "  Batch 190/910 - Loss: 0.0015\n",
      "  Batch 200/910 - Loss: 0.0069\n",
      "  Batch 210/910 - Loss: 0.0044\n",
      "  Batch 220/910 - Loss: 0.0116\n",
      "  Batch 230/910 - Loss: 0.0674\n",
      "  Batch 240/910 - Loss: 0.0156\n",
      "  Batch 250/910 - Loss: 0.0710\n",
      "  Batch 260/910 - Loss: 0.0536\n",
      "  Batch 270/910 - Loss: 0.0541\n",
      "  Batch 280/910 - Loss: 0.0142\n",
      "  Batch 290/910 - Loss: 0.0123\n",
      "  Batch 300/910 - Loss: 0.0328\n",
      "  Batch 310/910 - Loss: 0.0772\n",
      "  Batch 320/910 - Loss: 0.0537\n",
      "  Batch 330/910 - Loss: 0.0292\n",
      "  Batch 340/910 - Loss: 0.0015\n",
      "  Batch 350/910 - Loss: 0.0285\n",
      "  Batch 360/910 - Loss: 0.1422\n",
      "  Batch 370/910 - Loss: 0.0062\n",
      "  Batch 380/910 - Loss: 0.0106\n",
      "  Batch 390/910 - Loss: 0.0556\n",
      "  Batch 400/910 - Loss: 0.0473\n",
      "  Batch 410/910 - Loss: 0.0118\n",
      "  Batch 420/910 - Loss: 0.0525\n",
      "  Batch 430/910 - Loss: 0.0112\n",
      "  Batch 440/910 - Loss: 0.0248\n",
      "  Batch 450/910 - Loss: 0.0832\n",
      "  Batch 460/910 - Loss: 0.0227\n",
      "  Batch 470/910 - Loss: 0.0541\n",
      "  Batch 480/910 - Loss: 0.0078\n",
      "  Batch 490/910 - Loss: 0.0046\n",
      "  Batch 500/910 - Loss: 0.0140\n",
      "  Batch 510/910 - Loss: 0.0204\n",
      "  Batch 520/910 - Loss: 0.0033\n",
      "  Batch 530/910 - Loss: 0.0595\n",
      "  Batch 540/910 - Loss: 0.0165\n",
      "  Batch 550/910 - Loss: 0.0346\n",
      "  Batch 560/910 - Loss: 0.0006\n",
      "  Batch 570/910 - Loss: 0.0133\n",
      "  Batch 580/910 - Loss: 0.0059\n",
      "  Batch 590/910 - Loss: 0.0756\n",
      "  Batch 600/910 - Loss: 0.0176\n",
      "  Batch 610/910 - Loss: 0.1184\n",
      "  Batch 620/910 - Loss: 0.0056\n",
      "  Batch 630/910 - Loss: 0.1308\n",
      "  Batch 640/910 - Loss: 0.0035\n",
      "  Batch 650/910 - Loss: 0.0425\n",
      "  Batch 660/910 - Loss: 0.1182\n",
      "  Batch 670/910 - Loss: 0.0107\n",
      "  Batch 680/910 - Loss: 0.0079\n",
      "  Batch 690/910 - Loss: 0.0026\n",
      "  Batch 700/910 - Loss: 0.0044\n",
      "  Batch 710/910 - Loss: 0.0349\n",
      "  Batch 720/910 - Loss: 0.0239\n",
      "  Batch 730/910 - Loss: 0.0588\n",
      "  Batch 740/910 - Loss: 0.0333\n",
      "  Batch 750/910 - Loss: 0.0334\n",
      "  Batch 760/910 - Loss: 0.0150\n",
      "  Batch 770/910 - Loss: 0.0463\n",
      "  Batch 780/910 - Loss: 0.0562\n",
      "  Batch 790/910 - Loss: 0.0043\n",
      "  Batch 800/910 - Loss: 0.0195\n",
      "  Batch 810/910 - Loss: 0.1022\n",
      "  Batch 820/910 - Loss: 0.0239\n",
      "  Batch 830/910 - Loss: 0.0083\n",
      "  Batch 840/910 - Loss: 0.0146\n",
      "  Batch 850/910 - Loss: 0.0076\n",
      "  Batch 860/910 - Loss: 0.0181\n",
      "  Batch 870/910 - Loss: 0.0162\n",
      "  Batch 880/910 - Loss: 0.0141\n",
      "  Batch 890/910 - Loss: 0.0113\n",
      "  Batch 900/910 - Loss: 0.0588\n",
      "  Batch 910/910 - Loss: 0.0871\n",
      "Training Loss: 0.0364\n",
      "Validation Loss: 0.0997\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.04      0.08      4149\n",
      "         1.0       0.15      1.00      0.26       703\n",
      "\n",
      "    accuracy                           0.18      4852\n",
      "   macro avg       0.58      0.52      0.17      4852\n",
      "weighted avg       0.88      0.18      0.11      4852\n",
      "\n",
      "Epoch 12/20\n",
      "  Batch 10/910 - Loss: 0.0020\n",
      "  Batch 20/910 - Loss: 0.0466\n",
      "  Batch 30/910 - Loss: 0.0656\n",
      "  Batch 40/910 - Loss: 0.0231\n",
      "  Batch 50/910 - Loss: 0.0164\n",
      "  Batch 60/910 - Loss: 0.0033\n",
      "  Batch 70/910 - Loss: 0.0156\n",
      "  Batch 80/910 - Loss: 0.0620\n",
      "  Batch 90/910 - Loss: 0.0022\n",
      "  Batch 100/910 - Loss: 0.0051\n",
      "  Batch 110/910 - Loss: 0.0062\n",
      "  Batch 120/910 - Loss: 0.0621\n",
      "  Batch 130/910 - Loss: 0.0067\n",
      "  Batch 140/910 - Loss: 0.0083\n",
      "  Batch 150/910 - Loss: 0.0032\n",
      "  Batch 160/910 - Loss: 0.0021\n",
      "  Batch 170/910 - Loss: 0.0015\n",
      "  Batch 180/910 - Loss: 0.0012\n",
      "  Batch 190/910 - Loss: 0.1159\n",
      "  Batch 200/910 - Loss: 0.0065\n",
      "  Batch 210/910 - Loss: 0.0027\n",
      "  Batch 220/910 - Loss: 0.0175\n",
      "  Batch 230/910 - Loss: 0.0297\n",
      "  Batch 240/910 - Loss: 0.0544\n",
      "  Batch 250/910 - Loss: 0.0545\n",
      "  Batch 260/910 - Loss: 0.0052\n",
      "  Batch 270/910 - Loss: 0.0056\n",
      "  Batch 280/910 - Loss: 0.0654\n",
      "  Batch 290/910 - Loss: 0.0048\n",
      "  Batch 300/910 - Loss: 0.0387\n",
      "  Batch 310/910 - Loss: 0.0033\n",
      "  Batch 320/910 - Loss: 0.0676\n",
      "  Batch 330/910 - Loss: 0.0648\n",
      "  Batch 340/910 - Loss: 0.0021\n",
      "  Batch 350/910 - Loss: 0.0047\n",
      "  Batch 360/910 - Loss: 0.0022\n",
      "  Batch 370/910 - Loss: 0.0069\n",
      "  Batch 380/910 - Loss: 0.0080\n",
      "  Batch 390/910 - Loss: 0.0066\n",
      "  Batch 400/910 - Loss: 0.0055\n",
      "  Batch 410/910 - Loss: 0.0078\n",
      "  Batch 420/910 - Loss: 0.0084\n",
      "  Batch 430/910 - Loss: 0.0083\n",
      "  Batch 440/910 - Loss: 0.0668\n",
      "  Batch 450/910 - Loss: 0.0562\n",
      "  Batch 460/910 - Loss: 0.0036\n",
      "  Batch 470/910 - Loss: 0.0539\n",
      "  Batch 480/910 - Loss: 0.0602\n",
      "  Batch 490/910 - Loss: 0.0103\n",
      "  Batch 500/910 - Loss: 0.0836\n",
      "  Batch 510/910 - Loss: 0.0049\n",
      "  Batch 520/910 - Loss: 0.0029\n",
      "  Batch 530/910 - Loss: 0.0105\n",
      "  Batch 540/910 - Loss: 0.0660\n",
      "  Batch 550/910 - Loss: 0.0622\n",
      "  Batch 560/910 - Loss: 0.0579\n",
      "  Batch 570/910 - Loss: 0.0114\n",
      "  Batch 580/910 - Loss: 0.0489\n",
      "  Batch 590/910 - Loss: 0.0491\n",
      "  Batch 600/910 - Loss: 0.0433\n",
      "  Batch 610/910 - Loss: 0.0132\n",
      "  Batch 620/910 - Loss: 0.0179\n",
      "  Batch 630/910 - Loss: 0.0259\n",
      "  Batch 640/910 - Loss: 0.1764\n",
      "  Batch 650/910 - Loss: 0.0078\n",
      "  Batch 660/910 - Loss: 0.0077\n",
      "  Batch 670/910 - Loss: 0.0654\n",
      "  Batch 680/910 - Loss: 0.0321\n",
      "  Batch 690/910 - Loss: 0.0047\n",
      "  Batch 700/910 - Loss: 0.0380\n",
      "  Batch 710/910 - Loss: 0.0609\n",
      "  Batch 720/910 - Loss: 0.0028\n",
      "  Batch 730/910 - Loss: 0.0521\n",
      "  Batch 740/910 - Loss: 0.0047\n",
      "  Batch 750/910 - Loss: 0.0019\n",
      "  Batch 760/910 - Loss: 0.0447\n",
      "  Batch 770/910 - Loss: 0.0055\n",
      "  Batch 780/910 - Loss: 0.0427\n",
      "  Batch 790/910 - Loss: 0.0134\n",
      "  Batch 800/910 - Loss: 0.0105\n",
      "  Batch 810/910 - Loss: 0.0066\n",
      "  Batch 820/910 - Loss: 0.0044\n",
      "  Batch 830/910 - Loss: 0.0547\n",
      "  Batch 840/910 - Loss: 0.0534\n",
      "  Batch 850/910 - Loss: 0.0095\n",
      "  Batch 860/910 - Loss: 0.0064\n",
      "  Batch 870/910 - Loss: 0.0028\n",
      "  Batch 880/910 - Loss: 0.0033\n",
      "  Batch 890/910 - Loss: 0.0091\n",
      "  Batch 900/910 - Loss: 0.0139\n",
      "  Batch 910/910 - Loss: 0.0011\n",
      "Training Loss: 0.0327\n",
      "Validation Loss: 0.1097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.20      0.33      4149\n",
      "         1.0       0.17      1.00      0.30       703\n",
      "\n",
      "    accuracy                           0.31      4852\n",
      "   macro avg       0.59      0.60      0.31      4852\n",
      "weighted avg       0.88      0.31      0.32      4852\n",
      "\n",
      "Epoch 13/20\n",
      "  Batch 10/910 - Loss: 0.0013\n",
      "  Batch 20/910 - Loss: 0.0072\n",
      "  Batch 30/910 - Loss: 0.0379\n",
      "  Batch 40/910 - Loss: 0.0255\n",
      "  Batch 50/910 - Loss: 0.0137\n",
      "  Batch 60/910 - Loss: 0.0169\n",
      "  Batch 70/910 - Loss: 0.0017\n",
      "  Batch 80/910 - Loss: 0.0042\n",
      "  Batch 90/910 - Loss: 0.0113\n",
      "  Batch 100/910 - Loss: 0.0016\n",
      "  Batch 110/910 - Loss: 0.0256\n",
      "  Batch 120/910 - Loss: 0.0062\n",
      "  Batch 130/910 - Loss: 0.0683\n",
      "  Batch 140/910 - Loss: 0.0253\n",
      "  Batch 150/910 - Loss: 0.0664\n",
      "  Batch 160/910 - Loss: 0.0013\n",
      "  Batch 170/910 - Loss: 0.0172\n",
      "  Batch 180/910 - Loss: 0.0025\n",
      "  Batch 190/910 - Loss: 0.0189\n",
      "  Batch 200/910 - Loss: 0.0006\n",
      "  Batch 210/910 - Loss: 0.0094\n",
      "  Batch 220/910 - Loss: 0.0502\n",
      "  Batch 230/910 - Loss: 0.0712\n",
      "  Batch 240/910 - Loss: 0.0117\n",
      "  Batch 250/910 - Loss: 0.0810\n",
      "  Batch 260/910 - Loss: 0.0111\n",
      "  Batch 270/910 - Loss: 0.0563\n",
      "  Batch 280/910 - Loss: 0.0011\n",
      "  Batch 290/910 - Loss: 0.0044\n",
      "  Batch 300/910 - Loss: 0.0015\n",
      "  Batch 310/910 - Loss: 0.1404\n",
      "  Batch 320/910 - Loss: 0.0141\n",
      "  Batch 330/910 - Loss: 0.0697\n",
      "  Batch 340/910 - Loss: 0.0570\n",
      "  Batch 350/910 - Loss: 0.0623\n",
      "  Batch 360/910 - Loss: 0.0100\n",
      "  Batch 370/910 - Loss: 0.0046\n",
      "  Batch 380/910 - Loss: 0.0033\n",
      "  Batch 390/910 - Loss: 0.0015\n",
      "  Batch 400/910 - Loss: 0.0604\n",
      "  Batch 410/910 - Loss: 0.1035\n",
      "  Batch 420/910 - Loss: 0.0056\n",
      "  Batch 430/910 - Loss: 0.0006\n",
      "  Batch 440/910 - Loss: 0.0072\n",
      "  Batch 450/910 - Loss: 0.0077\n",
      "  Batch 460/910 - Loss: 0.0051\n",
      "  Batch 470/910 - Loss: 0.0631\n",
      "  Batch 480/910 - Loss: 0.0058\n",
      "  Batch 490/910 - Loss: 0.0086\n",
      "  Batch 500/910 - Loss: 0.0018\n",
      "  Batch 510/910 - Loss: 0.1031\n",
      "  Batch 520/910 - Loss: 0.0200\n",
      "  Batch 530/910 - Loss: 0.1010\n",
      "  Batch 540/910 - Loss: 0.0309\n",
      "  Batch 550/910 - Loss: 0.0010\n",
      "  Batch 560/910 - Loss: 0.0232\n",
      "  Batch 570/910 - Loss: 0.0016\n",
      "  Batch 580/910 - Loss: 0.0020\n",
      "  Batch 590/910 - Loss: 0.0056\n",
      "  Batch 600/910 - Loss: 0.0015\n",
      "  Batch 610/910 - Loss: 0.0595\n",
      "  Batch 620/910 - Loss: 0.0563\n",
      "  Batch 630/910 - Loss: 0.0048\n",
      "  Batch 640/910 - Loss: 0.1817\n",
      "  Batch 650/910 - Loss: 0.0083\n",
      "  Batch 660/910 - Loss: 0.0040\n",
      "  Batch 670/910 - Loss: 0.0050\n",
      "  Batch 680/910 - Loss: 0.0129\n",
      "  Batch 690/910 - Loss: 0.0581\n",
      "  Batch 700/910 - Loss: 0.0917\n",
      "  Batch 710/910 - Loss: 0.0275\n",
      "  Batch 720/910 - Loss: 0.0480\n",
      "  Batch 730/910 - Loss: 0.0012\n",
      "  Batch 740/910 - Loss: 0.0181\n",
      "  Batch 750/910 - Loss: 0.0454\n",
      "  Batch 760/910 - Loss: 0.0038\n",
      "  Batch 770/910 - Loss: 0.0072\n",
      "  Batch 780/910 - Loss: 0.0463\n",
      "  Batch 790/910 - Loss: 0.0023\n",
      "  Batch 800/910 - Loss: 0.0362\n",
      "  Batch 810/910 - Loss: 0.0114\n",
      "  Batch 820/910 - Loss: 0.0498\n",
      "  Batch 830/910 - Loss: 0.0719\n",
      "  Batch 840/910 - Loss: 0.0089\n",
      "  Batch 850/910 - Loss: 0.0928\n",
      "  Batch 860/910 - Loss: 0.0553\n",
      "  Batch 870/910 - Loss: 0.0218\n",
      "  Batch 880/910 - Loss: 0.0354\n",
      "  Batch 890/910 - Loss: 0.0356\n",
      "  Batch 900/910 - Loss: 0.0404\n",
      "  Batch 910/910 - Loss: 0.0858\n",
      "Training Loss: 0.0314\n",
      "Validation Loss: 0.1135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.51      0.67      4149\n",
      "         1.0       0.24      0.93      0.39       703\n",
      "\n",
      "    accuracy                           0.57      4852\n",
      "   macro avg       0.61      0.72      0.53      4852\n",
      "weighted avg       0.87      0.57      0.63      4852\n",
      "\n",
      "Epoch 14/20\n",
      "  Batch 10/910 - Loss: 0.0036\n",
      "  Batch 20/910 - Loss: 0.0018\n",
      "  Batch 30/910 - Loss: 0.0023\n",
      "  Batch 40/910 - Loss: 0.0015\n",
      "  Batch 50/910 - Loss: 0.0023\n",
      "  Batch 60/910 - Loss: 0.0011\n",
      "  Batch 70/910 - Loss: 0.0323\n",
      "  Batch 80/910 - Loss: 0.0031\n",
      "  Batch 90/910 - Loss: 0.0654\n",
      "  Batch 100/910 - Loss: 0.0018\n",
      "  Batch 110/910 - Loss: 0.0052\n",
      "  Batch 120/910 - Loss: 0.0030\n",
      "  Batch 130/910 - Loss: 0.0193\n",
      "  Batch 140/910 - Loss: 0.0010\n",
      "  Batch 150/910 - Loss: 0.0034\n",
      "  Batch 160/910 - Loss: 0.0064\n",
      "  Batch 170/910 - Loss: 0.0025\n",
      "  Batch 180/910 - Loss: 0.0367\n",
      "  Batch 190/910 - Loss: 0.0008\n",
      "  Batch 200/910 - Loss: 0.0014\n",
      "  Batch 210/910 - Loss: 0.0012\n",
      "  Batch 220/910 - Loss: 0.0059\n",
      "  Batch 230/910 - Loss: 0.0188\n",
      "  Batch 240/910 - Loss: 0.0057\n",
      "  Batch 250/910 - Loss: 0.0637\n",
      "  Batch 260/910 - Loss: 0.0078\n",
      "  Batch 270/910 - Loss: 0.0536\n",
      "  Batch 280/910 - Loss: 0.0269\n",
      "  Batch 290/910 - Loss: 0.0084\n",
      "  Batch 300/910 - Loss: 0.0737\n",
      "  Batch 310/910 - Loss: 0.0022\n",
      "  Batch 320/910 - Loss: 0.0320\n",
      "  Batch 330/910 - Loss: 0.1018\n",
      "  Batch 340/910 - Loss: 0.0109\n",
      "  Batch 350/910 - Loss: 0.0008\n",
      "  Batch 360/910 - Loss: 0.0537\n",
      "  Batch 370/910 - Loss: 0.0057\n",
      "  Batch 380/910 - Loss: 0.1591\n",
      "  Batch 390/910 - Loss: 0.0643\n",
      "  Batch 400/910 - Loss: 0.0050\n",
      "  Batch 410/910 - Loss: 0.0152\n",
      "  Batch 420/910 - Loss: 0.0022\n",
      "  Batch 430/910 - Loss: 0.0431\n",
      "  Batch 440/910 - Loss: 0.0075\n",
      "  Batch 450/910 - Loss: 0.0016\n",
      "  Batch 460/910 - Loss: 0.0016\n",
      "  Batch 470/910 - Loss: 0.0042\n",
      "  Batch 480/910 - Loss: 0.1000\n",
      "  Batch 490/910 - Loss: 0.0052\n",
      "  Batch 500/910 - Loss: 0.0670\n",
      "  Batch 510/910 - Loss: 0.0028\n",
      "  Batch 520/910 - Loss: 0.0586\n",
      "  Batch 530/910 - Loss: 0.0015\n",
      "  Batch 540/910 - Loss: 0.0292\n",
      "  Batch 550/910 - Loss: 0.0027\n",
      "  Batch 560/910 - Loss: 0.0049\n",
      "  Batch 570/910 - Loss: 0.0550\n",
      "  Batch 580/910 - Loss: 0.0594\n",
      "  Batch 590/910 - Loss: 0.0036\n",
      "  Batch 600/910 - Loss: 0.0015\n",
      "  Batch 610/910 - Loss: 0.0044\n",
      "  Batch 620/910 - Loss: 0.0649\n",
      "  Batch 630/910 - Loss: 0.0025\n",
      "  Batch 640/910 - Loss: 0.0042\n",
      "  Batch 650/910 - Loss: 0.0210\n",
      "  Batch 660/910 - Loss: 0.0052\n",
      "  Batch 670/910 - Loss: 0.0018\n",
      "  Batch 680/910 - Loss: 0.0017\n",
      "  Batch 690/910 - Loss: 0.0045\n",
      "  Batch 700/910 - Loss: 0.0282\n",
      "  Batch 710/910 - Loss: 0.0092\n",
      "  Batch 720/910 - Loss: 0.0050\n",
      "  Batch 730/910 - Loss: 0.0089\n",
      "  Batch 740/910 - Loss: 0.0385\n",
      "  Batch 750/910 - Loss: 0.0040\n",
      "  Batch 760/910 - Loss: 0.0090\n",
      "  Batch 770/910 - Loss: 0.0702\n",
      "  Batch 780/910 - Loss: 0.0840\n",
      "  Batch 790/910 - Loss: 0.0015\n",
      "  Batch 800/910 - Loss: 0.0013\n",
      "  Batch 810/910 - Loss: 0.0030\n",
      "  Batch 820/910 - Loss: 0.0076\n",
      "  Batch 830/910 - Loss: 0.0027\n",
      "  Batch 840/910 - Loss: 0.0713\n",
      "  Batch 850/910 - Loss: 0.0807\n",
      "  Batch 860/910 - Loss: 0.0071\n",
      "  Batch 870/910 - Loss: 0.0251\n",
      "  Batch 880/910 - Loss: 0.0478\n",
      "  Batch 890/910 - Loss: 0.0177\n",
      "  Batch 900/910 - Loss: 0.0717\n",
      "  Batch 910/910 - Loss: 0.1371\n",
      "Training Loss: 0.0286\n",
      "Validation Loss: 0.1087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.70      0.81      4149\n",
      "         1.0       0.33      0.85      0.47       703\n",
      "\n",
      "    accuracy                           0.72      4852\n",
      "   macro avg       0.65      0.78      0.64      4852\n",
      "weighted avg       0.87      0.72      0.76      4852\n",
      "\n",
      "Epoch 15/20\n",
      "  Batch 10/910 - Loss: 0.0083\n",
      "  Batch 20/910 - Loss: 0.0422\n",
      "  Batch 30/910 - Loss: 0.1274\n",
      "  Batch 40/910 - Loss: 0.0018\n",
      "  Batch 50/910 - Loss: 0.0037\n",
      "  Batch 60/910 - Loss: 0.0015\n",
      "  Batch 70/910 - Loss: 0.0905\n",
      "  Batch 80/910 - Loss: 0.0611\n",
      "  Batch 90/910 - Loss: 0.0699\n",
      "  Batch 100/910 - Loss: 0.0546\n",
      "  Batch 110/910 - Loss: 0.0677\n",
      "  Batch 120/910 - Loss: 0.0071\n",
      "  Batch 130/910 - Loss: 0.0011\n",
      "  Batch 140/910 - Loss: 0.0053\n",
      "  Batch 150/910 - Loss: 0.0027\n",
      "  Batch 160/910 - Loss: 0.0023\n",
      "  Batch 170/910 - Loss: 0.0028\n",
      "  Batch 180/910 - Loss: 0.0271\n",
      "  Batch 190/910 - Loss: 0.0594\n",
      "  Batch 200/910 - Loss: 0.0219\n",
      "  Batch 210/910 - Loss: 0.0008\n",
      "  Batch 220/910 - Loss: 0.0010\n",
      "  Batch 230/910 - Loss: 0.0768\n",
      "  Batch 240/910 - Loss: 0.0012\n",
      "  Batch 250/910 - Loss: 0.0018\n",
      "  Batch 260/910 - Loss: 0.0008\n",
      "  Batch 270/910 - Loss: 0.0063\n",
      "  Batch 280/910 - Loss: 0.0051\n",
      "  Batch 290/910 - Loss: 0.0073\n",
      "  Batch 300/910 - Loss: 0.0012\n",
      "  Batch 310/910 - Loss: 0.0041\n",
      "  Batch 320/910 - Loss: 0.0035\n",
      "  Batch 330/910 - Loss: 0.0573\n",
      "  Batch 340/910 - Loss: 0.0009\n",
      "  Batch 350/910 - Loss: 0.0017\n",
      "  Batch 360/910 - Loss: 0.0049\n",
      "  Batch 370/910 - Loss: 0.0073\n",
      "  Batch 380/910 - Loss: 0.0090\n",
      "  Batch 390/910 - Loss: 0.0587\n",
      "  Batch 400/910 - Loss: 0.0256\n",
      "  Batch 410/910 - Loss: 0.0048\n",
      "  Batch 420/910 - Loss: 0.0012\n",
      "  Batch 430/910 - Loss: 0.0059\n",
      "  Batch 440/910 - Loss: 0.0119\n",
      "  Batch 450/910 - Loss: 0.0007\n",
      "  Batch 460/910 - Loss: 0.1428\n",
      "  Batch 470/910 - Loss: 0.0168\n",
      "  Batch 480/910 - Loss: 0.1129\n",
      "  Batch 490/910 - Loss: 0.0045\n",
      "  Batch 500/910 - Loss: 0.0348\n",
      "  Batch 510/910 - Loss: 0.0011\n",
      "  Batch 520/910 - Loss: 0.0410\n",
      "  Batch 530/910 - Loss: 0.0018\n",
      "  Batch 540/910 - Loss: 0.0116\n",
      "  Batch 550/910 - Loss: 0.0040\n",
      "  Batch 560/910 - Loss: 0.0554\n",
      "  Batch 570/910 - Loss: 0.0543\n",
      "  Batch 580/910 - Loss: 0.0038\n",
      "  Batch 590/910 - Loss: 0.0626\n",
      "  Batch 600/910 - Loss: 0.0029\n",
      "  Batch 610/910 - Loss: 0.0053\n",
      "  Batch 620/910 - Loss: 0.0091\n",
      "  Batch 630/910 - Loss: 0.0087\n",
      "  Batch 640/910 - Loss: 0.0195\n",
      "  Batch 650/910 - Loss: 0.0246\n",
      "  Batch 660/910 - Loss: 0.0085\n",
      "  Batch 670/910 - Loss: 0.0097\n",
      "  Batch 680/910 - Loss: 0.1196\n",
      "  Batch 690/910 - Loss: 0.0026\n",
      "  Batch 700/910 - Loss: 0.1219\n",
      "  Batch 710/910 - Loss: 0.0103\n",
      "  Batch 720/910 - Loss: 0.0025\n",
      "  Batch 730/910 - Loss: 0.0046\n",
      "  Batch 740/910 - Loss: 0.0419\n",
      "  Batch 750/910 - Loss: 0.0544\n",
      "  Batch 760/910 - Loss: 0.0015\n",
      "  Batch 770/910 - Loss: 0.0613\n",
      "  Batch 780/910 - Loss: 0.0647\n",
      "  Batch 790/910 - Loss: 0.0410\n",
      "  Batch 800/910 - Loss: 0.0058\n",
      "  Batch 810/910 - Loss: 0.0025\n",
      "  Batch 820/910 - Loss: 0.0031\n",
      "  Batch 830/910 - Loss: 0.0039\n",
      "  Batch 840/910 - Loss: 0.0018\n",
      "  Batch 850/910 - Loss: 0.0022\n",
      "  Batch 860/910 - Loss: 0.0242\n",
      "  Batch 870/910 - Loss: 0.0127\n",
      "  Batch 880/910 - Loss: 0.0208\n",
      "  Batch 890/910 - Loss: 0.0522\n",
      "  Batch 900/910 - Loss: 0.0383\n",
      "  Batch 910/910 - Loss: 0.0036\n",
      "Training Loss: 0.0253\n",
      "Validation Loss: 0.1112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.87      0.90      4149\n",
      "         1.0       0.46      0.66      0.54       703\n",
      "\n",
      "    accuracy                           0.84      4852\n",
      "   macro avg       0.70      0.76      0.72      4852\n",
      "weighted avg       0.87      0.84      0.85      4852\n",
      "\n",
      "Epoch 16/20\n",
      "  Batch 10/910 - Loss: 0.0565\n",
      "  Batch 20/910 - Loss: 0.0045\n",
      "  Batch 30/910 - Loss: 0.0053\n",
      "  Batch 40/910 - Loss: 0.0030\n",
      "  Batch 50/910 - Loss: 0.0085\n",
      "  Batch 60/910 - Loss: 0.0063\n",
      "  Batch 70/910 - Loss: 0.0025\n",
      "  Batch 80/910 - Loss: 0.0115\n",
      "  Batch 90/910 - Loss: 0.0593\n",
      "  Batch 100/910 - Loss: 0.0031\n",
      "  Batch 110/910 - Loss: 0.0393\n",
      "  Batch 120/910 - Loss: 0.0721\n",
      "  Batch 130/910 - Loss: 0.0117\n",
      "  Batch 140/910 - Loss: 0.0062\n",
      "  Batch 150/910 - Loss: 0.0457\n",
      "  Batch 160/910 - Loss: 0.0587\n",
      "  Batch 170/910 - Loss: 0.0021\n",
      "  Batch 180/910 - Loss: 0.0310\n",
      "  Batch 190/910 - Loss: 0.1493\n",
      "  Batch 200/910 - Loss: 0.0030\n",
      "  Batch 210/910 - Loss: 0.1011\n",
      "  Batch 220/910 - Loss: 0.0116\n",
      "  Batch 230/910 - Loss: 0.0009\n",
      "  Batch 240/910 - Loss: 0.0012\n",
      "  Batch 250/910 - Loss: 0.0267\n",
      "  Batch 260/910 - Loss: 0.0506\n",
      "  Batch 270/910 - Loss: 0.0010\n",
      "  Batch 280/910 - Loss: 0.0596\n",
      "  Batch 290/910 - Loss: 0.0247\n",
      "  Batch 300/910 - Loss: 0.0137\n",
      "  Batch 310/910 - Loss: 0.1063\n",
      "  Batch 320/910 - Loss: 0.0365\n",
      "  Batch 330/910 - Loss: 0.0712\n",
      "  Batch 340/910 - Loss: 0.0701\n",
      "  Batch 350/910 - Loss: 0.0033\n",
      "  Batch 360/910 - Loss: 0.0016\n",
      "  Batch 370/910 - Loss: 0.0023\n",
      "  Batch 380/910 - Loss: 0.0032\n",
      "  Batch 390/910 - Loss: 0.0044\n",
      "  Batch 400/910 - Loss: 0.0016\n",
      "  Batch 410/910 - Loss: 0.0405\n",
      "  Batch 420/910 - Loss: 0.0553\n",
      "  Batch 430/910 - Loss: 0.0360\n",
      "  Batch 440/910 - Loss: 0.0148\n",
      "  Batch 450/910 - Loss: 0.0008\n",
      "  Batch 460/910 - Loss: 0.0650\n",
      "  Batch 470/910 - Loss: 0.0009\n",
      "  Batch 480/910 - Loss: 0.0334\n",
      "  Batch 490/910 - Loss: 0.0019\n",
      "  Batch 500/910 - Loss: 0.0013\n",
      "  Batch 510/910 - Loss: 0.0413\n",
      "  Batch 520/910 - Loss: 0.0063\n",
      "  Batch 530/910 - Loss: 0.0081\n",
      "  Batch 540/910 - Loss: 0.0084\n",
      "  Batch 550/910 - Loss: 0.0010\n",
      "  Batch 560/910 - Loss: 0.0426\n",
      "  Batch 570/910 - Loss: 0.0262\n",
      "  Batch 580/910 - Loss: 0.0145\n",
      "  Batch 590/910 - Loss: 0.0013\n",
      "  Batch 600/910 - Loss: 0.0056\n",
      "  Batch 610/910 - Loss: 0.0013\n",
      "  Batch 620/910 - Loss: 0.0108\n",
      "  Batch 630/910 - Loss: 0.0158\n",
      "  Batch 640/910 - Loss: 0.0606\n",
      "  Batch 650/910 - Loss: 0.1042\n",
      "  Batch 660/910 - Loss: 0.0075\n",
      "  Batch 670/910 - Loss: 0.0628\n",
      "  Batch 680/910 - Loss: 0.0283\n",
      "  Batch 690/910 - Loss: 0.0046\n",
      "  Batch 700/910 - Loss: 0.0023\n",
      "  Batch 710/910 - Loss: 0.0186\n",
      "  Batch 720/910 - Loss: 0.0622\n",
      "  Batch 730/910 - Loss: 0.0835\n",
      "  Batch 740/910 - Loss: 0.0031\n",
      "  Batch 750/910 - Loss: 0.0084\n",
      "  Batch 760/910 - Loss: 0.0646\n",
      "  Batch 770/910 - Loss: 0.0052\n",
      "  Batch 780/910 - Loss: 0.0049\n",
      "  Batch 790/910 - Loss: 0.0086\n",
      "  Batch 800/910 - Loss: 0.1069\n",
      "  Batch 810/910 - Loss: 0.0491\n",
      "  Batch 820/910 - Loss: 0.0569\n",
      "  Batch 830/910 - Loss: 0.0029\n",
      "  Batch 840/910 - Loss: 0.0004\n",
      "  Batch 850/910 - Loss: 0.0023\n",
      "  Batch 860/910 - Loss: 0.0083\n",
      "  Batch 870/910 - Loss: 0.0585\n",
      "  Batch 880/910 - Loss: 0.0141\n",
      "  Batch 890/910 - Loss: 0.0126\n",
      "  Batch 900/910 - Loss: 0.0019\n",
      "  Batch 910/910 - Loss: 0.0041\n",
      "Training Loss: 0.0244\n",
      "Validation Loss: 0.1091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      4149\n",
      "         1.0       0.14      1.00      0.25       703\n",
      "\n",
      "    accuracy                           0.14      4852\n",
      "   macro avg       0.07      0.50      0.13      4852\n",
      "weighted avg       0.02      0.14      0.04      4852\n",
      "\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/910 - Loss: 0.0023\n",
      "  Batch 20/910 - Loss: 0.0060\n",
      "  Batch 30/910 - Loss: 0.0739\n",
      "  Batch 40/910 - Loss: 0.0014\n",
      "  Batch 50/910 - Loss: 0.0494\n",
      "  Batch 60/910 - Loss: 0.0232\n",
      "  Batch 70/910 - Loss: 0.0268\n",
      "  Batch 80/910 - Loss: 0.0935\n",
      "  Batch 90/910 - Loss: 0.0009\n",
      "  Batch 100/910 - Loss: 0.0027\n",
      "  Batch 110/910 - Loss: 0.1106\n",
      "  Batch 120/910 - Loss: 0.0334\n",
      "  Batch 130/910 - Loss: 0.0013\n",
      "  Batch 140/910 - Loss: 0.0095\n",
      "  Batch 150/910 - Loss: 0.0311\n",
      "  Batch 160/910 - Loss: 0.0030\n",
      "  Batch 170/910 - Loss: 0.0009\n",
      "  Batch 180/910 - Loss: 0.0590\n",
      "  Batch 190/910 - Loss: 0.0720\n",
      "  Batch 200/910 - Loss: 0.0026\n",
      "  Batch 210/910 - Loss: 0.0022\n",
      "  Batch 220/910 - Loss: 0.0022\n",
      "  Batch 230/910 - Loss: 0.0028\n",
      "  Batch 240/910 - Loss: 0.0020\n",
      "  Batch 250/910 - Loss: 0.0501\n",
      "  Batch 260/910 - Loss: 0.0015\n",
      "  Batch 270/910 - Loss: 0.0864\n",
      "  Batch 280/910 - Loss: 0.0741\n",
      "  Batch 290/910 - Loss: 0.0678\n",
      "  Batch 300/910 - Loss: 0.0036\n",
      "  Batch 310/910 - Loss: 0.0104\n",
      "  Batch 320/910 - Loss: 0.0020\n",
      "  Batch 330/910 - Loss: 0.0030\n",
      "  Batch 340/910 - Loss: 0.0040\n",
      "  Batch 350/910 - Loss: 0.0042\n",
      "  Batch 360/910 - Loss: 0.0025\n",
      "  Batch 370/910 - Loss: 0.0049\n",
      "  Batch 380/910 - Loss: 0.0032\n",
      "  Batch 390/910 - Loss: 0.0010\n",
      "  Batch 400/910 - Loss: 0.0624\n",
      "  Batch 410/910 - Loss: 0.0017\n",
      "  Batch 420/910 - Loss: 0.0004\n",
      "  Batch 430/910 - Loss: 0.0048\n",
      "  Batch 440/910 - Loss: 0.0041\n",
      "  Batch 450/910 - Loss: 0.0033\n",
      "  Batch 460/910 - Loss: 0.0041\n",
      "  Batch 470/910 - Loss: 0.0740\n",
      "  Batch 480/910 - Loss: 0.0034\n",
      "  Batch 490/910 - Loss: 0.0162\n",
      "  Batch 500/910 - Loss: 0.0031\n",
      "  Batch 510/910 - Loss: 0.0061\n",
      "  Batch 520/910 - Loss: 0.0037\n",
      "  Batch 530/910 - Loss: 0.0066\n",
      "  Batch 540/910 - Loss: 0.0056\n",
      "  Batch 550/910 - Loss: 0.0680\n",
      "  Batch 560/910 - Loss: 0.0024\n",
      "  Batch 570/910 - Loss: 0.0043\n",
      "  Batch 580/910 - Loss: 0.0024\n",
      "  Batch 590/910 - Loss: 0.0785\n",
      "  Batch 600/910 - Loss: 0.0246\n",
      "  Batch 610/910 - Loss: 0.0028\n",
      "  Batch 620/910 - Loss: 0.0579\n",
      "  Batch 630/910 - Loss: 0.0686\n",
      "  Batch 640/910 - Loss: 0.1170\n",
      "  Batch 650/910 - Loss: 0.0023\n",
      "  Batch 660/910 - Loss: 0.0031\n",
      "  Batch 670/910 - Loss: 0.0033\n",
      "  Batch 680/910 - Loss: 0.0616\n",
      "  Batch 690/910 - Loss: 0.0028\n",
      "  Batch 700/910 - Loss: 0.0007\n",
      "  Batch 710/910 - Loss: 0.0040\n",
      "  Batch 720/910 - Loss: 0.0699\n",
      "  Batch 730/910 - Loss: 0.0502\n",
      "  Batch 740/910 - Loss: 0.0042\n",
      "  Batch 750/910 - Loss: 0.0013\n",
      "  Batch 760/910 - Loss: 0.0047\n",
      "  Batch 770/910 - Loss: 0.0454\n",
      "  Batch 780/910 - Loss: 0.0612\n",
      "  Batch 790/910 - Loss: 0.0040\n",
      "  Batch 800/910 - Loss: 0.0089\n",
      "  Batch 810/910 - Loss: 0.0197\n",
      "  Batch 820/910 - Loss: 0.0014\n",
      "  Batch 830/910 - Loss: 0.0613\n",
      "  Batch 840/910 - Loss: 0.0018\n",
      "  Batch 850/910 - Loss: 0.0007\n",
      "  Batch 860/910 - Loss: 0.0025\n",
      "  Batch 870/910 - Loss: 0.0024\n",
      "  Batch 880/910 - Loss: 0.0025\n",
      "  Batch 890/910 - Loss: 0.0659\n",
      "  Batch 900/910 - Loss: 0.0015\n",
      "  Batch 910/910 - Loss: 0.0033\n",
      "Training Loss: 0.0227\n",
      "Validation Loss: 0.1086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      4149\n",
      "         1.0       0.14      1.00      0.25       703\n",
      "\n",
      "    accuracy                           0.14      4852\n",
      "   macro avg       0.07      0.50      0.13      4852\n",
      "weighted avg       0.02      0.14      0.04      4852\n",
      "\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/910 - Loss: 0.0134\n",
      "  Batch 20/910 - Loss: 0.0611\n",
      "  Batch 30/910 - Loss: 0.0052\n",
      "  Batch 40/910 - Loss: 0.0359\n",
      "  Batch 50/910 - Loss: 0.0056\n",
      "  Batch 60/910 - Loss: 0.0187\n",
      "  Batch 70/910 - Loss: 0.0038\n",
      "  Batch 80/910 - Loss: 0.0027\n",
      "  Batch 90/910 - Loss: 0.0047\n",
      "  Batch 100/910 - Loss: 0.0049\n",
      "  Batch 110/910 - Loss: 0.0011\n",
      "  Batch 120/910 - Loss: 0.0026\n",
      "  Batch 130/910 - Loss: 0.1226\n",
      "  Batch 140/910 - Loss: 0.0029\n",
      "  Batch 150/910 - Loss: 0.0609\n",
      "  Batch 160/910 - Loss: 0.0049\n",
      "  Batch 170/910 - Loss: 0.0160\n",
      "  Batch 180/910 - Loss: 0.0600\n",
      "  Batch 190/910 - Loss: 0.1114\n",
      "  Batch 200/910 - Loss: 0.0653\n",
      "  Batch 210/910 - Loss: 0.0052\n",
      "  Batch 220/910 - Loss: 0.0076\n",
      "  Batch 230/910 - Loss: 0.0107\n",
      "  Batch 240/910 - Loss: 0.0026\n",
      "  Batch 250/910 - Loss: 0.0007\n",
      "  Batch 260/910 - Loss: 0.0006\n",
      "  Batch 270/910 - Loss: 0.0541\n",
      "  Batch 280/910 - Loss: 0.0022\n",
      "  Batch 290/910 - Loss: 0.0012\n",
      "  Batch 300/910 - Loss: 0.0529\n",
      "  Batch 310/910 - Loss: 0.0741\n",
      "  Batch 320/910 - Loss: 0.0634\n",
      "  Batch 330/910 - Loss: 0.0191\n",
      "  Batch 340/910 - Loss: 0.0652\n",
      "  Batch 350/910 - Loss: 0.0019\n",
      "  Batch 360/910 - Loss: 0.0009\n",
      "  Batch 370/910 - Loss: 0.0689\n",
      "  Batch 380/910 - Loss: 0.0115\n",
      "  Batch 390/910 - Loss: 0.0747\n",
      "  Batch 400/910 - Loss: 0.0009\n",
      "  Batch 410/910 - Loss: 0.0147\n",
      "  Batch 420/910 - Loss: 0.0046\n",
      "  Batch 430/910 - Loss: 0.0037\n",
      "  Batch 440/910 - Loss: 0.0857\n",
      "  Batch 450/910 - Loss: 0.0039\n",
      "  Batch 460/910 - Loss: 0.0767\n",
      "  Batch 470/910 - Loss: 0.0023\n",
      "  Batch 480/910 - Loss: 0.0631\n",
      "  Batch 490/910 - Loss: 0.0020\n",
      "  Batch 500/910 - Loss: 0.0472\n",
      "  Batch 510/910 - Loss: 0.0477\n",
      "  Batch 520/910 - Loss: 0.0088\n",
      "  Batch 530/910 - Loss: 0.0023\n",
      "  Batch 540/910 - Loss: 0.0288\n",
      "  Batch 550/910 - Loss: 0.0052\n",
      "  Batch 560/910 - Loss: 0.0024\n",
      "  Batch 570/910 - Loss: 0.0276\n",
      "  Batch 580/910 - Loss: 0.0049\n",
      "  Batch 590/910 - Loss: 0.0028\n",
      "  Batch 600/910 - Loss: 0.0005\n",
      "  Batch 610/910 - Loss: 0.0241\n",
      "  Batch 620/910 - Loss: 0.0074\n",
      "  Batch 630/910 - Loss: 0.0016\n",
      "  Batch 640/910 - Loss: 0.0020\n",
      "  Batch 650/910 - Loss: 0.0341\n",
      "  Batch 660/910 - Loss: 0.0531\n",
      "  Batch 670/910 - Loss: 0.0021\n",
      "  Batch 680/910 - Loss: 0.0064\n",
      "  Batch 690/910 - Loss: 0.0049\n",
      "  Batch 700/910 - Loss: 0.1053\n",
      "  Batch 710/910 - Loss: 0.0391\n",
      "  Batch 720/910 - Loss: 0.0091\n",
      "  Batch 730/910 - Loss: 0.0418\n",
      "  Batch 740/910 - Loss: 0.0195\n",
      "  Batch 750/910 - Loss: 0.0014\n",
      "  Batch 760/910 - Loss: 0.0890\n",
      "  Batch 770/910 - Loss: 0.0055\n",
      "  Batch 780/910 - Loss: 0.0003\n",
      "  Batch 790/910 - Loss: 0.0019\n",
      "  Batch 800/910 - Loss: 0.0012\n",
      "  Batch 810/910 - Loss: 0.0639\n",
      "  Batch 820/910 - Loss: 0.0657\n",
      "  Batch 830/910 - Loss: 0.0551\n",
      "  Batch 840/910 - Loss: 0.0080\n",
      "  Batch 850/910 - Loss: 0.0256\n",
      "  Batch 860/910 - Loss: 0.0052\n",
      "  Batch 870/910 - Loss: 0.0082\n",
      "  Batch 880/910 - Loss: 0.0049\n",
      "  Batch 890/910 - Loss: 0.0015\n",
      "  Batch 900/910 - Loss: 0.0399\n",
      "  Batch 910/910 - Loss: 0.0069\n",
      "Training Loss: 0.0216\n",
      "Validation Loss: 0.1076\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      4149\n",
      "         1.0       0.14      1.00      0.25       703\n",
      "\n",
      "    accuracy                           0.14      4852\n",
      "   macro avg       0.07      0.50      0.13      4852\n",
      "weighted avg       0.02      0.14      0.04      4852\n",
      "\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10/910 - Loss: 0.0008\n",
      "  Batch 20/910 - Loss: 0.0009\n",
      "  Batch 30/910 - Loss: 0.0512\n",
      "  Batch 40/910 - Loss: 0.0017\n",
      "  Batch 50/910 - Loss: 0.0011\n",
      "  Batch 60/910 - Loss: 0.0017\n",
      "  Batch 70/910 - Loss: 0.0617\n",
      "  Batch 80/910 - Loss: 0.0635\n",
      "  Batch 90/910 - Loss: 0.0023\n",
      "  Batch 100/910 - Loss: 0.1036\n",
      "  Batch 110/910 - Loss: 0.0072\n",
      "  Batch 120/910 - Loss: 0.0049\n",
      "  Batch 130/910 - Loss: 0.0282\n",
      "  Batch 140/910 - Loss: 0.0023\n",
      "  Batch 150/910 - Loss: 0.0003\n",
      "  Batch 160/910 - Loss: 0.0105\n",
      "  Batch 170/910 - Loss: 0.0015\n",
      "  Batch 180/910 - Loss: 0.0023\n",
      "  Batch 190/910 - Loss: 0.0014\n",
      "  Batch 200/910 - Loss: 0.0020\n",
      "  Batch 210/910 - Loss: 0.0046\n",
      "  Batch 220/910 - Loss: 0.0079\n",
      "  Batch 230/910 - Loss: 0.0070\n",
      "  Batch 240/910 - Loss: 0.0257\n",
      "  Batch 250/910 - Loss: 0.0708\n",
      "  Batch 260/910 - Loss: 0.0392\n",
      "  Batch 270/910 - Loss: 0.0565\n",
      "  Batch 280/910 - Loss: 0.0563\n",
      "  Batch 290/910 - Loss: 0.0035\n",
      "  Batch 300/910 - Loss: 0.0359\n",
      "  Batch 310/910 - Loss: 0.0564\n",
      "  Batch 320/910 - Loss: 0.0064\n",
      "  Batch 330/910 - Loss: 0.0122\n",
      "  Batch 340/910 - Loss: 0.0011\n",
      "  Batch 350/910 - Loss: 0.0020\n",
      "  Batch 360/910 - Loss: 0.0019\n",
      "  Batch 370/910 - Loss: 0.0596\n",
      "  Batch 380/910 - Loss: 0.0060\n",
      "  Batch 390/910 - Loss: 0.0009\n",
      "  Batch 400/910 - Loss: 0.0503\n",
      "  Batch 410/910 - Loss: 0.0025\n",
      "  Batch 420/910 - Loss: 0.0152\n",
      "  Batch 430/910 - Loss: 0.0013\n",
      "  Batch 440/910 - Loss: 0.0947\n",
      "  Batch 450/910 - Loss: 0.0136\n",
      "  Batch 460/910 - Loss: 0.0049\n",
      "  Batch 470/910 - Loss: 0.0007\n",
      "  Batch 480/910 - Loss: 0.0139\n",
      "  Batch 490/910 - Loss: 0.0115\n",
      "  Batch 500/910 - Loss: 0.0339\n",
      "  Batch 510/910 - Loss: 0.0700\n",
      "  Batch 520/910 - Loss: 0.0077\n",
      "  Batch 530/910 - Loss: 0.0015\n",
      "  Batch 540/910 - Loss: 0.0038\n",
      "  Batch 550/910 - Loss: 0.0096\n",
      "  Batch 560/910 - Loss: 0.0026\n",
      "  Batch 570/910 - Loss: 0.0993\n",
      "  Batch 580/910 - Loss: 0.0010\n",
      "  Batch 590/910 - Loss: 0.0506\n",
      "  Batch 600/910 - Loss: 0.0049\n",
      "  Batch 610/910 - Loss: 0.0029\n",
      "  Batch 620/910 - Loss: 0.0031\n",
      "  Batch 630/910 - Loss: 0.0434\n",
      "  Batch 640/910 - Loss: 0.0227\n",
      "  Batch 650/910 - Loss: 0.0023\n",
      "  Batch 660/910 - Loss: 0.0010\n",
      "  Batch 670/910 - Loss: 0.0033\n",
      "  Batch 680/910 - Loss: 0.0033\n",
      "  Batch 690/910 - Loss: 0.0026\n",
      "  Batch 700/910 - Loss: 0.0614\n",
      "  Batch 710/910 - Loss: 0.0563\n",
      "  Batch 720/910 - Loss: 0.0631\n",
      "  Batch 730/910 - Loss: 0.0070\n",
      "  Batch 740/910 - Loss: 0.0564\n",
      "  Batch 750/910 - Loss: 0.1579\n",
      "  Batch 760/910 - Loss: 0.0014\n",
      "  Batch 770/910 - Loss: 0.0023\n",
      "  Batch 780/910 - Loss: 0.0246\n",
      "  Batch 790/910 - Loss: 0.0283\n",
      "  Batch 800/910 - Loss: 0.0057\n",
      "  Batch 810/910 - Loss: 0.0614\n",
      "  Batch 820/910 - Loss: 0.0363\n",
      "  Batch 830/910 - Loss: 0.0010\n",
      "  Batch 840/910 - Loss: 0.0040\n",
      "  Batch 850/910 - Loss: 0.0034\n",
      "  Batch 860/910 - Loss: 0.0015\n",
      "  Batch 870/910 - Loss: 0.0080\n",
      "  Batch 880/910 - Loss: 0.0028\n",
      "  Batch 890/910 - Loss: 0.0011\n",
      "  Batch 900/910 - Loss: 0.0652\n",
      "  Batch 910/910 - Loss: 0.0015\n",
      "Training Loss: 0.0199\n",
      "Validation Loss: 0.1149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.73      0.83      4149\n",
      "         1.0       0.34      0.81      0.48       703\n",
      "\n",
      "    accuracy                           0.74      4852\n",
      "   macro avg       0.65      0.77      0.65      4852\n",
      "weighted avg       0.87      0.74      0.78      4852\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import XLNetForSequenceClassification\n",
    "\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        encoding = self.tokenizer(\n",
    "            row['content'], # Replace 'content' with 'article_content' if working with real data\n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt' #  return the output in the form of PyTorch tensors\n",
    "            )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(row['relevant'], dtype=torch.float), # for BCEWithLogitsLoss use float\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(train_df, tokenizer, max_length=128)\n",
    "val_dataset = TextDataset(val_df, tokenizer, max_length=128)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['relevant']), y=train_df['relevant'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# load the XLNet model\n",
    "XLNet_model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=1)\n",
    "XLNet_model.to(device)\n",
    "\n",
    "# load from checkpoint\n",
    "checkpoint = torch.load('best_XLNet_model_epoch_3_BCEWithLogitsLoss.pt')\n",
    "XLNet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Loss function and optimizer\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "optimizer = AdamW(XLNet_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "current_epoch = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(current_epoch, num_epochs):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    XLNet_model.train() #switch to training mode\n",
    "    train_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad() # Clear old gradients\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device),\n",
    "        }\n",
    "\n",
    "        outputs = XLNet_model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(train_dataloader) # average training loss\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    XLNet_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device),\n",
    "            }\n",
    "            outputs = XLNet_model(**inputs)\n",
    "            val_loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            preds = torch.sigmoid(logits).cpu().numpy() > 0.5  # threshold = 0.5 for  now\n",
    "            val_preds.extend(preds)           \n",
    "            val_labels.extend(inputs['labels'].cpu().numpy())\n",
    "            \n",
    "    val_loss /= len(val_dataloader) # average validation loss\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(classification_report(val_labels, val_preds))\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f\"Validation loss improved. Saving model.\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': XLNet_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss\n",
    "    }, f\"best_XLNet_model_epoch_{epoch}_BCEWithLogitsLoss.pt\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "XLNet_model.eval()\n",
    "y_probs, y_true = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device),\n",
    "        }\n",
    "        outputs = XLNet_model(**inputs)\n",
    "        val_loss += outputs.loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.sigmoid(logits).cpu().numpy()\n",
    "        y_probs.extend(preds)\n",
    "        y_true.extend(inputs['labels'].cpu().numpy())\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(recall, precision, color='b', label=\"Precision-Recall curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Compute F1 scores \n",
    "f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the performance with different threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(test_df, tokenizer, max_length=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "XLNet_model.eval()\n",
    "y_preds, y_true = [], []\n",
    "with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device),\n",
    "            }\n",
    "            outputs = XLNet_model(**inputs)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.sigmoid(logits).cpu().numpy() > 0.55 # testing threshold = 0.65 for now\n",
    "            y_preds.extend(preds)\n",
    "            y_true.extend(inputs['labels'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93      4158\n",
      "         1.0       0.58      0.52      0.55       694\n",
      "\n",
      "    accuracy                           0.88      4852\n",
      "   macro avg       0.75      0.73      0.74      4852\n",
      "weighted avg       0.87      0.88      0.87      4852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
