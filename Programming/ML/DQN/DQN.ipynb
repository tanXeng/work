{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e52f3e",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "We shall attempt build a DQN model that can play atari games. We will use a CNN that will take frames of the game as input and output Q values for each move. For starters, we will try to train a model that can play cartpole - a relatively simple game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46811768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebae7ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b12bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# creating a seperate env for visualization\n",
    "env_human = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eccccc",
   "metadata": {},
   "source": [
    "### CNN, Loss function and optimizer\n",
    "Here we define the model, the loss function and the optimizer. If one NN is used to determine the target Q values, it could lead to instability, as the distribution of target Q values will constantly change. Hence the target NN, with frozen parameters, is defined to reduce such instability. Every few episodes, the online NN is copied over to the target NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10f53fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=4096, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # layers may be tweaked depending on the task\n",
    "        self.conv1 = nn.Conv2d(3, 6, 9)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 7)\n",
    "        self.fc1 = nn.Linear(16 * 16 * 16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2) # cart pole has 2 possible actions per state\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.leaky_relu(self.conv1(x)))\n",
    "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "online_NN = DQN()\n",
    "target_NN = DQN()\n",
    "online_NN.to(device)\n",
    "target_NN.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f9155",
   "metadata": {},
   "source": [
    "For DQNs, the MSE between the predicted Q values and the target Q is used as the loss function. As for the optimizer, we will use AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "336a6ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "298bedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamW(online_NN.parameters(), lr=2e-5, weight_decay=0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32271e8a",
   "metadata": {},
   "source": [
    "### Dataset and Preprocessing Functions\n",
    "Since we are passing frames into the CNN, we it to be preprocessed into an input of the appropriate form \n",
    "i.e. the input should be of shape (84,84) and normalized between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "246b6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Preprocess an RGB frame for DQN input.\n",
    "\n",
    "    Args:\n",
    "        frame (np.ndarray): Input RGB frame as a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed frame resized to (84, 84) and normalized to [0, 1].\n",
    "    \"\"\"\n",
    "    # resizing the frame and normalizing it\n",
    "    frame_resized = cv2.resize(frame, (84, 84))\n",
    "    frame_normalized = frame_resized.astype(np.float32) / 255\n",
    "\n",
    "    # seperating each channel\n",
    "    r = frame_normalized[:, :, 0]\n",
    "    g = frame_normalized[:, :, 1]\n",
    "    b = frame_normalized[:, :, 2]\n",
    "\n",
    "    # converting to tensor\n",
    "    preprocessed_frame = torch.tensor([r, g, b], dtype=torch.float32)\n",
    "\n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8067dfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 84, 84])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = env.render()\n",
    "preprocess_frame(frame).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa700a2",
   "metadata": {},
   "source": [
    "To enable batching with a DataLoader, we first define a custom Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNDataset(Dataset):\n",
    "    def __init__(self, replay_buffer):\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.replay_buffer)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        state, action, reward, next_state, done = self.replay_buffer[idx]\n",
    "        return state, \\\n",
    "               torch.tensor(action, dtype=torch.int64), \\\n",
    "               torch.tensor(reward, dtype=torch.float32), \\\n",
    "               next_state, \\\n",
    "               torch.tensor(done, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f869c",
   "metadata": {},
   "source": [
    "### Training\n",
    "With reference to the DQN paper (Mnih et al., 2013), in each episode, we use the online NN to collect a sample of experiences, filling the replay buffer. Then with the replay buffer, perform SGD on the online NN, using the target NN to determine the target Q values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9965ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(env, epsilon, env_human=None):\n",
    "        \n",
    "    \"\"\"\n",
    "    Collect a single transition using an epsilon-greedy policy.\n",
    "\n",
    "    With probability `epsilon`, a random action is selected, otherwise, the action with the highest Q-value is chosen.\n",
    "    The function returns the state, action, reward, next state, and done flag.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment instance in rgb mode.\n",
    "        env_human (gym.Env, optional): The environment instance in human mode. If `None`, the GUI will not be rendered. Default is `None`.\n",
    "        epsilon (float): The probability of selecting a random action.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (state, action, reward, next_state, done), where:\n",
    "            - state (np.ndarray): The current state.\n",
    "            - action (int): The selected action.\n",
    "            - reward (float): The reward received.\n",
    "            - next_state (np.ndarray): The next state.\n",
    "            - done (bool): True if the episode is done (either due to termination or truncation).\n",
    "    \"\"\"\n",
    "\n",
    "    # obtaining the initial state\n",
    "    frame = env.render()\n",
    "    if env_human:\n",
    "        env_human.render()\n",
    "    state = preprocess_frame(frame)\n",
    "\n",
    "    # obtaining the Q predictions\n",
    "    state_batch = state.unsqueeze(0).to(device)\n",
    "    Q_prediction = online_NN.forward(state_batch)\n",
    "\n",
    "    # choose a random action with probability epsilon\n",
    "    rand_num = np.random.uniform(0, 1)\n",
    "    if rand_num < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q_prediction) \n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if env_human:\n",
    "        env_human.step(action)\n",
    "\n",
    "    # obtaining the next state\n",
    "    frame = env.render()\n",
    "    if env_human:\n",
    "        env_human.render()\n",
    "    next_state = preprocess_frame(frame)\n",
    "\n",
    "    # store 1 if the state is a terminal or truncted\n",
    "    done = int(terminated or truncated)\n",
    "\n",
    "    return (state, action, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4aafcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_replay_buffer(init_buffer_size, env, epsilon):\n",
    "\n",
    "    \"\"\"\n",
    "    Fill the replay buffer with random or epsilon-greedy experience.\n",
    "\n",
    "    Args:\n",
    "        init_buffer_size (int): Number of transitions to fill the buffer with initially.\n",
    "        env (gym.Env): The environment instance in rgb mode.\n",
    "        epsilon (float): Probability of choosing a random action (exploration).\n",
    "\n",
    "    Returns:\n",
    "        list: List of (state, action, reward, next_state, done) transitions.\n",
    "    \"\"\"\n",
    "\n",
    "    replay_buffer = []\n",
    "\n",
    "    for i in range(init_buffer_size):\n",
    "        \n",
    "        # collecting an experience\n",
    "        experience = collect_experience(env, epsilon)\n",
    "\n",
    "        # storing the experience in the replay buffer\n",
    "        replay_buffer.append(experience)\n",
    "\n",
    "        # reseting if state is truncated or terminated\n",
    "        if experience[-1]:\n",
    "            env.reset() \n",
    "\n",
    "    return replay_buffer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d228cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Q(states, NN):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform a forward pass through the NN to compute the Q values.\n",
    "\n",
    "    Args:\n",
    "        states (torch.Tensor): A tensor of states (batch_size, state_size).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed Q-values (batch_size, num_actions).\n",
    "    \"\"\"\n",
    "\n",
    "    Q_values = NN.forward(states)\n",
    "\n",
    "    return Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "643fed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_Q(batch, target_NN, gamma):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the target Q-values using the Bellman equation.\n",
    "\n",
    "    Args:\n",
    "        batch (tuple): A tuple containing states, actions, rewards, next_states, and done flags.\n",
    "        target_NN (nn.Module): The target neural network.\n",
    "        gamma (float): The discount factor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The target Q-values for each state-action pair in the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # unpacking values\n",
    "    states, actions, rewards, next_states, done = batch\n",
    "\n",
    "    # moving tensors to the gpu (if there is one)\n",
    "    rewards = rewards.to(device)\n",
    "    next_states = next_states.to(device)\n",
    "    done = done.to(device)\n",
    "\n",
    "    # computing target Q\n",
    "    next_Q_values = compute_Q(next_states, target_NN)\n",
    "    target_Q_values = rewards + (gamma * torch.max(next_Q_values, dim=1)[0] * (1 - done))\n",
    "\n",
    "    return target_Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60968a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(batch, target_Q, device):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform one SGD step on a sampled batch from the replay buffer.\n",
    "\n",
    "    Args:\n",
    "        batch (tuple): A batch of (states, actions, rewards, next_states, dones).\n",
    "        device (torch.device): Device to run computations on.\n",
    "\n",
    "    Returns:\n",
    "        float: The loss for this step.\n",
    "    \"\"\"\n",
    "\n",
    "    # unpacking values\n",
    "    states, actions, rewards, next_states, done = batch\n",
    "\n",
    "    # moving tensors to the gpu (if there is one)\n",
    "    states = states.to(device)\n",
    "    actions = actions.to(device)\n",
    "\n",
    "    # computing the Q values from the batch\n",
    "    predicted_Q_values = compute_Q(states, online_NN)\n",
    "    Q_predictions_of_actions = predicted_Q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # calculating the loss\n",
    "    loss = loss_fn(Q_predictions_of_actions, target_Q.detach())\n",
    "\n",
    "    # back propagating and updating weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbe722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(samples_dataloader)) \u001b[38;5;66;03m# there is only 1 batch \u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# computing target Q\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m target_Q_values \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_target_Q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_NN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGAMMA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# performing SGD\u001b[39;00m\n\u001b[0;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m sgd_step(batch, target_Q_values, device)\n",
      "Cell \u001b[1;32mIn[29], line 25\u001b[0m, in \u001b[0;36mcompute_target_Q\u001b[1;34m(batch, target_NN, gamma)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# computing target Q\u001b[39;00m\n\u001b[0;32m     24\u001b[0m next_Q_values \u001b[38;5;241m=\u001b[39m compute_Q(next_states, target_NN)\n\u001b[1;32m---> 25\u001b[0m target_Q_values \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (gamma \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(next_Q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_Q_values\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:848\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "M = 10000 # the total number of episodes\n",
    "T = 10000 # the maximum number of actions per episode\n",
    "N = 4 # number of experiences to collect before performing SGD\n",
    "C = 100 # the number of weight updates between updating the target NN\n",
    "UPDATES_BETWEEN_VAL = 1000 # the number of weight updates between model validation\n",
    "EPSILON = 1 # the proability that a random move will be selected\n",
    "MIN_EPSILON = 0.1 # the minimum epsilon attainable\n",
    "EPSILON_DECAY_RATE = EPSILON / M # how much epsilon is reduced per episode\n",
    "GAMMA = 0.9 # the priority placed on future rewards\n",
    "BATCH_SIZE = 32\n",
    "INIT_BUFFER_SIZE = int(10000 / 2) # number of experiences to fill the buffer with initially\n",
    "MAX_BUFFER_SIZE = 10000 # the maximum number of experiences the buffer can hold\n",
    "\n",
    "print('Starting training...')\n",
    "\n",
    "# filling the replay buffer\n",
    "replay_buffer = fill_replay_buffer(INIT_BUFFER_SIZE, env, EPSILON)\n",
    "\n",
    "# initialising num_updates to keep track of the number of times the model weights are updated\n",
    "num_updates = 0\n",
    "\n",
    "for episode_number in range(M):\n",
    "    \n",
    "    # reseting the environment at the start of each episode\n",
    "    env.reset()\n",
    "    env_human.reset()\n",
    "    \n",
    "    # starting a new episode if game is not terminated in T actions\n",
    "    for t in range(T):\n",
    "\n",
    "        # collecting N new experiences to update the buffer\n",
    "        new_experiences = []\n",
    "        for i in range(N):\n",
    "\n",
    "            # collecting a new experience\n",
    "            experience = collect_experience(env, EPSILON, env_human=env_human)\n",
    "            new_experiences.append(experience)\n",
    "\n",
    "            # ending the episode if the new state is terminal\n",
    "            if experience[-1] == 1:\n",
    "                break\n",
    "            \n",
    "        # adding N new experiences to the buffer\n",
    "        replay_buffer.extend(new_experiences)\n",
    "\n",
    "        # ensuring the buffer size does not exceed MAX_BUFFER_SIZE\n",
    "        if len(replay_buffer) > MAX_BUFFER_SIZE:\n",
    "            replay_buffer = replay_buffer[-MAX_BUFFER_SIZE:]\n",
    "\n",
    "        # preparing the replay buffer for sampling\n",
    "        replay_buffer_prepared = DQNDataset(replay_buffer)\n",
    "        sampler = RandomSampler(replay_buffer_prepared, replacement=True, num_samples=BATCH_SIZE)\n",
    "\n",
    "        # sampling from the replay experience\n",
    "        samples_dataloader = DataLoader(replay_buffer_prepared, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "\n",
    "        # extracting the sample\n",
    "        batch = next(iter(samples_dataloader)) # there is only 1 batch \n",
    "\n",
    "        # computing target Q\n",
    "        target_Q_values = compute_target_Q(batch, target_NN, GAMMA)\n",
    "\n",
    "        # performing SGD\n",
    "        loss = sgd_step(batch, target_Q_values, device)\n",
    "\n",
    "        # incrementing num_updates\n",
    "        num_updates += 1\n",
    "\n",
    "        # validating\n",
    "        # if num_updates % STEPS_BETWEEN_VAL == 0:\n",
    "        #     print('Validating...')\n",
    "\n",
    "        # updating target_NN weights\n",
    "        if num_updates % C == 0:\n",
    "            target_NN.load_state_dict(online_NN.state_dict())\n",
    "\n",
    "        # reducing EPSILON as training progresses\n",
    "        EPSILON = max(MIN_EPSILON, EPSILON - EPSILON_DECAY_RATE * episode_number)\n",
    "\n",
    "# saving the model weights\n",
    "torch.save({'model_state_dict': online_NN.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()},\n",
    "            f\"Users\\\\tanxe\\\\Programming\\\\ML\\\\DQN\\\\models\\\\DQN_cartpole.pt\")\n",
    "    \n",
    "env.close()\n",
    "env_human.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d0130",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
